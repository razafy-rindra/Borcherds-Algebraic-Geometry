\input{template.tex}
\usepackage{tikz-cd} 
\usepackage{graphicx}
\graphicspath{ {./images/} }
\newcounter{Chapcounter}
%\newcommand\showmycounter{\addtocounter{Chapcounter}{1}\themycounter}
\newcommand{\lecture}[1] 
{ {\newpage
  \phantomsection %This is too make sure that the toc points to the correct page. Make sure the this doens't break.
  \newpage
  \addtocounter{Chapcounter}{1} \Large \underline{\textbf{ \color{Sepia} Lecture \theChapcounter: #1}} }   
  \addcontentsline{toc}{section}{ \color{Sepia} Lecture:~\theChapcounter~~#1}    
}
\usepackage{eso-pic}

\newlist{Properties}{enumerate}{2}
\setlist[Properties]{label=Property \arabic*.,itemindent=*}


%\pagecolor[rgb]{0.98, 0.96, 0.89}

%\usepackage[pages=some]{background}

%\backgroundsetup{
%scale=1,
%color=black,
%opacity=0.4,
%angle=0,
%contents={%
%  \includegraphics[width=\paperwidth,height=\paperheight]{scroll-background}
%  }%
%}

\pagecolor[rgb]{0.94,0.86,0.69} %Trying to get that old paper look. 
\color[rgb]{0.2,0.2,0.2} %Trying to get a more faded text look.

\newcommand{\verteq}{\rotatebox{90}{$\,=$}}
\newcommand{\equalto}[2]{\underset{\scriptstyle\overset{\mkern4mu\verteq}{#2}}{#1}}
\newcommand{\bverteq}[2]{\underbrace{#1}_{\scriptstyle\overset{\mkern4mu\verteq}{#2}}}


\author{Rindra Razafy, ``Hagamena''}
\title{Borcherd Algebraic Geometry 1}
\setcounter{section}{-1}
\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Prologue}
Some quick notes summarising the ``Algebraic geometry 1'' video lectures from R.E. Borcherds, found \href{https://youtube.com/playlist?list=PL8yHsr3EFj53j51FG6wCbQKjBgpjKa5PX}{here}.


\lecture{Preliminaries}
\section{Introduction}
\subsection{Examples}
\subsubsection{Pythagorean triangles}
\textbf{Problem:} How do we classify all Pythagorean triangles.

We will look at two ways of solving this:\begin{enumerate}
    \item \textbf{Algebraic way:} 
    We want to solve\begin{equation}
        x^2+y^2 = z^2 \text{ with }x,y,z\text{ coprime integers}
    \end{equation}
    
    If we look at the equation mod $4$ we notice that $x^2,y^2,z^2 \equiv 0,1 \mod 4$, since the squares mod $4$ all take these forms.
    
    So $z$ is odd and WLOG we assume that $x$ is even and $y$ is odd. 
    We rearange the equation:\begin{equation}
        y^2 = z^2-x^2 = (z-x)(z+x)
    \end{equation}
    Assume that $z-x = dm_1$ and $z+x = dm_2$, therefore we have that $2z = d(m_1+m_2)$ and $2x = d(m_2-m_1)$, then since $d\mid 2z$ and $d\mid 2x$, and $\gcd(x,z) = 1$ we have two cases, either $d$ divides both $x$ and $z$, which would imply that $d=1$.
    
    Or $d$ divides $2$ which means that $d = 1$, or $d=2$. But note that since $x,z$ are of opposite parity $z+x$ is odd so $d\neq 2$.
    
    So in all cases, $d = 1$. So $(z-x)$ and $(z+x)$ are coprime.
    
    But since their product is a square this implies that $z-x$ and $z+x$ are squares, so:\begin{equation}
        z-x = r^2, \text{ and }z+x = s^2, \text{ where }s,r\text{ are odd and coprime}
    \end{equation}
    
    So we conclude that $z = \frac{r^2+s^2}{2}$, $x = \frac{s^2-r^2}{2}, y=rs $ for any $r,s$ odd and coprime.
    
    \item \textbf{Geometric solution}
    Let $X = \frac{x}{z}$, $Y = \frac{y}{z}$ and we want to solve\begin{equation}
        X^2+Y^2=1, \ X,Y\text{rational}
    \end{equation}
    
    So we are looking for rational points on the unit circle.
    
    Note if we draw the line from $(-1,0)$ to $(X,Y)$ on the unit circle with $X,T\in \Q$.It will intersect the y-axis at the point $(0,t)$ where $t=\frac{Y}{X+1}\in \Q$.
    
    Conversely, if we are given $t$ we can find $(X,Y)$, since we know that\begin{equation*}
        Y=t(X+1) \text{ and }t^2{(X+1)}^2 + X^2 = 1 \Rightarrow (X+1)((t^2+1)X+t^2-1) = 0
    \end{equation*}
    And finding roots we see that $X = \frac{1-t^2}{1+t^2}$ and $Y=\frac{2t}{1+t^2}$, for $t\in \Q$.
    
    \
    
    So there is a correspondence between points on the circle except for the point at $(-1,0)$ and points on the $y-$axis. This is what is called a Birational Equivalence.
    
    \begin{definition}
        \textbf{Birational Equivalence}
        An equivalence excepts on subsets of co-dimension at least $1$.
    \end{definition}
\end{enumerate}
Treating this problem as a geometrical problem gives us additional insights. Indeed, for example the circle forms a group of rotations with operation:\begin{equation}
    (x_1,y_1)\times (x_2,y_2) = (x_1x_2-y_1y_2,x_1y_2+x_2y_1)
\end{equation}

This is the cosine and sign of the sum of two angles, indeed if $(x_1,y_1) = (\cos\theta_1,\sin\theta_1)\text{ and }(x_2,y_2) = (\cos\theta_2,\sin\theta_2)$ then:\begin{equation}
 (\cos\theta_1,\sin\theta_1)\times (\cos\theta_2,\sin\theta_2) = (\cos\theta_1\cos\theta_2-\sin\theta_1\sin\theta_2, \dots) = (\cos(\theta_1+\theta_2),\sin(\theta_1+\theta_2))
\end{equation}
This is the simplest example of what is called an Algebraic group.

\begin{definition}
    \textbf{Algebraic Groups} We can think of this as functor from (commutative) Rings to Groups.

    \begin{equation}
        G\colon R\rightarrow (\{(x,y)\in R^2\mid x^2+y^2=1\},\times)
     \end{equation}
     Where the operation is defined as above, and the identity is $(1,0)$ and ${(x,y)}^{-1} = (x,-y)$.
     
\end{definition}

\begin{example}
    $G(\C) = \{(x,y)\in\C \mid x^2+y^2 = 1\}$
\end{example} 

But note that $1 = x^2+y^2 = {\underbrace{(x+iy)}_z}{\underbrace{(x-iy)}_{\overline{z}}}$. So we see that \begin{equation}
    G(\C) = \{(x,y)\in\C \mid x^2+y^2 = 1\}\simeq \{z\in \C \mid z\text{ is invertible}\} = \C^\ast
\end{equation}

\textbf{Summary}
There are many ways to view a circle:\begin{enumerate}
    \item Subset of $\R^2$
    \item Polynomial $x^2+y^2-1 \rightarrow$   Algebraic set
    \item Ideal $(x^2+y^2-1)$ in ring $\R[x,y]$.
    \item Ring $\R[x,y]/(x^2+y^2-1) = $ coordinate ring of $S^1$. Can be seen as the set of polynomials on the circle.
    \item (Smooth) manifold
    \item Group (Algebraic Group)
    \item Functor from Rings to Groups or Sets (Grothendieck)
\end{enumerate}

\section{Two cubic curves}
In this section we will discuss some cubic cubes.\begin{enumerate}
    \item $y^2 = x^3+x^2$
    
There is almost a 1-to-1 correspondence between $(x,y)$ rational on this curve and $t\in \Q$, via $t = \frac{y}{x}$, the slope of the line through $(x,y)$ and the origin. 
Indeed since $y=tx$, if $x\neq 0$, we have:\begin{equation}
    t^2x^2 = x^3+x^2 \Rightarrow t^2 = 1+x \Rightarrow x=t^2-1 \text{ and } y=t^3-t 
\end{equation}

We don't quite get a 1--1 correspondence because $t=1$ and $t=-1$ both correspond to $(x,y) = (0,0)$. 

So we can think of this cubic curve as a copy of $\Q$, but two of these points are mapped to the same point.

\begin{definition}
    Resolution of Singluarity
    A singularity is a ``bad'' point of our curve, and a resolution is getting a ``nice'' map from a curve without singularities to our curve.    
\end{definition} 

\

The resolution in the above case is done by a process called ``blowing-up''.
\begin{remark}
    Hironaka, showed that blowing-up resolves singularites in zero characteristic. (The problem in non-zero characteristic is still unsolved).
\end{remark}
\begin{remark}
    Finding rational points on curves can be difficult. For example:\begin{equation}
        x^n + y^n = 1 \Rightarrow X^n+Y^n=Z^n \text{ where }x=X/Z \text{ and }y=Y/Z
    \end{equation}
    This is Fermat's Last Theorem, which was very hard to solve.        
\end{remark}

\item $x^3+y^3 = 9$

Note on this curve we can define an algebraic operation ``+'', if we add in a point at infinity.
In that case, the point at infinity is the identity ``0'', and $a,b,c$ on the curve lie on a line if and only if $a+b+c = 0$ in the group. To check that the group operation is associative we use the fact that: $a_1+a_2+\cdots = b_1+b_2+\dots \iff $there is a rational function with poles at $a_i$ and zeroes at the $b_i$.

\begin{definition}
    Groups of this kind are called \textbf{elliptic curves}, there are the 1-dimensional case of what is called \textbf{Abelian varieties}. 
    Abelian varieties are algebraic groups that are ``projective'', roughly they have no missing points.
\end{definition} 

\end{enumerate}

\section{Bézout, Pappus, Pascal}
\subsection{Bézout's theorem}
\begin{theorem} \textbf{Bézout}
    Informally: Two curves of degree $m,n$ in the plane have at most $mn$ intersection, if they have no components in common.    

    \textbf{Stronger version of Bézout}
    There have exactly $mn$ intersection points if:\begin{enumerate}
        \item We are working over $\C$
        \item Count points at infinity
        \item Counting multiplicities (for example a straight line tangent to a parabola, we need to count the intersection point as two points).
    \end{enumerate}
\end{theorem}
This theorem was originally stated by Newton, though he didn't really prove it.

\

It is actually quite difficult to make sense of multiplicities.

\begin{proof}
    Informal proof: Suppose the curves are $f(x,y) = 0$ of degree $m$ and $g(x,y) = 0$ of degree $n$.

    Perturb $f,g$ so that $f = p_1\dots p_m$ and $g = q_1\dots q_n$, with $p_i.q_j$ linear.

    Problem: How do we know the number of intersection points doesn't change as we perturb $f,g$
\end{proof}

This style of proof was very common in the Italian School of Algebraic Geometry, but with these informal reasonings caused them to introduce many false theorems.

Weil and Zariski put Algebraic Geometry on much firmer foundations, but the proofs became much more complicated. 

Nowadays, these informal proofs are mostly useful just to guess what the right answer is (for example understanding the reasoning for the above proof, we can understand the reasoning for 
the anologue of Bézout's theomre in higher dimensions).


\subsection{Pappus' theorem}

\begin{theorem}
    Informally: Take two straight lines in the plane and on each line choose any three points.
Number them and join them to every point of a different number on the other line, looking at the intersection point of these lines.
These three intersection points lie on a straight line.
\end{theorem}

This theorem is equivalent to commutativity in multiplication. If we look at the anologuous result in a plane over a division ring then:\begin{equation*}
    \text{Pappus' theorem is true }\iff \text{ The division ring is a field (so multiplication commutes)}
\end{equation*}

\subsection{Pascal's theorem}
\begin{theorem}
    Informally: Choose any six points on an ellipse, seperate your ellipse into two, so that three points lie on one side and three points lie on the other side. Number the points on the first side as 1,2,3 and likewise for the points on the other side. 
    then join them to every point of a different number on the opposite side, looking at the intersection point of these lines.
    These three intersection points lie on a straight line.
\end{theorem}

    \href{https://youtu.be/-9qugwEZDJs?t=955}{\includegraphics[scale = 0.25]{pascal_theorem_pic}}

    \

    \textit{Illustration from the lecture video}

    \

The line on which their intersection lies is called the \textbf{Pascal line}.

\

Pappus' theorem is a degenerate case of Pascal's theorem, Pascal's theorm holds for any degree two curve and two straight lines are a degenerate case of a degree 2 curve.

How do we prove Pascal's theorem? We will use a proof using Algebraic Geometry and Bézout's theorem. 

\begin{proof}
    We number the lines as in the picture above (noting that they form a funny kind of hexagon) and choose six linear polynomials, 
    $p_i$, for $i\in \{1,\dots, 6\}$ where $p_i = 0$ on line $i$.

    Now look at $p_1p_3p_5$ and $p_2p_4p_6$, these polynomials vanish on all six points, so choose $\lambda$ such that:\begin{equation}
        p_1p_3p_5 - \lambda p_2p_4p_6 \text{ this is of degree 3 curve}
    \end{equation}
    Vanishes on a seventh point of the conic. Since the conic is of degree $2$, by Bézout there are at most $6$ intersection points 
    UNLESS they have a common component. So the conic must be contained in the degree $3$ curve.

    So this degree $3$ curve is equal to the union of a conic and a line, which is Pascal's line. Indeed since $p_1p_3p_5$ and $p_2p_4p_6$ both vanish on the 
    three intersection points, since they are on the curve but not on the conic, they must be on the line.
\end{proof}

\section{Kakeya sets}

We will continue to look at examples from Algebraic Geometry. Kakeya Sets are constructs from real analysis.

\begin{definition}
    The first definition of a \textbf{Kakeya set} is a set such that if you have a unit line in the set we can turn the line around
    in the set, for e.g. A circle, or an equilateral triange.

    Slight variation of the defintion, is that it is a set contianing a unit line in every direction.
\end{definition}

    A Kakeya set over a finite field F is a set that contains a line in every direction. A conjecture from T.Wolff:

    \begin{center}
    The size  of a Kakeya set over $F$ in $F^n$ is at least $c_n|F|^n$.
\end{center}
    
    This was proved in 2008 by Dvir with $c_n = \frac{1}{n!}$.
    
    The proof is in two steps:\begin{enumerate}
        \item  A Kakeya Set in $F^n$ cannot lie in a hypersurface of degree $d<|F|$.
        \begin{proof}
            Suppose $f$ is a polynomial of degree $d<|F|$ defining a hypersurface which is a Kakeya Set, and let $f_d$ be the highest degree component.
            Note that for any $v$ we can find $x$ so that $f(x+vt)$ vanishes for all $t$. This is what is meant by the zeroes of $f$ are a Kakeya set.
            For any direction $v$ we can find a line such that $f$ vanishes on that line.

            \

            So coefficient $f_d(v)$ of $t^d$ vanishes for any $v$, so $f_d$ has degree < $|F|$, since if $f_d$ was non-zero it would have at most $< |F|$ zeroes
            we must have that $f_d = 0$. So $f = 0$.
        \end{proof} 

        \item Observe, the polynomials of degree at most $|F|-1$ form a vector space of dimension $\binom{n+|F|-1}{n}$.

        So we can find hypersurface of degree at most $|F|-1$ vanishing on any set with less than $\binom{n+|F|-1}{n}$ points.
    \end{enumerate}

So a Kakeya set has at least $\binom{n+|F|-1}{n}$ points, but \begin{equation*}
    \binom{n+|F|-1}{n} = \frac{|F|(|F|+1)\cdots (|F|+n-1)}{1\cdot 2 \cdots n} \geq \frac{|F|^n}{n!}
\end{equation*}
    
\begin{example}
    27 lines on a cubic surface.

We will prove that the cubic surface:\begin{equation*}
    w^3+x^3+y^3+z^3 = 0 \text{ in }\mathbb{P}^3
\end{equation*}
Has exactly 27 lines on it.

\

Note $(w\colon x\colon y\colon z)\in \mathbb{P}^3$ then $(w\colon x\colon y\colon z) = (\lambda w\colon \lambda x\colon \lambda y\colon \lambda z)$, for all $\lambda\neq 0$.


Note there is an obvious line:\begin{equation*}
    (a\colon -a \colon b \colon -b) \text{ since }a^3+(-a)^3+b^3+(-b)^3 = 0
\end{equation*}

Note we can permute the coordinates and we can multiply by $\omega$ such that $\omega^3 = 1$.

This gives us $3\times 3 \times 3 = 27$ possibilities.

\end{example} 

\lecture{Affine varieties}
\section{Affine space and Zariski topology}

\subsection{Affine space}

\begin{definition}
    Let $k$ be any field (most commononly taken as $\C$, $\R$ or a finite field), then an \textbf{Affine space} is just $k^n$ as a vector space, with slightly different automorphism group.
    \begin{itemize}
        \item automorphism of a vector space is:\begin{equation*}
            GL_n(k) = \{n\times n\text{ matrices such that }\det \neq 0\}
        \end{equation*} 
        \item automorphism of Affine space is:\begin{equation*}
            GL_n(k) \text{ and } \{\text{translations, i.e a map }x\rightarrow x+v\text{ for some vector }v\}
        \end{equation*}

        If $n=2$ the group can be pictured of as the group of matrices of the following shape:
        \[\begin{bmatrix}
            \ast_1 & \ast_1 & \ast_2\\
            \ast_1 & \ast_1 & \ast_2\\
            0 & 0 & 1
        \end{bmatrix}\]

        Where $\begin{bmatrix}
            \ast_1 & \ast_1 \\
            \ast_1 & \ast_1 
        \end{bmatrix}\in GL_n(k)$ and $\begin{bmatrix}
             \ast_2\\
             \ast_2
        \end{bmatrix}$ corresponds to a translation.
    \end{itemize}
    We write an affine space as $\mathbb{A}^n$.

    Roughly speaking an affine space is a vector space where we have ``forgotten'' what our origin is.

    If we have a vector space we can get an affine space by ``forgotting'' $0$, and if we have an affine space we can choose any point to be the origin and it gives us a vector space.
\end{definition}

\begin{example}
    Note that if we look at the universe, the $3D$ space we live in is an affine space as there is no natural way to choose the origin.
    But if we choose the origin to be the center of the earth then $3D$ space becomes a vector space.
\end{example}

\subsubsection{Affine geometry}
\begin{definition}
    \textbf{Affine geometry} can be thought as the study of affine space that is invariant under translations and linear transformations.
\end{definition}

\paragraph*{Properties of affine geometry}

\begin{itemize}
    \item points
    \item lines
    \item parallel lines
    \item conics
    \item Polynomial functions
\end{itemize}

Are all well-defined in affine geometry.

\paragraph*{Not affine geometry}
\begin{itemize}
    \item circles
    \item angles
    \item lengths
\end{itemize}

\paragraph*{Coordinate ring of $\mathbb{A}^n$}
Algebraic geometry tends to use the coordinate ring of affine space.\begin{definition}
    The coordinate ring of $\mathbb{A}^n$, is just the space of polynomials on $\mathbb{A}^n$. Where $k$ is infinite.
    If we got affine space we can reconstruct the ring of polynomials on it. Conversely if we are given a polynomial ring over $k$ we can reconstruct affine space as:\begin{align*}
        \mathbb{A}^n &= \{\text{homomorphism from }k[x_1,\dots,x_n] \rightarrow k \text{ (as a k-algebra)} \}\\ 
    \end{align*}
    
    Indeed a homomorphism taking $k[x_1,\dots,x_n]\rightarrow k$, just takes $x_i\rightarrow a_i$ for some $a_i\in k$. This corresponds to the point $(a_1,\dots,a_n)\in \mathbb{A}^n$.    
\end{definition}

Because of this the study of affine space is more or less equivalent to the study of this polynomial ring. In particular the automorphism group of these two are the same.

\subsection{Zariski topology}
\begin{definition}
    An \textbf{algebraic set} is a set of zeros of some set of polynomials in $k[x_1,\dots,x_n]$. 
\end{definition}

\begin{example}
    If $f(x,y) = x^2+y^2-1$, then our algebric set is the circle. 
\end{example}
\begin{example}
    If $f(x) = x-a$ and $g(y) = y-b$, the our algebraic set is the point $(a,b)$.
\end{example}

Algebraic sets are closed under these operations:\begin{itemize}
    \item Intersection: Indeed if $C_1,C_2,\dots$ are the zero sets of $P_1,P_2,\dots$ then $\bigcap C_i$ are the zeroes of $\bigcup P_i$.
    \item Finite unions: If $C_1,C_2$ are the zeroes of $\{f_1,f_2,\dots\}$ and $\{g_1,\dots\}$ respectively, then $C_1\cup C_2$ are the zeroes of $\{{f_i}{g_j}\}$
    \item Clear that $\mathbb{A}^n$ and $\emptyset$ are algebraic sets, taking the zero set of $0$ and of a constant non-zero polynomial respectively.
\end{itemize}

With these properties we see that we can create a topology where the closed sets are the algebraic sets. We call this topology the \textbf{Zariski topology}. 

\begin{example}
    Take ${\mathbb{A}^1}$, this is just the line. The closed sets:\begin{itemize}
        \item $\mathbb{A}^1$, the zero set of $0$.
        \item Any finite sets, since $\{a_1,\dots,a_n\}$ is the zero set of $(x-a_1)\cdots (x-a_n)$
    \end{itemize}

These are the only closed sets, since a polynomial in one variable is either the zero polynomial or has a finite amount of roots.


Take any points $x\neq y$ and nbhs $U$ of $x$ and $V$ of $y$. Since $U^c$ and $V^c$ are finite (note if they are infinite then $U$ or $V$ is the empty set which is impossible). So $U$ and $V$ both contain all the points of $\mathbb{A}^1$
except for a finite number of points. Since $k$ is infinite this means that $U\cap V \neq \emptyset$. This is true for all nbhs of $x$ and $y$, which means this space is not Haussdorff.
\end{example}

\begin{example}
    Take $\mathbb{A}^2$. The closed sets are:\begin{itemize}
        \item Points $(a,b)$
        \item Any curve, the set of zero of $f(x,y) = 0$
        \item Union of finite amount of curves and points
    \end{itemize}

    Note $\mathbb{A}^2\neq \mathbb{A}^1\times \mathbb{A}^1$. Indeed since $\mathbb{A}^1\times \mathbb{A}^1$ are unions of finite amount of vertical lines, horizontal lines and points.
\end{example}

\begin{example}
    \textbf{\textit{Determinantal variety:}}
Take $\mathbb{A}^{mn} = \text{linear maps }k^m\rightarrow k^n = m\times n \text{ matrices}$

Determinantal variety = set all linear maps of rank $\leq i$.

\

We claim that this is an algebraic set. Indeed this set is given by the vanishing of all $(i+1)\times (i+1)$ minors of $m\times n$ matrix. This is a set of polynomials.

In partucular the subset of maps from $k^m\rightarrow k^n$ that are onto is open in the Zariski topology.
\end{example}

\section{Noetherian spaces and Noetherian Rings}

\subsection{Noetherian rings}
\begin{definition}
    A \textbf{Noetherian ring} is a ring satisfying these three equivalent conditions:\begin{itemize}
        \item Every ideal is finitely generated
        \item Every nonempty set of ideals has a maximal element
        \item Every chain of increasing ideals,$I_0\subseteq I_1\subseteq I_2\subseteq \dots$, is eventaully constant. I.e. there is a $n$ such that $I_n=I_m$ for all $m\geq n$. 
    \end{itemize}
\end{definition}
\begin{theorem} \textbf{\textit{Noether}}

    \

    If $R$ is Noetherian then $R[x]$ is Noetherian.
    \begin{proof}
        Let $I$ be an ideal of $R[x]$ and look at the chain $I_0\subseteq I_1\subseteq I_2\subseteq \dots$, of $R$ where:\begin{equation*}
            I_n = \text{leading coeffs of polynomials of degree }\leq n\text{ in I}
        \end{equation*}
Since $R$ is Noetherian this stabilises, so $I_N=I_{N+1}=\dots$, for some $N$.

Take the set of polynomials $s_0,s_1,\dots,s_N$ where:\begin{equation*}
    s_i= \text{ degree }i\text{ polynomials whose leading coefficients generate }I_i, \text{ for }i=1,\dots,N
\end{equation*}
Note the sets $s_i$ are finite since $R$ is Noetherian.


Then $s_0,s_1,\dots,s_N$ generate the ideal $I$.
    \end{proof}
\end{theorem}
\begin{corollary} \textbf{\textit{Hilbert}}

    \

    $k[x_1,\dots,x_n]$ is Noetherian

    \begin{proof}
        Since $k$ is a field any ideal in $k$ is either generated by $0$ or generated by $1$. So $k$ is Noetherian, so $k[x_1]$ is Noetherian.
        So inductively we can see that $k[x_1,\dots,x_n]$ is Noetherian.
    \end{proof}
\end{corollary}

\subsection{Noetherian spaces}

\begin{definition}
    A topological space is called \textbf{Noetherian} if equivalently:\begin{itemize}
        \item The closed sets satisfy the descsending chain condition. So any decreasing sequence:\begin{equation*}
            C_0\supseteq C_1\supseteq C_2\supseteq \dots
        \end{equation*}
        Stabilises, i.e. $C_n=C_{n+1}=\dots$, for some $n$
        \item Any nonempty collection of closed sets has a minimal element.
    \end{itemize}
\end{definition}
These are in some sense the dual of the definition of Noetherian Ring.

\begin{theorem}
    $\mathbb{A}^n$ with Zariski topology is Noetherian.

    \begin{proof}
        Sketch:

        Closed sets of $\mathbb{A}^n$ correspond to some ideals of $k[x_1,\dots,x_n]$. A descsending chains of closed sets of $\mathbb{A}^n$ correspond to an ascending chain of ideals in $k[x_1,\dots,x_n]$
    \end{proof}
\end{theorem}

\

Noetherian spaces are ``WEIRD'', the Noetherian condition is equivalent to saying that every open set is compact or quasicompact. Note quasicompact actually means the same as the regualar defintion of compact, Bourbaki made a mistake in the defintion and 
only considered Haussdorff compact spaces to be compact, so when non-Haussdorff compact spaces were seen to be important they had to use the term quasicompact.

\

\paragraph*{Borcherds' Rule of Thumb} If we see the word ``quasi'' in mathematics then someone, somewhere and somewhen screwed up the terminology and had to use the term ``quasi'' to fix it.

\

\begin{remark}
    In analysis we almost never see open compact sets, it can be shown that if a space is Noetherian and Haussdorff then it is finite.
\end{remark}

\subsection{A first definition of Algebraic Varieties}

\subsubsection{Irreducible sets}

\begin{definition}
    A set is called \textbf{irreducible} if and only if it is nonempty and not the union of $2$ proper closed subsets.
\end{definition}

\begin{definition}
    \textbf{Noetherian Induction}, pick a maximal closed set of some collection of closed sets.
\end{definition}

\begin{theorem}
    Any Noetherian space is a finite union of irreducible subspaces.

    \begin{proof}
        Proof by Noetherian Induction:

        We will show that every closed subset is a finite union of irreducibles. If not, pick a minimal counterexample $C$. We have two cases\begin{enumerate}
            \item If $C$ is irreducible, then we are done and we have found a contradiction
            \item If $C$ is not irreducible, then we can write $C=C_1\cup C_2$, where $C_1,C_2$ are smaller. By induction, $C_1,C_2$ are a finite union of irreducible sets, and so is $C$. Which is a contradiction.
        \end{enumerate}

        In all cases we have a contradiction so, there can't be a closed set that is not a finite union of irreducibles. So all closed sets, in particular the whole space is a finite union of irreducibles.
    \end{proof}
\end{theorem}

So we can reduce the study of Noetherian spaces to the study of irreducible Noetherian spaces.


\

\begin{corollary}
    Every algebraic set is a finite union of irreducible algebraic sets.
\end{corollary}

\begin{definition}
    \textbf{A provisional definition of Algebraic Varieties}:
    
    They are irreducible closed subset of affine space.
\end{definition}

\subsubsection{Examples}

\begin{example}
  This definition is not perfect. Suppose we take the variety given by: $xy=1$ and the set of nonzero points in $\mathbb{A}^1$.

Note the set of nonzero points in $\mathbb{A}^1$ is not a closed set, but since we can map the hyperbola to this by the mapping $(x,y)\rightarrow x$, we should consider it an algebraic variety.

\

\href{https://youtu.be/D_eJ8BWLb24?t=1110}{\includegraphics[scale = 0.25]{algebraic-varieties.png}}

\

We shall give a better definition later (I guess we can clook at these like ``quasi-algebraic varieties'').
\end{example}

\begin{example}
    Take the algebraic set defined by $x^2+y^2 - 2z^2 = 0 \text{ and }2x^2-y^2-z^2 = 0$.

    This is the union of four irreducible subsets:\begin{enumerate}
        \item $x=y=z$
        \item $x=0y=z$
        \item $x=-y=-z$
        \item $x=y=-z$
    \end{enumerate}

    So the intersection of irreducible sets may not be irreducible.
\end{example}
\newpage 
\begin{example}
    If we take $xy=0$, we have the union of the $x$-axis and the $y$-axis.

    \

    If we take $xy=1$, the we hyperbola, which is irreducible and connected in the Zariski topology, but disconnected in the usual topology.

    \

\href{https://youtu.be/D_eJ8BWLb24?t=1295}{\includegraphics[scale=0.25]{connected-irreducible-spaces}}

\end{example}

We have now conclueded the section on Noetherian spaces, next section we will look at the Hilbert Nullstellensatz, and the connection between Algebraic Varieties and Ideals.


\section{Hilbert's Nullstellensatz}
\begin{definition}
    \textbf{Hilbert's Nullstellensatz}(zero's theorem) describes the relation between Ideals in a polynomial ring and Algebraic subsetes of the corresponding affine sets.

    \begin{itemize}
        \item If we have a subset $Y\subseteq \mathbb{A}^n$ we can map it to the ideal $I(Y)\subseteq k[x_1,\dots,x_n]$ where $I(Y)$ is the set of polynomials vanishing on $Y$.
        \item If we have an ideal $\mathfrak{a}\subseteq k[x_1,\dots,x_n]$ we can map it top the set $Z(\mathfrak{a})$ which is the set of zeros of all the polynomials of $\mathfrak{a}$.
    \end{itemize}
\end{definition}
\paragraph*{What is the relationship between these two operations?}
\begin{align*}
    Z(I(Y)) = \text{closure of }Y \text{ in the Zariski topology}
\end{align*}

This fact basically comes from the definiton of the Zariski topology, let $W$ be any closed set contianing $Y$. We know that $W$ is the zero set of some set of polynomials $S$. For all $f\in S$ we have:\begin{equation*}
    f(x) = 0\text{ for all }x\in Y
\end{equation*}
Therefore $S\subseteq I(Y)$, therefore for all $f\in S$ and we have by definition of $Z(\cdot)$:\begin{equation*}
    f(y) = 0 \text{ for all }y\in Z(I(Y))
\end{equation*}

So $y\in Z(S)$, therefore $Z(I(Y))\subseteq W = Z(S)$. Since $Z(I(Y))$ is the smallest closed set containing $Y$ we indeed see that it is the closure.

\

On the other hand is it true that $I(Z(\mathfrak{a})) = \mathfrak{a}$ for all ideal $\mathfrak{a}\subseteq k[x_1,\dots,x_n]$? \textbf{NO}:\begin{example}
    Let $\mathfrak{a} = (x^2)\subseteq k[x]$, and $Z(\mathfrak{a}) = 0$ so $I(Z(\mathfrak{a})) = I(0) = (x)$.
\end{example}

More generally, if $f^n\in \mathfrak{a}$, then $f\in I(Z(\mathfrak{a}))$, since certainly $f$ vanishes at all points of $Z(\mathfrak{a})$ since $f^n$ does.

Therefore \begin{equation*}
    \{f\in k[x_1,\dots,x_n]\mid f^n\in \mathfrak{a}, \text{ for some }n\in\N^\ast\} = \sqrt{\mathfrak{a}} \subseteq I(Z(\mathfrak{a}))
\end{equation*}

Note $\sqrt{\mathfrak{a}}$ is an ideal.


\paragraph*{Is $\sqrt{a} = I(Z(\mathfrak{a}))$?}

\textbf{NO!}

\begin{example}
    If $\mathfrak{a} = (x^2+1)\in \R[x]$, then $Z(\mathfrak{a}) = \emptyset$, and $I(Z(\mathfrak{a})) = \R[x] \neq \sqrt{(x^2+1)} = (x^2+1)$
\end{example}

\subsection{Weak Nullstellensatz}
\paragraph*{What are the maximal ideals of a polynomial ring $k[x_1m\dots,x_n]$}

There are Obvious maximal ideals:

Let $(a_1,\dots,a_n)\in \mathbb{A}^n$ and take the ideal $(x-a_1,x-a_2,\dots,x-a_n) = $functions vanishing on $(a_1,\dots,a_n)$.

\

Note that \begin{equation*}
    k[x_1,\dots,x_n]/(x-a_1,x-a_2,\dots,x-a_n) \simeq k
\end{equation*}

So this is a maximal ideal. Are all ideals of this form? Not in general, if we take $\R[x]$ then $(x^2+1)$ is a maximal ideal, since $\R[x]/(x^2+1) \simeq \C$.

But this is true if $k$ is \textbf{Algebraically closed!} This is the weak Nullstellensatz.

\begin{theorem} \textbf{Weak Nullstellensatz}
For $k$ algebraically closed all maximal ideals of $k[x_1\dots,x_n]$ are of the form $(x_1-a_1,\dots,x_n-a_n)$.
    \begin{proof}
        \begin{lemma}
            If $K$ is a field an is a finitely generated algebra over $k$, then $K$ is a finitely generated module over $k$.
            \begin{proof}
                We are going to cheat: We will assume that $k$ is uncountable. (If $k$ is countable the proof is harder).

                Since $K$ is a finitely generated algebra, then $K$ is at most countable dimension as a module. If $x\in K$ is transcendental over $k$, if we had, for some $a_1,\dots,a_n\in k$ such that  $\sum_{k=1}^n \frac{a_k}{x-a_k} = 0 \Rightarrow \sum_{k=1}^n a_k\prod_{i\neq k}(x-a_j) = 0$, which is impossible since $x$ is transcendental.
                
                \
                
                So the elements $\frac{1}{x-a}$ for $a\in k$ form an uncountable linearly independant set, which is a contradiction to the fact that $K$ is a finitely generated algebra over $k$.

                \

                So all $x\in K$ are algebraic over $k$, therefore $K$ is finitely generated as a module over $k$.
            \end{proof}
        \end{lemma}
        Suppose that $I$ is a maximal ideal of $k[x_1,\dots,x_n]$ we want to show that $I = (x-a_1,\dots,x-a_n)$ for some $(a_1,a_2,\dots)$.

        Let $K = k[x_1,\dots,x_n]/I$, then $K$ is a field and is finitely generated as an algebra. So by the previous lemma $K$ is finietely generated as a module. In other words $K$ is algebraic over $k$,

        But since we assumed that $k$ is algebraically closed we have $k=K$.

        Therefore, $x_i+I\in k$ for all $i$ so $x_i+I=a_i+I$ for some $a_i\in k$ so $x_i-a_i\in I$. So $(x_1-a_1,\dots,x_n-a_n)\subseteq I$ so $I = (x_1-a_1,\dots,x_n-a_n)$, since $(x_1-a_1,\dots,x_n-a_n)$ is a maximal ideal and $I\not=k[x_1,\dots,x_n]$.
    \end{proof}
\end{theorem}

\subsection{Strong Nullstellensatz}

\paragraph*{Proof of SN with Rabinovitsch trick}

\begin{theorem}\textbf{Strong Nullstellensatz}
    If $k$ is algebraically closed then for all ideals $\mathfrak{a}\subseteq k[x_1,\dots,x_n]$ we have $I(Z(\mathfrak{a})) = \sqrt{a}$.

\begin{proof}
    
Suppose that $\mathfrak{a}$ is generated by elements $f_1,\dots,f_n$ and that $f\in I(Z(\mathfrak{a}))$, we want to show that $f\in\sqrt{\mathfrak{a}}$.

\ 

\textbf{Rabinovitsch idea} is too add an extra variable $x_0$.

\

So $f_1,\dots,f_m,1-x_0f$ have no common zeroes in $\mathfrak{A}^{n+1}$, since if $x$ is a zero of $f_1,\dots,f_n$ then it is a zero of $f$, so $1-x_0f(x) = 1$.

Now apply the weak Nullstellensatz in $\mathbf{A}^{n+1}$, since $f_1,\dots,f_m,1-x_0f$ have no common zeroes, they are not contained in any of the maximal ideals(Since if they are contained in a maximal ideal $(x_1-a_1,\dots,x_n-a_n)$ then $(a_1,\dots,a-n)$ would be a common zero) so they generate the unit ideal.

So there exists $g_i\in k[x_0,x_1,\dots,x_n]$ such that $1 = a_0(1-x_0f)+g_1f_1+\dots+{g_n}{f_n}$

Let $x_0 = \frac{1}{f}$, (we are now working in the ring of rational function) we have:\begin{equation*}
    1 = g_1\bigg(x_1,\dots,x_n,\frac{1}{f}\bigg)f_1+g_2\bigg(x_1,\dots,x_n,\frac{1}{f}\bigg)f_2+\dots+g_n\bigg(x_1,\dots,x_n,\frac{1}{f}\bigg)f_n
\end{equation*}
We can clear denominators by multiplying by a high power of $f$ and we get:\begin{equation*}
    f^n = \sum h_if_i \text{ where }h_i = {f^n}{g_i}\in k[x_1,\dots,x_n]
\end{equation*}
So $f^n\in (f_1.\dots,f_n)$ so $f\in \sqrt{(f_1.\dots,f_n)} = \sqrt{\mathfrak{a}}$.
\end{proof}
\end{theorem}
 \begin{tabular}{|c|c|c|}
    \hline
    Affine space $\mathbb{A}^n$& correspondence & $k[x_1,\dots,x_n]$\\
    \hline
    Points $(a_1,\dots,a_n)$ & $\underbrace{\iff}_{Weak Nullstellensatz}$ & maximal ideals $(x_1,\dots,x_n)$ \\
    \hline
    Algebraic sets  & $\underbrace{\iff}_{Strong Nullstellensatz}$  & Radical ideals $\mathfrak{a} = \sqrt{\mathfrak{a}}$\\
    \hline 
    closed subschemes &$\underbrace{\iff}_{\textit{Will see in second part of course}}$ & All Ideals \\
    \hline
\end{tabular}


\begin{example}

    In this example we have a curve described by the radical ideal $(y-x^2)$ and one given by the radical ideal $(y)$.

    We look at the intersection between the two curves by looking at the zero set of the ideal $(y-x^2,y) = (y,x^2)$. However, this ideal is not a radical ideal, as the root of this ideal is $(x,y)$ which corresponds to the point $(0,0)$ in affine space.

    \

    \href{https://youtu.be/1UvW5iTkbLw?t=772}{\includegraphics[scale = 0.25]{Strong_Null.png}}

    \

    So we have a non-radical ideal that is the intersection of two curves. This ideal is non-radical since if we look at the intersection between the two curves we should be counting the intersection point as two points.
\end{example}

\begin{example}\textit{Nilpotent matrices}

    \begin{definition}
        A matrix $A\in M_n(k)\simeq\mathbb{A}^{n^2}$ is \textbf{Nilpotent} if $A^n = 0$ for some $n\in \N$,
    \end{definition}
 
Notice that if:\begin{equation*}
    A = \begin{bmatrix}
        a_{11} & a_{12} & \dots\\
        a_{21}\\
        \vdots
    \end{bmatrix} \Rightarrow A^n = \begin{bmatrix}
        \text{ something complicated, homogenous polynomials in degree }n\text{ in coeffs }a_{ij}
    \end{bmatrix}
\end{equation*}

The entries of the matrix $A^n$ generate an ideal $I$ in the coordinate ring of $A^{n^2}$, in some sense this ideal describes the set of nilpotent matrices. Is $I = \sqrt{I}$? The answer is NO!

If $A$ is nilpotent, then all eigenvalues are $0$, so the trace is $0$. So $a_{11}+a_{22}+\dots+a_{nn}\not\in I$, but this element is in $\sqrt{I}$ by Hilbert's Nullstellensatz.

\

Let us take $n=1$, let $A = \begin{bmatrix}
    a & b\\
    c & d
\end{bmatrix}$ such that $A^2 = 0$. But $A^2 = \begin{bmatrix}
    a^2+bc & b(a+d)\\
    c(a+d) & d^2+bc
\end{bmatrix}$

So $I = (a^2+bc, b(a+d), c(a+d), d^2+bc)$. We have seen that some power of $\text{tr}(A) = a+d$ is in $I$. What is the smallest power?
An easy assumption is that $(a+d)$ is in $I$, but this is false! The smallest power of $(a+d)$ in $I$ is given for $(a+d)^3$.

\begin{equation*}
    (a+d)^2 = a^2 + 2ad + d^2
\end{equation*}

\begin{equation}
    (a+d)^3 = a^3+3a^2d+3ad^2+d^3
\end{equation}

\end{example}

\paragraph*{What about commuting matrices?}
\begin{example}
    Let $AB=BA$, with $A=(a_{ij}),B=(b_{ij})\in M_n(k)$.

    Let $I$ be the ideal generated by the entries in $AB-BA$, is $I = \sqrt{I}$?

    We don't know this is a hard open problem.
\end{example}

\section{The Lasker Noether Theorem}

\subsection{Some background}

Let us recall that Algebraic sets correspond to radical ideals. Since algebraic sets correspond to finite unions of irreducible algebraic sets, and these sets corresponds to Prime ideals, from this we can conclude the following theorem.

\begin{theorem}
    Any radical ideal is the intersection of finite number of prime ideals.
\end{theorem}

\paragraph*{Question: What about ideals that are not radical? Is there a similar decomposition?}

The answer is given to us by Lasker: Ideals of $k[x_1.\dots,x_n]$ is the intersection of a finite number of primary ideals.

\

Noether generalised this theorem by showing that it is true for Noetherian rings.

\begin{definition}\textbf{Associated Prime}
    Let $M$ be a module over $R$, and \textbf{associated prime }$\mathfrak{p}$ is a prime ideal $\mathfrak{p}$ such that M contains a submodule isomomorphic to $R/\mathfrak{p}$, for brevity we write $R/\mathfrak{p}\subseteq M$.
\end{definition}

\begin{definition}\textbf{Coprimary Module}
    We have two definitions:\begin{itemize}
        \item A module $M$ over $R$ is called \textbf{coprimary} if:
        \begin{equation*} ab = 0, \text{ for }a\in R\text{ and }b\in M, \Rightarrow b=0\text{ or }a^n=0\text{ for some n} \tag{$\dagger$}\end{equation*}
        \item\textit{A more convinent definition} A module $M$ over $R$ is coprimary if it has exactly one associated prime $\mathfrak{p}$.  
    \end{itemize}
\end{definition}

\begin{remark}
    If the ring $R$ is Noetherian, the two definitions of coprimary are equivalent for finitely generated modules.
\end{remark}

\begin{definition}\textbf{Primary ideal}
\begin{itemize}
    \item  \textit{A first definition by Lasker}: $\mathfrak{p}\subseteq R$ is primary if and only if $ab\in \mathfrak{p}$ implies $a\in \mathfrak{p}$ or $b^n\in \mathfrak{p}$ for some $n$.
    \begin{example}
        In $k[x]$: $(x)$ is prime and $(x^n)$ is primary. Note in general primary ideals need not be powers of prime ideals
    \end{example}

    \item \textit{Another definition}: An ideal $\mathfrak{p}\subseteq R$ is primary if and only if the module $R/\mathfrak{p}$ is coprimary
\end{itemize}
\begin{remark}
    In general for a module $M$ over $R$, $N\subseteq M$ is primary $\iff$ $M/N$ is coprimary.
\end{remark}
There are two different definitions for this primary ideal under the other definition but they are equivalent if $R$ is Noetherian.       
\end{definition}

\begin{theorem}\textit{\textbf{Lasker-Noether Theorem for finitely generated modules over Noetherian Rings}}

    Let $M$ be a f.g. module over a Noetherian Ring. Then $0$ an intersection of primary submodules of $M$. 

    Alternatively: $M\subseteq \oplus_{\text{finite}} \text{coprimary modules}$
\end{theorem}
\begin{example}
    For $\Z$-modules (abelian groups): Some example of coprimary modules:\begin{itemize}
        \item $\Z^n$ where the prime ideal is $(0)$
        \item Any finite group of order $p^n$ where the prime ideal is $(p)$
    \end{itemize}
    So our theorem says that a finitely generated abelian group is contained in direct sum of copies of $\Z$ and finite groups of prime power order. If we recall the structure theorem this inclusion is actually an equality.        
\end{example}

\begin{proof}\textit{Lasker-Noether Theorem}
    
    This proof has two steps:\begin{enumerate}
        \item \begin{lemma}
            Any submodule $M$ is an intersection of a finite number of irreducible submodules.

            \begin{proof}
                Noetherian Induction. What does this mean? The submodules of $M$ have the property that any non-empty subset has a maximal element (since $M$ is f.g.  over a Noetherian ring).
            So suppose $N$ is a maximal submodule that is not a finite intersection of irreducibles, so either this submodule is irreducible which is a contradiction since it is an interesection of itself, or it is the intersection of two larger submodules which are by induction a finite intersection of irreducibles, contradiction.
            \end{proof}
        \end{lemma}
        \item \begin{lemma}
            Any irreducible submodule $N$ is primary
        \end{lemma}
        \begin{proof}
            We can reduce to the case where $N$ is $0$ by taking quotients. So we want to show that $(0)$ is irreducible implies that $M$ is coprimary.

            But if $M$ is not coprimary, then there are two associated primes $\mathfrak{p}$, $\mathfrak{q}$ such that $M$ has submodules isomorphic to $R/\mathfrak{p}$ and $R/\mathfrak{q}$.

            These submodules have intersection $0$ which implies that $0$ is not irreducibles.
        \end{proof}
    \end{enumerate}

    Combining the two previous lemmas gives us our proof.
\end{proof}
\begin{example}
    Let us look at the ideal generated by $(xy,y^2)$ in $k[x,y]$.

    The algebraic set is given by the intersection of $V(xy) = x\text{ and }y-axis$ and $V(y^2) = x$-axis (doubled up in some informal sense).

    The intersection of these two sets is just the $x$-axis, whith a sort of ``double point'' near the origin.

    We can understand this ``double point'' by looking at the primary decomposition of the ideal:
    \[(xy,y^2) = (y)\cap \underbrace{(x.y^2)}_{\text{embedded component}}\]

    \href{https://youtu.be/1oKh8QJ1I4k?t=619}{\includegraphics[scale = 0.25]{lasker_noether.png}}

Note this primary decomposition is not unique:\begin{align*}
    (xy,y^2) &= (y)\cap (x,y^2)\\
             &= (y)\cap (x+y,y^2)
\end{align*}
\end{example}

\section{Quotients of varieties by groups}

\subsection{Coordinate rings}
Recall for any algebraic set, $Y$ we can go to it's coordinates ring $k[x_1,\dots,x_n]/I(Y)$, this can be thought of as the ring of all polynomials on the algebraic set $Y$.
Because we are basically ``setting to zero'' all the polynomials which vanish on $Y$, so any two polynomials that are equal on $Y$ are considered the same. So we can find an isomorphism between the ring of polynomials on $Y$ and this coordinate ring by sending $f+I(Y)\rightarrow f|_Y$. This is well definied since if $g+I(Y) = f+I(Y)$, then since $g-f\in I(Y)$, $g(\bar{x}) = f(\bar{x})$ for $\bar{x}\in Y \Rightarrow f|_Y = g|_Y$.

This is an injection since if $f|_Y = g|_Y$, then $g(\bar{x}) = f(\bar{x})$ for $\bar{x}\in Y$, so $f-g\in I(Y)\Rightarrow f+I(Y) = g+I(Y)$.

Finally let $\omega$ be any polynomial on $Y$, then $\omega(x_1,\dots,x_n) = \sum_{i} {a_i}{m_i(x_1,\dots,x_n)}$, for some monomial $m_i$. So we just need to set $f = \sum_{i} {a_i}{m_i(x_1,\dots,x_n)}\in k[x_1,\dots,x_n]$.

\

This coordinate ring has three properties:\begin{enumerate}
    \item Algebraic over $k$
    \item finitely generated
    \item No nilpotent elements
\end{enumerate}

On the other hand, if we have an algebra with these three properites, then by strong Nullstellensatz it corresponds to an algebraic set which is not unique, since it depends on the generators chosen. But they are isomorphic in some sense.


Informally algebraic sets (up to isomorphism) as being more or less equivalent to algebras with the above properites. In technical terms the category of algebraic sets is equiv to opposite of category of coordinate rings.

\subsection{Application}
Suppose we have an algebraic set $Y$ acted on by a group $G$. Can we form the quotient $Y/G$? This doesn't really work. One of the problems is how do we embed the quotient in affine space? But if we look at coordinate rings this is easier to do, since the functions on $Y/G$ should correspond to the functions on $Y$ invariant under $G$.
\begin{center}
    So we can look at the \textbf{Ring of invariant} ${\bigg(k[x_1,\dots,x_n]/I(Y)\bigg)}^G$.     
\end{center}
So not this is an algebra over $k$ and has no nilpotent elements, but is it finitely generated? The answer is sometimes, but Hilbert proved that in many cases it is finitely generated but some examples of when it isn't finietely generated were found after him.


\begin{example}
    Let $\mathbb{A}^n =$ affine space, and $G  = S_n$ permutations of coordinates. The polynomial ring is ${k[x_1,\dots,x_n]}^{S_n}$ which is generated by the \textbf{elementary invariant polynomial}:\begin{itemize}
        \item $p_1 = x_1+\cdots + x_n$
        \item $p_2 = x_1x_2+\cdots$
        \item $\vdots$
        \item $p_n = x_1\dots x_n$
    \end{itemize}

    So \[\mathbb{A}^n/S_n \simeq \mathbb{A}^n\]

    While this example is easy, in general quotients of affine space by groups are complicated. Most of times we get a nice answer when the group is a reflection group (like $S_n$).
\end{example}

\begin{remark}
    We have to be careful, the quotient might not be what we expect. If we take the real line and have the group $\Z/2\Z$ acting by $x\rightarrow -x$, we might think that the quotient is the half closed interval (which us what we would get if we were identifying points) it isn't the same as what we get in our construction above.

    We get the whole real line back. The positive points are given by identifying $(2,-2)$ but the points in the negative side are given by identifying $(2i,-2i)$, these points look like complex points but they count as real points in the quotient.
\end{remark}

\begin{example}
    Let us look at:\begin{equation}
        GL_n(k)\text{ acting on }k^n \Rightarrow \text{The orbits are }0, \text{ or everything else}
    \end{equation}

    So we might think that the quotient would give us two points, but instead we only have one point since the only invariant polynomials acted on by $GL_n(k)$ are constants. Since we have for all $A\in GL_n(k):$ \[f(A(x_1,\dots,x_n)) = f(x_1,\dots,x_n)\text{ for all }(x_1,\dots,x_n)\in k^n\] 


Then pick any non-zero points $(x_1,\dots,x_n); (y_1,\dots,y_n)$ and we see that $f(x_1,\dots,x_n) = f(A(x_1,\dots,x_n)) = f(y_1.\dots,y_n)$ for some $A\in GL_n(k)$. Since this polynomial is the same at an infinite amount of points, it is the constant polynomial.

So the only invariant polynomials are the constant polynomials, this ring is isomorphic to $k$ which is the coordinate ring of a point.
\end{example}

\begin{example}\textbf{Classical invariant theory}
Let $G = SL_2(\C)$, it acts on ${a_n}x^n + {a_{n-1}}x^{n-1}y+\cdots + a_0y^n$, by:\[\begin{bmatrix}
    a & b\\c&d
\end{bmatrix}\begin{bmatrix}
    x\\y
\end{bmatrix} = \begin{bmatrix}
    ax+by\\cx+dy
\end{bmatrix}\]

So $(a_0,\dots,a_n)\in \mathbb{A}^n$ we can ask what is $\mathbb{A}^{n+1}/SL_2(\C)$? The coordinate ring are all polynomials in $(a_n,a_{n-1},\dots)$ which are invariant under $SL_2(\C)$. These are called invariants and finding them is part of classical invariant theory.

For example $a_2x^2+a_1xy+a_0y^2$, the invariant is the \textbf{discriminant} which is $a_1^2-4({a_0}{a_2})$

\

Is the ring of invariants finitely generated? Paul Gordan (``king of invariant theory'') proved that this was true, but Hilbert gave us an shorter way of proving this, the \textbf{Hilbert finiteness theorem}. 

\end{example}

\subsection{Hilbert's fintieness theorem}

\begin{definition}
    Let $A = k[x_1,\dots,x_m]$ and $G$ be a group acting on $k^n =\text{span}\{x_1,\dots,x_m\}$ so $G$ acts on $A$. Let $A^G = $invariant elements of $A$ and assume that $G$ is finite and $\text{char} k = 0$.
We define the \textbf{Reynolds operator} to be $\rho$, where $\rho(a) = \text{average of }A \text{ under G}$. So for $G$ finite we have\begin{equation}
    \rho(a) = \frac{1}{|G|}\sum_{g\in G}g(a)
\end{equation}
Note we needed the assumptions in our statement to define this number (the sum is defined since $G$ is finite and we can define the average since characteristic of $k$ is zero, so we are not dividing by zero).

This operator has the following properties:\begin{itemize}
\item $\rho(1) = 1$
\item $\rho(a+b) = \rho(a)+\rho(b)$
\item In general $\rho(ab)\neq \rho(a)\rho(b)$, this is not a homomorphism of algebras
\item $\rho(ab) = a\rho(b) = \rho(a)\rho(b), \textit{ if }a=\rho(a)$
\end{itemize}

So $\rho$ is a homomorphism from $A$ to $A^G$ of $A^G$ modules.
\end{definition}
\begin{theorem}
    Let $A = k[x_1,\dots,x_m]$ and $G$ be a group acting on $k^n =\text{span}\{x_1,\dots,x_m\}$ so $G$ acts on $A$. Let $A^G = $invariant elements of $A$.
    $A^G$ is finitely generated as a $k$-algebra if $G$ is finite and $\text{char} k = 0$.
    
    \begin{proof}
        Notice that $A$ is graded by degree:\begin{eqnarray}
            A = A_0\oplus A_1\oplus A_2\dots
        \end{eqnarray}
        Where $A_0 = k$, $x_1,\dots,x_n\in A_1$, etc.,.. This is the usual way of grading a polynomial ring.

        Let $I = $ideal of $A$ generated by the homogenous elements of $A^G$ of degree greater than zero. Notice that $I$ is finitely generated as an ideal, we can assume that it has a finite number of generators in $A^G$ and are homogenous.

        \

        Suppose $i_1,\dots,i_n$ are generators as above for the ideal $I$, we want to sho they genereate the $k$-algebra $A^G$. 

        We show by induction of the degree that if $x\in A^G$ and $x$ is homogenous, then $x$ is in the algebra generated by $i_1,\dots,i_n$. 
        
        This is obvious for degree $0$, so assume that $\deg x>0$. Let $x=a_1i_1+\cdots+{a_k}{i_k}$, for some $a_i\in A$ homogenous. So $x$ is in the ideal generated by $i_1,\ldots,i_n$ but $a_i$ NEED NOT be in $A^G$.

        Applying the Reynolds operator:\[x \underbrace{=}_{x\in A^G} \rho(x) = \rho(a_1)i_1+\rho(a_2)i_2+\ldots\] 
        
        Since $\rho(i_m) = i_m$ (since $i_m\in A^G$).
        


        By properties of the Reynold operator $\rho(a_i)\in A^G$, therefore $x$ is a polynomial in elements of $A^G$ of smaller degree. So $\deg \rho(a_i)<\deg x$ so by induction $a_i$ are polynomials in $i_m$ and so $x$ is also.
    \end{proof}
\end{theorem}

Hilbert didn't just prove this for finite groups but also for more general groups.

\paragraph*{Extensions of this thorem}\begin{enumerate}
    \item For $G$ compact and $k=\R$ or $\C$, the same proof works since we can define a Reynods operator:$\rho(a) = \frac{1}{\text{vol}G}\int_Gg(a)$
    \item $SL_n(\C)$? We use Weyl's unitarian trick. Since $SL_n(\C)\supseteq \underbrace{SU_n(\C)}_{\text{compact}}$. Complex actions of $SL_n(\C)$ on complex vector space $V$ (finite dimension) are the ``same'' as actions of $SU_n(\C)$ on $V$.
\end{enumerate}


\paragraph*{Nagata's Example}
This is an example of group $G$ acting on $k^n$ so that $A^G$ is not finietely general.

First take group $k\simeq \begin{bmatrix}
    1 & \ast\\
    0 & 1
\end{bmatrix}$ acts on $k^2$, (this is a sort of universal counter example to everything called the \textbf{Unipotent action}, where all eigenvalues of all matrices are $1$).

Then take sixteen copies of this so $k^{16}$ acts on $k^{32}$, and $G =$ ``generic'' $13$-dim subspaces of $k^16$.

\section{Three examples of quotients of algebraic sets}
\subsection{Cyclic quotient singularity}

We take $\mathbb{A}^2$ whose coordinate ring is $k[x,y]$. And $G = \Z/n\Z$ generated by $\sigma$ of order $n$, where $\sigma(x) = \zeta x$ and $\sigma(y) = \zeta y$, where $\zeta$ is a $n$-th root of unity.

Now what are the invariants of the coordinate ring over $G$?

Well notice that \begin{equation}
    \sigma({x^i}{y^j}) = {\zeta}^i{i+j}{x^i}{y^j} = {x^i}{y^j} \text{ if and only if }i+j = 0\mod n
\end{equation}
So the ring of invariants has a basis ${x^i}{y^j}$, where $i+j = 0\pmod n$. This ring is generated by $z_0 = x^3, \ z_1 = x^2y, \ z_3 = xy^2, \ z_4 = y^3$, but these elements are not independant, since ${z_i}{z_j} = {z_k}{z_l}$ when $i+j = k+l$

So in reality the ring of invariant is equal to \[k[z_0,\dots,z_4] / ({z_1}{z_2}={z_0}{z_3}, {z_1}^2 - {z_0}{z_2}, {z_2}^2 - {z_1}{z_2} ) \tag{$\dagger$} \]

So the quotient of the affine plane by the cyclic group is an affine variety whose coordinate ring is the $\dagger$


\subsection{Parameter space of cyclohexane}

\begin{definition}
    A\textbf{ parameter space} is some sort of space whose points corresponds to some ``configurations'', say some sort of algebraic subset of an algebraic variety. Like a parameter space of line inside of space, or parameter space of conics in a plane 
\end{definition}

We are going to look at the parameter space of cyclohexane, recall from chemestry cyclohexane is a ring of six carbon atoms and sixteen hydrogen atoms. 

A carbon atoms centre is specified by three number so a carbon atom corresponds to $\mathbb{A}^3$, so six carbon atoms correspond to $\mathbb{A}^{3\cdot 6} = \mathbb{A}^{18}$. However there are some conditions that this need to satisfy, if one carbon atom is $(x_1,y_1,z_1)$ and another is $(x_2,y_2,z_2)$ they must satisfy:\begin{equation}
    (x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2 = c \text{ a constant }
\end{equation}

So $6$ quadrics must be satisfied for the distances and $6$ for the angles. So we have $18$-dimension affine space and the points must satisfy $12$ equations. Subce we can translate and rotate our cyclohexane then we can guess that the parameter space is taken by:\begin{equation}
    \text{space of dimension }18-6-6 / \text{group of translations, rotations (six dimension group)}
\end{equation}

What is the dimension? $0$? \textbf{NO} There are two different forms of cyclohexane, the chair form and the boat form. Furthermore the boat form is felxible and we can bend it around to form another boat form.

So parameter space has at least two components, one component that is a point corresponding to the chair form and a one dimensional component corresponding to the boat form of cyclohexane.

So we must be a bit careful when guessing dimension of quotients, the naïve guess is sometimes just wrong.

\subsection{Moduli space}

\begin{definition}
    \textbf{A moduli space} is space whose points corresponds to isomorphism classes of things
\end{definition}
This is similar to a parameter space, it is traditional to use paramater space if we are classifying thins embedding in something else, and moduli space if we are classifying things that aren't really embedded in anything.

\

Now let us look very briefly at the moduli space of elliptic curves.

\begin{definition}
    \textbf{A Elliptic curve} over the complex numbers is a non-singular curve tbat is topologically isomorphoic to a torus. (We will discuss elliptic cutves lates)
\end{definition}

We will later see that elliptic curves can be written as:\begin{align*}
    y^2&= x^3+ax^2+bx+c\\
    y^2&= (x-\alpha)(x-\beta)(x-\gamma)\\
    y^2 &= x(x-\beta)(x-\gamma) \text{ by adding a constant to }x\\
    y^2 &= x(x-1)(x-\lambda) \text{ by rescaling }x, \text{ with }\lambda\neq 0,1
\end{align*}

This is invariant under changing $\lambda \rightarrow \frac{1}{\lambda}, 1-\lambda, \frac{1}{1-\lambda}, \frac{\lambda}{\lambda-1}, \frac{\lambda - 1}{\lambda}$
These six transformations form a group isomorphic to $S_4$.

So we have an affine variety given by \begin{equation}
    (\mathbb{A}^1\setminus\{0,1\})/S_3 \tag{$\dagger$}
\end{equation}

Note $\mathbb{A}^1\setminus\{0,1\}$ is indeed an affine set since it is isomorphic to the curve $\lambda(\lambda-1)\mu = 0$ in $\mathbb{A}^2$ where $\lambda,\mu$ are coordinates.

What is this space $\dagger$? Well if we take the coordinate of this space here which is $k[\lambda, \frac{1}{\lambda}, \frac{1}{\lambda-1}]$ and take a subring that is invariant under the group $S_3$ of order six. In order to describe this we want to find some polynomial in these three coordinates
that is invariant under $S_3$ this is the simplest one:\begin{equation}
    j = \frac{2^8(\lambda^2-\lambda+1)}{\lambda^2(\lambda-1)^2}
\end{equation}

And it turns out that $k[\lambda, \frac{1}{\lambda}, \frac{1}{\lambda-1}] \simeq k[j]$. This $j$ is the so called $j$-invariant of an elliptic curve that we will study more later on.

\section{Dimension}
\subsection{Defintions of Dimension}
The concept of dimension is difficult to define precisely. Here are some definitions that were proposed
\begin{definition}
    \begin{itemize}
        \item \textbf{Dimension} is the number of parameters needed to define a point. This definition fails since $\R^2$ and $\R$ have the same number of points and by Peano there are continuous maps from $\R$ onto $\R^2$. So in some sense points of $\R^2$ can be specified by only one parameter. This defintion works if we confine ourself to \textbf{smooth manifolds}.
        \item \textbf{Lebesgue covering dimension}, a set has dimension at most $n$ if every open cover has a refinement so that each point is in at most $n+1$ sets. So for example in $\R^2$, if we cover the plane by open sets some points will be in three open sets but no points will be in four open sets. Lebesgue Convering dimension works fine in Euclidean space but not in algebraic geometry.
        \item \textbf{Brower, Menger, Urysohn's definition}: A topological space has dimension at most $n$ where $n=-1,0,1,2,\ldots$  if all points have arbitrarly small nbhs whose boundary has dimension $<n$. For example if we take the plane we can take a small nbh around a point such that they have boundary of dimension 1. Traditionally this defitnition is only used for seperable metric spaces, but spaces in alg geo are not seperable. But this definition works for algebraic sets!
        \item \textbf{Krull dimension}: Supremum of the numbers $n$ for which there is a chain $Z_0\subsetneq Z_1\subsetneq \cdots\subsetneq Z_n$ of \textit{irreducible} subsets. For example if we look at a plane, $Z_0$ can be a point, $Z_1$ is a curve and $Z_2$ is the whole plane. This only works well for Noetherian spaces, since Haussdorff spaces all have Krull dimension $0$ (since the only irreducible subsets are points). However for Noetherian spaces the Krull dimension is the same as the BMU dimension.
        \item \textbf{Various variations of the Haussdorff dimension}: The Haussdorff dimension is got by counting the number of balls used to cover a metric space and see what happens to the number of balls as the radius goes to zero, this dimension can take non-integer values. Though this does not seem useful in alg geo since we don't see many sets of non-integer dimensions here.
        \item \textbf{Transfinite dimensions}: Defintions such as the Krull and BMMU dimension can be extended so that they can be a countable or even uncountable ordinal. Again doesn't seem to be much of use in alg geo.
        \item \textbf{Deviation of poset} A poset has deviation at most $\alpha$ (an ordinal) if for all descsending chains all the finite number of intervals of chain have deviation $<\alpha$. For Noetherian rings, we look at poset of ideals and deviation is equal to the Krull dimension. This definition works well for module, and non-commutative rings.
    \end{itemize}
\end{definition}


\subsection{Algebraic defintion of dimension}

The idea is that a set of high dimension has \textit{many} functions on it. What do we mean by many? There are several ways of understanding what we mean by many:
\begin{definition}
\begin{itemize}
    \item Suppose $B$ variety over $k$ Look at the quotient field of coordinate rings of $B$ and we set $\dim (B) = $ transcendence dimensions of a quotient field. Where we recall a transcendence dimension is the largest amount of algebraically independant elements in the field. For example if $B = \mathbb{A}^2$, then the coordinate ring is $k[x,y]$, so the quotient field is $k(x,y)$ which has dimension $2$. This definition works for algebraic varieties, but this doesn't work very well for other objects in alg geo like non-irreducible algebraic set or schemes.
    \item \textbf{Hilbert polynomial}: Recall a local ring is a ring with a unique maximal ideal $\mathfrak{m}$. For a local ring $A$, we look at the length of $A/\mathfrak{m}^k$ which is a polynomial in $k$(for large $k$) of degree $d$. The dimesion of $A$ is $d$. This defintion looks artificial but works well.
    \item \textbf{Gelfond-Kirillov dimension}: This applies to finietely generated algebras over a field, and is defined to be \[\limsup_n \log(\dim R_n/\log(n))\] Where $R_n$ is generated by the monomials f length less that $n$. If $R$ is commutative this is the same as the dimension from the Hilbert polynomial, if it is not a commutative it could be a real number $\leq 2$ so again we can get a non-integral dimesion. Again this is not that useful in alg geo.
    \item \textbf{Dimesnsion of tangent space of a point}: This definition fails for varieties for singularites, but we can use this to define singular points. A space is non-singular at a point if the dimension of the tangent space is equal to it's dimension.
    \item \textbf{Minimal number of elements of system of parameters}: This defintion works but is not that intuitive.
    \item \textbf{Homological dimension}: We can define Homological dimension by asking when do various homology groups vanish. 
\end{itemize}
\end{definition}

To summarise for Noetherian topological spaces or Noetherian rings, there are several different ways of defining dimension, Krull dimension, Hilbert polynomial, BMU, system of parameters and for any reasonable object in algebraic geometry this will give us the same notion of dimension.

\lecture{Onwards, to Projective varieties!}
\section{Projective varieties}

We will now move on from affine variety to projective varieties and start by defining projective space:\begin{definition}
    $\mathbb{P}^n(k) = \{\text{1 dimensional subspaces of }k^{n+1}\}$, in other words each line in $k^{n+1}$ corresponds to a point of $\mathbb{P}^n$ as follows:\begin{equation*}
        \text{Coordinates of projective space are } (x_0\colon x_1\colon \cdots\colon x_n) = (\lambda x_0\colon \lambda x_1\colon \cdots\colon \lambda x_n) \text{ for not all }x_i = 0\text{ and }\lambda\in k\setminus\{0\} 
    \end{equation*}
\end{definition} 

How does projective space looks like? If $x_0$ is non-zero:\begin{equation*}
    (x_0\colon \cdots \colon x_n) = (1\colon \underbrace{y_1\colon \cdots \colon y_n}_{\text{in }\mathbb{A}^n}) \text{ where } y_i = \frac{x_i}{x_1}
\end{equation*}

If $x_0 = 0$:\begin{equation*}
    \underbrace{(0\colon x_1\colon \cdots\colon x_n)}_{\text{in }\mathbb{P}^{n-1}}
\end{equation*}

So $\mathbb{P}^n = $ disjoint union of $\mathbb{A}^n$ and $\underbrace{\mathbb{P}^{n-1}(k)}_{\text{``points at infinity''}}$

\subsection{Projective space over the reals}

Let $(x_0\colon\cdots\colon x_n)\in \mathbb{P}^n(\R)$ we can rescale the points such that $x_0^2+\cdots+x_n^2 = 1$. So $(x_0\colon\cdots\colon x_n)$ correspond to points on a sphere but since $(x_0\colon\cdots\colon x_n) = (-x_0\colon\cdots\colon -x_n)$, opposite points are identified. So:\[\mathbb{P}^n(\R) = S^n/\text{opposite points}\]

\begin{example}
\begin{align*}
    \mathbb{P}^0(\R) &= \text{point}\\
    \mathbb{P}^1(\R) &= \text{circle}\\
    \mathbb{P}^2(\R) &= S^2/\text{opposite points} = \text{ non-orientable surface}
\end{align*}
\end{example}

\subsection{Projective space over complex numbers}
Let $(x_0\colon\cdots\colon x_n)\in \mathbb{P}^n(\R)$ we can rescale the points such that $\underbrace{|x_0|^2+\cdots+|x_n|^2 = 1}_{\text{In }S^{2n+1}}$. But since this point is equal to $(\lambda x_0\colon \cdots\colon\lambda x_n)$ for $|\lambda| = 1$, from this we can make a map $S^{2n+1}\rightarrow \mathbb{P}^n(\C)$, where the fibers are $S^1$. We call this a fibration, in algebraic topology.

\begin{align*}
    \mathbb{P}^1(\C) = \C + \text{point} = \text{Riemann sphere}\simeq S^2 \text{ we are getting a fibration }\underbrace{S^1\rightarrow}_{\text{fibers}}S^3\rightarrow S^2 \text{ this is the Hopf fibration}
\end{align*}

Another example of a fibration would be:\begin{equation*}
    S^1\rightarrow S^1\times S^2\rightarrow S^2
\end{equation*}

We can also cover $\mathbb{P}^n(k)$ with copies of affine space $\mathbb{A}^n$, so $(x_0\colon \cdots x_n)$ if we take $x_i\neq 0$ we get a copy of affine space. So all together we get $n+1$ copies of $\mathbb{A}^n$. So we shall think of projective space of glueing copies of affine space together.

\subsection{Background of Projective geometry}

What properties are preserved by projection? One property that isn't preserved is parallele lines (if we try to draw railway track, you will see the lines of the track meet at point at infinity).


There are two approaches to geometry:\begin{definition}
    \begin{enumerate}
        \item \textbf{Synthetic geometry} where we write out a set of axioms
        \item \textbf{Analytic geometry} where we use coordinates, and turn geometry into algebra
    \end{enumerate}
\end{definition}
We have been discussing the analytic approach to projective geometry. But we can talk about synthetic geometry for projective geometry.

\subsubsection{Synthetic approach to projective geometry}

\textbf{Axioms}\begin{enumerate}
    \item Any two distinct points meet at unique line
    \item Any two distinct lines in the ``same plane'' meet in one point. Two lines meet in a same plane if when we take four points on them and join them up like this:\newline \href{https://youtu.be/iNfFNwIK_co?t=937}{\includegraphics[scale = 0.24]{lines_plane.png}} these two lines meet. 
    \item Any line meets at least 3 points. (this is a sort of non-degeneracy condition)
\end{enumerate}

We can ask what object can we get satisfying these axioms.\begin{itemize}
    \item Dimension $0$: point
    \item Dimension $1$: Line + points
    \item Dimension $2$: Projective plane, one example is the Fano plane, which has seven straight lines and seven points: \href{https://en.wikipedia.org/wiki/Fano_plane}{\includegraphics[scale = 0.20]{fano_plane}} Notice one of the lines look like a circle.
    \item Dimension $\geq 3$: $\mathbb{P}^n(k)$, over a field or division ring. (Note being a projective space over a field is equivalent to pappus theorem). 
\end{itemize}

So we don't get much by using axioms to describe projective space.

\begin{remark}
    There are two sort of project planes, Desarguseian ones and non-Desarguseian ones. These are planes relating to Desargues theorem.  
\end{remark}

\subsubsection{Desargues' theorem}
\begin{theorem}
    Suppose we take a point (think of it as an observer), looking at a triangle $ABC$ and drawing on some easel where they draws three points $a,b,c$ which are the images of the points of the triangle they see. Join the lines $ab$ and $AB$, $bc$ and $BC$, $ac$ and $AC$. So we have three points from these intersection, these points lie on a line.

    \href{https://youtu.be/idd0eE5Bhsc?t=229}{\includegraphics[scale = 0.15]{desargues.png}}

\begin{proof}
    Key point of the proof is to imagine $abc$ and $ABC$ in 3d space, with planes containing these triangles are different. Since the lines $aA$ and $bB$ meet then the lines $ab$ and $AB$ are on the same plane so meet at a point in space. Likewise the lines $ac$, $AC$ and $bc$, $BC$ meet in space.

    So these three points are well defined in spac, so these three points lie on the plane containing $abc$ and the plane containing $ABC$, so these points meet in the intersection of these planes which is (usually) a line. (The intersection is not a line if the planes are equal but in that case we can take limits to still deduce the theorem)
\end{proof}
\end{theorem}

If we got a projective space of dimension at least three, it automatically satisfies Desargues' theorem, but if we have a projective space of dimension two it may not satisfy Desargues' theorem. 

\begin{definition}\textbf{Non-Desarguian planes}

    These are projective planes that don't satisfy Desargues' theorem. 
\end{definition}

Desargues' theorem is in some sense equivalent to the associativity law for multiplication. We can set coordinates on a ring for a projective space and\begin{align*}
    \text{Desargues} &\iff \text{Associative}\\
    \text{Pappus} &\iff \text{Commutative}
\end{align*}

So synthetic projective geometry is almost the same as analytic projective geometry, except for these non-desarguian planes (one example is the projective plane is a plane over the ring of octionions).

\subsection{Duality for projective space}

\begin{example}
    Let us look at a projective plane and the axioms:\begin{itemize}
        \item Any two distinct points meet in one line
        \item Any two distinct line meet in one point
    \end{itemize}

    So for any theorem we say about lines and points, we can find a dual theorem by replacing points with lines and vice-versa. 

    \

    For example for Pascal's theorem:\begin{itemize}
        \item If we take three points given by intersecting pairs of lines they lie on a straight line.
        \item If we take three lines given by joining up pairs of points in three different ways then they meet at a point.
    \end{itemize}
\end{example}


One way of thinking about duality is that it is just duality of vector spaces:\begin{itemize}
    \item  points of $\mathbb{P}^n$ correspond to lines of $\mathbb{A}^{n+1} = k^{n+1}$
    \item  lines of $\mathbb{P}^n$ correspond to planes of $\mathbb{A}^{n+1} = k^{n+1}$
\end{itemize}

For any vector space $k^{n+1}$ we can take a dual space and if we have a line we can take a dual which will correspond to a hyperplane. So duality of projective space is closely realted to taking the dual of the vector space.

\section{Affine and projective varieties} 
Recall:\begin{align*}
    \text{Affine space } &\iff k[x_1,\ldots,x_n]\\
    \text{Affine alg subsets} &\iff \text{Radical ideal}\\
    \text{Projective space} &\iff k[x_0,\ldots,x_n] \text{ graded ring}\\
    \text{Projective algebraic subsets} &\iff \text{Graded radical ideal}
\end{align*}

Where recall a graded ideal is one where every element is a sum of homogenous elements in the ideal; i.e. $p(x_0,\dots.x_n) = 0\iff p(\lambda x_0,\dots,\lambda x_n) = 0$ (except $(x_0,\dots,x_n)$).

\

If we have a projective variety defined by various equations $p(x_0,\dots,x_n)$ we can look at the corresponding affine variety, invariant under scaling in $\mathbb{A}^{n+1}$ define by the same equations.


\subsection{Examples}

\begin{example}
    Let us take the affine variety $y=x^3$ where $(x,y)\in \mathbb{A}^2$. To make it into a projective variety we look at points $(x:y:z)\in \mathbb{P}^2$, where $(x,y)\leftrightarrow (x:y:1)$. In order to define a closed subset of projective space we need to make out polynomial homogenous, so we add in powers of $z$:\[yz^2 = x^3\]

  Recall that: $\mathbb{P}^2$ is covered by three copies of affine space: $x = 1$ ($yz$-plane) or $y = 1$ ($xz$-plane) or $z = 1$ ($xy$-plane), here we have a curve living in projective space we will look at the restriction of this curve in each copie of affine space:\begin{itemize}
    \item $z = 1$, $y=x^3$
    \item $y=1$, $z^2 = x^3$
    \item $x=1$, $yz^2 = 1$
  \end{itemize}

  \href{https://youtu.be/-znTMw_3mu0?t=797}{\includegraphics[scale = 0.25]{projective-curves.png}}


  The point $(0\colon 0\colon 1)$ in the graph $z=1$, becomes the point at infinity in $y=1$ and a point at infinity for $x=1$. Likewise for the point $(0\colon 1\colon 0)$ in the graph $y=1$.

  All other points appear on all three diagrams. nNote the second diagram has a wonky point at a cusp, that is a singular point.
\end{example}

\begin{example}
    Let us look at the elliptic curve:\begin{itemize}
        \item Affine: $y^2 = x^3 + bx+c$, $(x,y)$
        \item Projective: $y^2z = x^3+bxz^2+cz^3$, $(x\colon y\colon z)$
    \end{itemize}

    If we take \begin{itemize}
        \item $z=1$, then we get $y^2 = x^3+bx+c$
        \item $z=0$: then $x^2 = 0\Rightarrow x = 0$, so the only point at infinity is the point $(0\colon 1\colon 0)$.
    \end{itemize}

    How does this point at infinity look like?

    \href{https://youtu.be/-znTMw_3mu0?t=1017}{\includegraphics[scale = 0.25]{elliptic-curve.png}}
\end{example}

\begin{example}
    Example with more than one point at infinity:

    Look at the curve $y^2 = x^2+1$, we can guess infinite points $(1\colon 1\colon 0)$ and $(1\colon -1\colon 0)$.If we want to be rigourous it is easy, we homogenize it.

    \href{https://youtu.be/-znTMw_3mu0?t=1143}{\includegraphics[scale = 0.25]{two-infinite.png}}
\end{example}

\subsubsection{Twisted Cubic}

In affine space this is the set of points of the form $(t,t^2,t^3) = (x,y,z)$. We want to look at the ideal that defines this curve, which we can see by inspection is: $(y-x^2, z-x^3)$. What is the closure of this in $\mathbb{P}^3$?

\paragraph*{The WRONG answer}

Let's homogenize the generators! \[(wy-x^3,w^2z-x^3)\in k[w,x,y,z]\] This defines something in $\mathbb{P}^3$. Notice there is something fishy going on, the orignal ideal contains $y^2 - zx$ and $z-xy$ but our ideal doesn't contain $y^2-zx$ or $wz-xy$.

\textbf{We can't just homogenize an ideal by homogenizing a set of generators!} We need to homogenize all elements of the ideal.


\paragraph*{The CORRECT answer}
In projective space the twisted cubic's closure is given by $(s^3\colon s^2t\colon st^2\colon t^3)$ (so it is the curve $s=1$ plus an extra point $s=0$). We can check that the ideal of polynomials that vanish of this is the ideal: \[(wy-x^2, y^2-zx, wz-xy)\]


\

What algebraic set in projective space does $(wy-x^2,w^2z-x^3)\in k[w,x,y,z]$ represent?

Well $\mathbb{P}^3$ is covered by four copies of $\mathbb{A}^3$ with $w=1,x=1,y=1$ or $z=1$.\begin{itemize}
    \item $w=1$: $y-x^2=0$, $z-x^3 = 0$, the twisted cubic
    \item $x=1$: $wy = 1$, $w^2z = 1$, affine $z$-line minus a point
    \item $y=1$: $w=x^2$, $w^2z=x^3$; substituing $w$ in the second equation we get:\begin{equation*}
        x^4z-x^3 = 0 \Rightarrow x^3(zx-1)=0
    \end{equation*} 
    So we get a ``triple'' line at $x=0=w$. 
    \item $z=1$, similar to above we pick up the curve with a strange triple line.
\end{itemize}
So if we take our cubic in affine space, and try to make it a part of projective space in this careless way we pick up a triple line at infinity, which is not what we wanted! The fact that this contains twp components corresponds to the Lasker-Noether style decomp of this ideal:\[(wy-x^2,w^2z-x^3) = \underbrace{(wy-x^2,y^2-xz, wz-xy)}_{\text{the line we want}}\bigcap \underbrace{(wy-x^2, w^2, wx)}_{\text{corresponds to triple line at infinity}}\]

\section{Product of varieties}

First of all:\begin{align*}
    \mathbb{A}^m&\times \mathbb{A}^n = \mathbb{A}^{n+m}\\
    (x_1,\dots,x_m) &\ (y_1,\dots,y_n) \rightarrow (x_1,\dots,x_m,y_1,\dots,y_n)\\
    k[x_1,\dots,  x_m] &\ k[y_1,\dots,y_n] \rightarrow k[x_1,\dots,  x_m] \otimes k[y_1,\dots,y_n] = k[x_1,\dots,x_m,y_1,\dots,y_n]
\end{align*} 

For algebraic sets, suppose $X$ is defined by the ideal $I$ and $Y$ by $J$ then $X\times Y$ is defined by the ideal $(I,J)$.


\subsection{Product of projective space}

Is $\mathbb{P}^m\times \mathbb{P}^n$ equal to $\mathbb{P}^{n+m}$? NO! Since $S^1\times S^1 = \mathbb{P}^1(\R)\times \mathbb{P}^1(\R)$, but this is not equal to $\mathbb{P}^2(\R)$, since the first set is orientable but the second one isn't.

\paragraph*{What if we use maps?}

If we try to make a map $(x_0\colon \ldots\colon x_m) \ (y_0,\ldots, y_n) \rightarrow (x_0\colon x_1\colon \ldots\colon x_m \colon y_1\colon \ldots)$. This does not make since since it is not well defined! We can scale $(x_0\colon \ldots)$ by $\lambda$ and $(y_0\colon \ldots)$ by $\mu$, and the result makes no sense.

But note we can create a map: \[\mathbb{P}^m\times \mathbb{P}^n \underbrace{\rightarrow}_{\text{not onto}} \mathbb{P}^{mn+m+n}\]
\[(x_0\colon \ldots\colon x_m) \cdot (y_0,\ldots, y_n) \rightarrow (x_0y_0\colon x_1y_0\colon \ldots\colon x_my_0 \colon x_0y_1\colon \ldots x_my_n \colon y_1\colon \ldots) = (w_{00}\colon w_{10}\colon \ldots \colon w_{m0}\colon \ldots \colon w_{mn})\] Where we have the relations $w_{ij} = x_iy_j$ and $w_{ij}w_{kl} = w_{il}w_{kj}$

\begin{remark}
    More abstract way of defining this map is by \begin{align*}
        k^{m+1} \times k^{n+1} &\rightarrow k^{m+1} \otimes k^{n+1}\\
        (v,w) &\rightarrow v\otimes w
    \end{align*} 
\end{remark}

We want to show the map: \[\mathbb{P}^m\times \mathbb{P}^n \rightarrow \{(w_{00}\colon w_{10}\colon \ldots \colon w_{m0}\colon \ldots \colon w_{mn}) \mid w_{ij}w_{kl} = w_{il}w_{kj}\}\text{ this map is the Segre embedding} \] Is onto.

We can assume (by rearraging terms if need be) that $w_{00} = 1$. So we have $w_{kl} = w_{0l}w_{k0} = y_lx_k$ since $x_0=y_0 =1$

So the product of two projective spaces is a projective variety! By more or less comparing the argument for affine spaces we can see that the product of any projective variety is a projective variety.


\subsection{Example of the Segre map}
\begin{align*}
    \mathbb{P}^1\times \mathbb{P}^1 &\rightarrow \mathbb{P}^3\\
    (x_0\colon x_1), \ (y_0\colon y_1) &\rightarrow (x_0y_0\colon x_0y_1\colon x_1y_0\colon x_1y_1) = (w_0\colon w_1\colon w_2\colon w_3)     
\end{align*}

With $w_0w_3 = w_1w_2$, which is a quadric in $\mathbb{P}^2$. \textit{Over any algebraically closed field} with characteristic not equal to 2, any two nonsingular quadrics are isomomorphic, and they must be of the form $x_0^2+x_1^2+\cdots+x_n^2 = 0$.

So any quadric in $\mathbb{P}^3$ is isomomorphic to $\mathbb{P}^1\times \mathbb{P}^1$, so it got two lines on it. For example if we take the sphere $x^2+y^2+z^2 = 1$, we claim it has straight lines over $\C$! For example $x = 1, y=iz$

\begin{exercise}

Find all straight lines on $x^2+y^2+z^2 = 1$ over $\C$, (note $(x+iy)(x-iy) = (1-z)(1+z)$)
\end{exercise}

\section{Two examples of Projective Varieties}

\subsection{Veronese surface}
This contains of all points of the form \[(x^2\colon xy\colon xz\colon y^2\colon yz\colon z^2)\in \mathbb{P}^5\]
We can look at this as a map from $\mathbb{P}^2\rightarrow \mathbb{P}^5$, obviously this map is not onto. We would like to describle the subset of $\mathbb{P}^5$ by finding explcit formula for it.


So we take $(w_{00}\colon w_{01}\colon w_{02}\colon w_{11}\colon w_{12}\colon w_{22})\in \mathbb{P}^5$. Note that the map satisfies $w_{ij}w_{kl} = w_{ik}w_{jl}$, and $w_{ij} = w_{ji}$.

We can check that the map is onto.


\

We can also take of this map as from $\mathbb{A}^3\rightarrow \mathbb{A}^6$, where we can think of $\mathbb{A}^6$ as $S^2(\mathbb{A}^3)$, where $S^2(\mathbb{A}^3)$ is the set of all degree two monomials in $\{x,y,z\}$ where this is the basis of $\mathbb{A}^3$.



\subsection{Variety of all lines in $\mathbb{P}^3$}

What does the dimesnion of this vartiety be? If we take a line in $\mathbb{P}^3$ and a random plane, then the line will pass through some point in the plane (2 degrees of freedom) and then it can point in a direction in ordinary euclidean space, this direction is given by a point on the sphere $S^2$, which gives us 2 more degrees of freedom.

So we expect the dimension of this variety to be of dimension $4$. A line in $\mathbb{P}^3 \iff 2-\text{dim}$ subspace of $k^4$. This is a special case of what is called a Grassmanian.

\begin{definition}
    The \textbf{Grassmanian} $\mathbf{G(m,n)}$ is the set of all $m$ dimensional subspaces of $k^{m+n}$.

    For example:\begin{itemize}
        \item $G(0,m) = G(m,0) = $ point
        \item $G(1,m) = \mathbb{P}^m$
        \item $G(m,n) \simeq G(n,m)$. Indeed since we have $m \text{dim}$ subspace $k^{m+n} \underset{\text{dual}}{\rightarrow} (k^{m+n})^\ast \simeq k^{m+n} \text{ n dimensional subspace}$
        \item So $G(m,1) = \mathbb{P}^m$
        \item The first non-trivial case that isn't a point or projective space is \[G(2,2) = \text{2}-dim \text{ subspaces of }k^4\]
    \end{itemize}
    In general, grassmanians are special cases of Hilbert schemes, these are schemes(we will discuss this later) whose points correspond to certain configurations of projective space satisfying certain conditions. The simplest condition is that it must be a linear subspace of projective space. So grassmanians are the simplest examples of Hilbert schemes. 
\end{definition}

We will embed $G(2,2)$ into $\mathbb{P}^5$.

Pick a line in $\mathbb{P}^3 \text{ corresponding to a point of }G(2,2)$. Pick two distinct  points on the line, $(a_0\colon a_1\colon a_2\colon a_3)\neq(b_0\colon b_1\colon b_2\colon b_3)$, now we create a two by four matrix from these two points:\[\begin{pmatrix}
    a_0\colon a_1\colon a_2\colon a_3\\
    b_0\colon b_1\colon b_2\colon b_3
\end{pmatrix}\]

Let $s_{ij} = \det \text{ of columns }i,j$; for example $s_{01} = \det\begin{pmatrix}
    a_0& a_1\\
    b_0& b_1
\end{pmatrix} = a_0b_1-a_1b_0$

So we get six coordinates: $(s_{01}\colon s_{02}\colon s_{03}\colon s_{12}\colon s_{13}\colon s_{23})$, note that $s_{30} = \lambda s_{03}$ for some $\lambda$, we also see $s_{ii} = 0$ so we can ignore these.


Notice that this point $\mathbb{P}^5$ only depends on the line we have chosen. If we multiply all the a's or b's by a constant all these determinants are multiplied by a square of that constant so the point is the same. Secondly we had a lot of choice of which two points we placed on the line, we can pick other points by replacing b by a multiple of $a$, which is equivalent to adding a multiple of the first row to the second row which 
won't change the determinant, so it won't change the point in $\mathbb{P^5}$.

\

So we have a well defined map from lines in $P^3$ to points in $\mathbb{P}^5$, Note that by checking dimensions we can clearly see that this map is not onto.  So what is the relation between these $s_{ij}$? It's the Plucker Relation.

\paragraph*{Plucker Relation}
\[s_{01}s_{23} - s_{02}s_{13}+s_{03}s_{12} = 0\]

(Proof that this relation holds, is that each term of the form ${a_i}{b_j} - {a_j}{b_i}$ occurs twice with opposite signs).


Are there any other relation satisfying these points? No! If we got a set of numbers satisfying the plucker relation there is a line in $G(2,2)$ corresponding to that point.

\paragraph*{Showing the map $G(2,2)\rightarrow $ solutions of the plucker relation is onto}

Some $s_{ij}$ must be non-zero, se we can assume it is $1$. So we may assume that $s_{01} = 1$. But notice that:\begin{equation*}
    s_{01}s_{23} = s_{02}s_{13}-s_{03}s_{12}
\end{equation*}

So $s_{23}$ is determined by $s_{02},s_{13},s_{03},s_{12}$. So pick the two points:\begin{align*}
    &(1\colon 0\colon s_{12}\colon s_{13})\\
    &(0\colon 1 \colon s_{02}\colon s_{03})
\end{align*}

These two points determine a line in $G(2,2)$ which maps to the given point in $\mathbb{P}^5$.

\subsubsection{Find cohomology of quadric in $\mathbb{P}^5$}

Notice that the Grassmanian is the union of the following subsets:\begin{equation*}
    \begin{pmatrix}
        1\colon 0\colon \ast\colon \ast\\
        0\colon 1 \colon \ast\colon \ast
    \end{pmatrix},\begin{pmatrix}
        1\colon \ast\colon 0\colon \ast\\
        0\colon 0 \colon 1\colon \ast
    \end{pmatrix}, \begin{pmatrix}
        1\colon \ast\colon \ast\colon 0\\
        0\colon 0 \colon 0\colon 1
    \end{pmatrix}, \begin{pmatrix}
        0\colon 1\colon 0\colon \ast\\
        0\colon 0 \colon 1\colon \ast
    \end{pmatrix},
    \begin{pmatrix}
        0\colon 1\colon \ast\colon 0\\
        0\colon 0 \colon 0\colon 1
    \end{pmatrix},\begin{pmatrix}
        0\colon 0\colon 1\colon 0\\
        0\colon 0 \colon 0\colon 1
    \end{pmatrix}
\end{equation*}
These form six copies of various affine space, $A^4,A^3,A^2, A^2, A^1,A^0$ respectively. The dimesion of the cohomology groups can be read out from these.

We can also use this answer an alternative question:

\paragraph*{How many points does $G(2,2)$ have over a FINITE field $k$}

Note that $A^n$ has $q^n$ points, $(q = |K|)$.

So $G(2,2)$ has \[q^0+q^1+2q^2+q^3+q^4 \text{points}\]

While \[\mathbb{P}^4 = q^0+q^1+2q^2+q^3+q^4 \text{points}\]

So there is some relation between cohomology group of a variety over complex numbers and the number of points of the variety over a finite field. This is the theme of the \textbf{Weil conjectures} of a variety over a finite field. 

\section{Grassmanians}
Let's now look at more general grassmanians $G(m,n)$. 

\begin{theorem}
    $G(m,n)\subseteq \mathbb{P}^{\binom{m+n}{n}-1}$
\begin{proof}
    Suppose given some $m$-dimensional subspace of $k^{m+n}$, pick $m$ vectors spanning it and create a matrix of size $m(m+n)$. \[\begin{pmatrix}
        a_1 &\cdots& a_{m+n}\\
        b_1 &\cdots& b_{m+n}\\
        b_1 &\cdots& b_{m+n}\\
        \vdots
    \end{pmatrix}\]

Where each row form the coordinates of one these points. Now pick any $m$ colums and look at the determinant of these columns (so the determinant of an $m\times m$ minor of this matrix). So this gives us $\binom{m+n}{m}$ numbers, let's call these numbers $p_{i_1\dots i_m}$ if we picked colums $i_1,\dots,i_m$, So these give points in $\mathbb{P}^{\binom{m+n}{n}-1}$. 
\end{proof}

\begin{remark}
More abstract way of showing this:

Suppose we have a vector space $V$ of dimension $m$ which is a subspace of $W$ of dimension $m,n$. 

Taking the $m^{th}$ exterior power of $V$ we can find a map:\[\underbrace{\Lambda^m V}_{\dim = 1} \rightarrow \underbrace{\Lambda^m W}_{\dim = \binom{m+n}{n}}\]

So whenever we have an $m$ dimensional of a $m+n$ dim space, we have a corresponding $1$ dimensional space of a $\binom{m+n}{n}$ dimensional space. This corresponds to a point of the projective space $\mathbb{P}(\Lambda^m W)$.
\end{remark}
\end{theorem}

Of course this is map is not onto, we have some plucker relations.


\subsection{Plucker relations:}


\begin{align*}
    \sum_\lambda  (-1)^\lambda p_{i_1\ldots i_{m-1},j_\lambda} p_{j_1\ldots j_{\lambda-1}j_{\lambda+1}\ldots j_{m+n}} = 0
\end{align*}
This equality is true since every monomial occurs twice with opposite signs. So notice the Plucker relations f a lot of quadric relations.

\begin{theorem}
    The map $G(m,n) \rightarrow $ space of zeroes of plucker relations is onto.

    \begin{proof}
        We can assume that $p_{1\dots m} = 1$ by changing coordinates if necessary. Then we can find a point in the grassmanian with given values of:\[p_{1\cdots r-1,r+1\cdots m,s} \text{ by choosing a matrix whose left comes from the identity}\]

        But then plucker relations determine all the other $p$'s.
    \end{proof}
\end{theorem}

\subsection{Applications}
There are several applications of grassmanians.\begin{itemize}
    \item Grassmanians are covered by affine spaces.
    \item Product of Grassmanians are given by the Littlewood-Richardson rule. 
    \item ``Line complexes'', these are certain varieties with a rich structure. The quadric line complex is given by $G(2,2)\subseteq \mathbb{P}^5$ and we intersect it with a quadric in $\mathbb{P}^5$. (cf. last chapter Griffiths \& Harris Algebraic Geometry)
    \item \[G(m,n) = GL_{m+n}(k)/\begin{pmatrix}
        \ast & \ast \\
        0 & \ast
    \end{pmatrix}\]

    Note that $GL_{m+n}(k)$ and $\begin{pmatrix}
        \ast & \ast \\
        0 & \ast
    \end{pmatrix}$ are affine, but their quotient $G(m,n)$ may not be affine since it is a projective variety.
        In fact the quotient of two affine varieties doesn't always have to be affine or projective for example, suppose we take: $k^2\setminus (0.0)$, this variety is neither affine, nor projective. But $GL_2(k)$ acts transitevly on it and the subgroup fixing a point is $\begin{pmatrix}
            1 & \ast \\
            0 & 1
        \end{pmatrix}$, which is an affine line.

        So $k^2\setminus (0.0) = GL_2(k)/\begin{pmatrix}
        1 & \ast \\
        0 & 1
        \end{pmatrix}$, is a quotient of affine group by affine group but is neither affine nor projective.  
    \item Construction of Hilbert Scheme by Grothendieck, the idea of the Hilbert scheme is that it parameterizes subschemes of projective space. What does this mean? We take coordinate ring of projective space $k[x_0,\ldots,x_n]$ and we take graded ideals $I = 0\oplus I_1\oplus I_2$. So we want to classify graded ideals of this ring and in particular we want to show that graded ideals correspond to points of some projective space.
          We will see later that $\dim(I_j)$ is a polynomial in $j$, for $j$ large (It's more or less the Hilbert polynomial). Suppose that $I_d$ generates $I_{d+1},I_{d+2},\ldots$, so $I_d\subseteq S_d$ (degree $d$ monomials), which is a point of a grassmanian which is iteself contained in some large projective space. 
\end{itemize} 

\subsection{Why is the mapping natural?}
Now what do we mean by there is a \textbf{natural} mapping between the lines in $\mathbb{P}^3$ and the points of $G(2,2)\subseteq \mathbb{P}^5$. Notice in some sense it is trivial to find a variety whose points correspond to lines in $\mathbb{P}^3$, all we need to do is to find a variety of the same cardinality and take a bijective map. But this is a useless correspondence, we want that makes sense.

How do we define a natural correspondence? This was defined by Grothendieck, look at \textbf{functors} from commutative rings $R$. \begin{itemize}
    \item $F\colon R\rightarrow $ Lines in $\mathbb{P}^3(R)$
    \item $R \rightarrow G(2,2)$ over $R$
    \item $G\colon R\rightarrow R$-valued points of a quadric given by the plucker relation $p_{01}p_{12}+\cdots + \cdots  = 0$ in $\mathbb{P}^5$
\end{itemize}

Grothendieck's way of saying that the lines in $\mathbb{P}^3$ correspond naturally to the points of the above quadric, says that these three are \textbf{ISOMORPHIC AS FUNCTORS}

So not only $F(R), G(R)$ are isomomorphic as sets, but if we have a morphism $R\rightarrow S$ we can create this commuting diagram\[\begin{tikzcd}
    F(R) \arrow{r}{} \arrow[swap]{d}{\text{iso}} & F(S) \arrow{d}{\text{iso}} \\%
    G(R) \arrow{r}{}& G(S)
    \end{tikzcd}
    \]

In order for all of this to make sense we need to work over general commutative ring $R$, it is not enough to work over fields.

More generally Grothendieck showed that any scheme is defined by it's functor of points. 

\section{More examples of projective varieties: Projective space bundles}

\begin{definition}
    Recall a \textbf{fibre bundle} is a space that looks locally like a product space. So assume we have a space $E$ (total space), $F$ (the fibre), $B$ (the base space of the bundle). Such that there is a surjective map $E\rightarrow B$.

    Then for small open sets $U\subseteq B$, we have $U$ looks locally like the projection $F\times U\rightarrow U$
\begin{example}
    Let $B = S^1$, and $F = \R^1$, then we can map $S^1\times \R \rightarrow S^1$. This will be a special case of a fibre bundle (it will look like a cylindre mapping to a circle).

    \

    On the other hand we can map a Moebius Band $\rightarrow S^1$, where we can think of the Moebius bands projected on a circle where the fibres are copies of $\R$. Locally this map looks like a product, if we take a small piece of a circle it's pre-image looks like the product of $\R$ and this little piece. But notice that while locally the Moebius band looks like a product, globally it doesn't.
\end{example}

\

Fibre bundles are sort of twisted products. Quite often the Fibre is a vector space in this case we talk about \textbf{vector bundle}. If the Fibre is a copy of projective space we talk about a \textbf{projective space bundle}.
\end{definition}

\subsection{Hirzebruch surfaces}
Let us look at the space:\[(\mathbb{A}^2\setminus 0)\times(\mathbb{A}^2\setminus 0) \text{ acted on by }G_m\times G_m\]

Where $G_m = \C^\ast$. So if we let $G_m$ take $(x,y)\rightarrow (\lambda x,\lambda y)$ $\mathbb{A}^2/G_m$, then $(\mathbb{A}^2\setminus 0)/G_m = \mathbb{P}^1$. So if we take the obvious action we see that $(\mathbb{A}^2\setminus 0)\times(\mathbb{A}^2\setminus 0)/G_m\times G_m = \mathbb{P}^1\times \mathbb{P}^1$.

But we can let $G_m$ act in an other way:

\

\[\underbrace{(\lambda,\mu)}_{G_m\times G_m}\underbrace{((s,t),(x,y))}_{(\mathbb{A}^2\setminus 0)\times(\mathbb{A}^2\setminus 0)} = (\lambda s, \lambda t, \mu x, \lambda^{-a}\mu y), \text{ where }a\in \Z\]

Now under this action $F = (\mathbb{A}^2\setminus 0)\times(\mathbb{A}^2\setminus 0)/G_m\times G_m$, is called the Hirzebruch surface.

There is a map $F\rightarrow \mathbb{P}^1$ given by $(s,t,x,y)\rightarrow (s\colon t)$. Similarly the fibre at any point is also isomorphic to $\mathbb{P}^{1}$. We can check that locally this a product but globally it is not a product. We have a surface that is a fibre bundle of $\mathbb{P}^1$, whose fibres are also copies of $\mathbb{P}^1$. It is a kind of ``twisted'' copy of $\mathbb{P}^1\times \mathbb{P}^1$.

\subsection{Scrolls}

We act on $(\mathbb{A}^2\setminus 0)\times (\mathbb{A}^n\setminus 0)$ by $G_m\times G_m$ where:\begin{equation*}
    (\lambda,\mu)(s,t,x_1,\dots,x_n) = (\lambda s, \lambda t, \lambda^{-a_1}\mu x_1,\lambda^{-a_2}\mu x_2\dots), \text{ where }a_i\in \Z
\end{equation*}

The \textbf{Scroll} is given by the quotient $(\mathbb{A}^2\setminus 0)\times (\mathbb{A}^n\setminus 0)/G_m\times G_m$.

Note just as before we get a map from the scroll to $\mathbb{P}^1$ by taking $(s,t,x_1,\dots, x_n)\rightarrow (s\colon t)$ and the fibres are copies of $\mathbb{P}^{n-1}$. 

Let's map this to projective space: Assume that all the $a_i>0$, then we can consider the monomials: \[{s^i}{t^{a_j0-i}}x_j \text{ for }j=1,2,\ldots,n \text{ and }0\leq i \leq a_j\]

We can consider these as points in $\mathbb{P}^{(\sum a_j +1) - 1}$, we can see that each of these fibres are mapped to a linear subspace of this projective space. 

It's a bit clumbsy to have to map everything into projective space, it is sometime useful to use ``abstract algebraic varieties''.

\subsubsection{Abstract varieties (Weil)}
Weil defined them in order to do a construction called doing the ``Jacobian of a curve'', it wasn't obvious at the time that these object were embeddable in projective space. So he came up with this concept which let him construct varieties without having to embed them into projective space.

\paragraph*{Differentiable manifolds}
\begin{itemize}
    \item \textbf{The old view}: Subset of $\R^n$ defined by some equations. For example $S^2 = \text{ points such that }x^2+y^2+z^2 = 1$ in $\R^3$. But this is not really a differentiable manifold, it is a differentiable manifold with an embedding into euclidean space.
We don't think of differentiable manifolds as coming with embeddings of euclidean space anymore.

    \item \textbf{The new view}: We think of covering our manifold by charts. For example we might cover $S^2$ with two pieces, each of these pieces which look like an open disk in euclidean space. So we might think of $S^2$ as two open disk which are glued together.
    \item   
\end{itemize}


\paragraph*{Bringing this into algebraic geometry}
\begin{itemize}
    \item \textbf{The old view}: Variety is a subset of $\mathbb{P}^n$ defined by equations.
    \item \textbf{The new view}: Take some AFFINE varieties and glue them together.
\end{itemize}

For example $\mathbb{P}^1$ can be thought as two copies of the affine line $\mathbb{A}^1$ glued together. (We can find copies $\mathbb{A}^1 = (1\colon y)$ and $\mathbb{A}^1 = (x\colon 1)$) How do we glue them together? we take the subsets $\mathbb{A}^1\setminus 0$ from each copy and glue them by mapping $x\rightarrow \frac{1}{y}$, where $x\in \mathbb{A}\setminus 0$ and $y$ is in the other copy. 

What happens when you glue them by the map $x\rightarrow y$, then we are identifying all the non-zero points together so we get something that looks like the line with two origins (non-Haussdorff space).

This gives an example of an abstract variety that isn't a projective variety, abstract varieties are more general then projective varieties. In turns out all ``reasonable'' abstract varieties (i.e. that ignoring these kind of varieties) in dimension $1$ are projective varieties, in $2$ dimension if they don't have singularities are projective. But there are some 3 dimensional examples of non-singular, non-projective abstract varieties.

\subsection{Toric varieties}
We will start with an example of how to construct affine varieties from cones. 

\subsubsection{2 dimensional example}

We start with $k[x,x^{-1},y,y^{-1}]$ which is the coordinate ring of $k^\ast\times k^\ast$ (note even though the non-zero points in $k$ don't form an algebraic set, we can think of them as points on a hyperbola). If we draw a picture of this ring we get a sort of lattice.  By defining cones we can take the subring generated by monomials on that cone. 

Whenever we take a subring generated by a cone it has the following properties:\begin{enumerate}
    \item Algebraic over $k$
    \item No nilpotent
    \item It is sometimes finitely generated. It is fin gen if it ``rational'', if its edges passes through a rational point in the two dimensional case.
\end{enumerate}

If it is finitely generated we get the coordinate ring of an affine variety.

\href{https://youtu.be/Sjp-99Xyiic?t=360}{\includegraphics[scale = 0.25]{cones}}


\paragraph*{What happens to varieties associated to certain cones?}

Suppose we take two cones as in the picture. What is the relation between the corresponding varieties. We get a map from the orange ring to the red ring; so the maps from the varieties go in the opposite directions. This is a bit annoying since we would prefer a map from the orange variety to the red one.


\href{https://youtu.be/Sjp-99Xyiic?t=527}{\includegraphics[scale = 0.25]{two-cones.png}}


We can get around this by using duality:

\begin{definition}
    If we have a lattice $\Z^2$ we can look at the dual lattice, where every cone in $\Z^2$ we can look at the \textbf{dual cone}; which are the points on the dual of $\Z^2$ which are positive on the cone. If a cone is contained in another cone their dual cones will have the containmeent reversed.

    \

    \href{https://youtu.be/Sjp-99Xyiic?t=527}{\includegraphics[scale = 0.25]{dual-cones.png}}
\end{definition}

Instead of associated a variety to a cone we associate a variety by taking the ring associated to it's dual cone.

For each cone $C$ in $\Z^2$ look at the dual cone and take the corresponding ring. This gives the variety of $C$. 

\begin{example}
    The cone that is the line likw in the picture has dual cone the half-plane. The coordinate ring of the half-plane is $k[x.x^{-1},y]$ which corresponds to teh variety $k^\ast \times k$:

    On the other hand if we take a quadrant, it's dual will be the same quadrant which corresponds to $k[x,y]$ which is $k\times k$, So the inclusion order for varieties and cones are the same!

    \

    \href{https://youtu.be/Sjp-99Xyiic?t=662}{\includegraphics[scale=0.25]{dual-cone-example.png}}
\end{example}

\paragraph*{Another example with an interesting conclusion}


Now we can look at several cones in 1 dimension:

\begin{example}
The red cone which is a line, the blue cone which is the opposite line and the green cone, their intersection which is their intersection a point. The dual of the red and blue cones are the same but the dual of the green cone is the whole line. 

    These give us three varieties, two copies of $\mathbb{A}^1$ and a copy of $\mathbb{A}\setminus 0$, we can glue them together to get $\mathbb{P}^1$.
    
    
    \href{https://youtu.be/Sjp-99Xyiic?t=848}{\includegraphics[scale=0.25]{projective-cone.png}}
\end{example}

Now let us look in two dimensions:

\begin{example}
    If we split the lattice into four quadrants we see that our image is the product if two copies of the figure we had in the last example, so we get not $\mathbb{P}^2$ but $\mathbb{P}^1\times \mathbb{P}^1$. The varieties we get like this are sometimes called \textbf{Toric varieties}

    \

    \href{https://youtu.be/Sjp-99Xyiic?t=947}{\includegraphics[scale=0.25]{projective-cone-2D.png}}

    \paragraph*{Getting $\mathbb{P}^2$ with cones}

    \

    \href{https://youtu.be/Sjp-99Xyiic?t=1103}{\includegraphics[scale=0.20]{p2-cones.png}}

    We are gluing together three copies of $\mathbb{A}^2$ along the intersections $\mathbb{A}^1\times \mathbb{A}^1\setminus 0$

\newpage

    \begin{exercise}\textbf{ What is this?}

    \ 

        \href{https://youtu.be/Sjp-99Xyiic?t=1139}{\includegraphics[scale=0.20]{exercise-cones.png}}

    \end{exercise}

\end{example}

\paragraph*{Abstract Varieties}

We can also find some more exotic examples, for example in the image here, we are glueing an infinite sequence of cones. What project variety do we get? We don't get one! It is too big to be a projective variety, in fact it is not quasi-compact. It is an abstract variety.

\

\href{https://youtu.be/Sjp-99Xyiic?t=1306}{\includegraphics[scale=0.20]{abstract-cone.png}}


\paragraph*{Toric varieties}
The sort of varieties we get like this are called \textbf{Toric varieties}, since they all contain a torus as a dense subvariety.

If we take any collection of cones we can look at a single point, the variety of this point will map to the abstract variety we have since it is contained in all cones.

In two-dimension the point maps to $k[x,x^{-1},y,y^{-1}]$ which is the \textbf{coordinate ring of a torus}.

\subparagraph*{Why is this called a torus?}

Recall from algebraic topology a torus is the product of any number of copies of $S^1$. I.e. ${(S^1)}^n$

We can form $S^1$ over the reals, it will be the set $x^2 + y^2 = 1$, over the complex numbers this gives us $(x+iy)(x-iy) = 1 = z_1z_2$ is a hyperbola isomorphic to $\C\setminus 0$ 

So $\C^\ast$ is an analogue of $S^1$. More generally over the complex numbers we can call ${(\C^\ast)}^n$ a \textbf{torus}. A torus of this form plays the same role as the ordinary torus plays in the theory of compact Lie Groups.

So toric varieties all have a torus like this as a dense subset.

\lecture{Morphisms}

\section{Category Theory}

\begin{definition}
    A \textbf{category}, Is a collection of \textit{objects} and \textit{morphisms}, such that for any two objects there is a set of morphisms between them,
    And such that the following conditions hold:\begin{enumerate}
        \item Given objects $A,B,C$; $f\colon A\rightarrow B$, $g\colon B\rightarrow C$, are morphisms then there is a morphism $g\circ f\colon A\rightarrow C$ \textbf{(composition of morphisms)},
        \item For each object $A$, there is an identity morphism, such that the composition of a morphism $f$ and the identity gives $f$.
        \item  When defined composition is associative
    \end{enumerate}   

    A category is usually named after it's objects,
\end{definition}

\begin{definition}
    An \textbf{isomorphism} is a morphism that has two sided inverse, under composition.
\end{definition}

\subsection{Examples}

For example we have:\begin{itemize}
    \item \textit{Category of sets}, with morphisms the set of \textit{functions} from $A$ to $B$; for sets $A$ and $B$
    \item \textit{Category of groups} with morphisms the \textit{homomorphisms}.
    \item \textit{Category of topological spaces} with morphisms the \textit{continuous functions}.
    \item \textit{Category of Commutative ring} with morphisms the \textit{homomorphisms of rings}.
\end{itemize}

\paragraph*{How do we define a product of two objects?}

\begin{definition}
    Suppose we have two objects $A,B$. Then a we define the \textbf{product}, $A\times B$, to be an object with maps $A\times B\rightarrow B$ and $A\times B\rightarrow A$, which is \textit{universal} with this property. I.e. for any object $C$ with given morphisms $C\rightarrow B$, $C\rightarrow A$. Then there is a unique morphisms $C\rightarrow A\times B$ such that this diagram commutes:

    \[\begin{tikzcd}
        C 
        \arrow[drr] 
        \arrow[dr]
        \arrow[ddr] & &\\
            & A\times B \arrow[r] \arrow[d] & B \\
         & A &
        \end{tikzcd}
        \]

Note the product is NOT unique, but is unique upto \textbf{unique isomorphism}. Indeed if we have $A, B$ and $X,Y$ were two objects with this universal property then:    \[\begin{tikzcd}
    X
    \arrow[drr, bend left] 
    \arrow[dr, "\phi", shift left]
    \arrow[ddr, bend right] & &\\
        & Y \arrow[r] \arrow[ul, "\psi", shift left] \arrow[d] & B \\
     & A &
    \end{tikzcd}
    \]

Where $\phi$ is unique that making all the diagrams commute, and $\psi$ is a unique map making all the diagrams commute. So the composition of these two maps is a map from $Y$ to itself making these diagram commute so it must be the identity. 
\end{definition}

\begin{exercise}
    The product of sets, topological spaces, and of rings. All have this universal property.
\end{exercise}

\subsection{Varieties and Categories}
We want to turn varieties and algebraic sets into categories. There are two ways of doing this:\begin{itemize}
    \item Using regualar maps (analogue of smooth maps)
    \item Using rational maps
\end{itemize}

\begin{definition}
    The \textbf{opposite} category is given by reversing all arrows in the morphisms. So for a category $C$ the opposite category $C^0$ is such that hte morphisms from $A$ to $B$ are the same morphisms of $C$ from $B$ to $A$.

This is a sort of generalization of the dual of a vector space.
\end{definition}

\begin{proposition}
We will see that Affine varieties and regular maps are \textbf{dual} to the category of finite algebras over $k$ with \textit{no nilpotents}.

If we have a map $V\rightarrow W$, we get a map $\text{functions on }V\leftarrow \text{functions on }W$. 
\end{proposition}

\subsubsection{Regular functions}
\begin{definition}
    For an affine variety $Y\subseteq \mathbb{A}^n$ is definied by some ideal $I$ and the coordinate ring of $Y$ is $k[x_1,\dots,x_n]/I$, this coordinate ring is the ring of \textbf{regular functions} on $Y$. We can think of this as the restriction of polynomials to $Y$.   
\end{definition}

More generally we want to define a regualar function for $U$ open in $Y$, we call $U$ quasi-affine. 

\begin{example}
\begin{itemize}
    \item $U = \mathbb{A}^1\setminus 0$, so $U\simeq \{xy = 1 \text{ in }\mathbb{A}^2 \}$. In this case we want $x^{-1}$ to be regular on $U$, so we want the ring of regular funtion to be $k[x,x^{-1}]$. 
    \item $U = \mathbb{A}^2\setminus (0,0)$, which is not any affine variety. 
\end{itemize}    
\end{example}

\begin{definition}
    A function $f\colon U\rightarrow k$, where $U$ is quasi-affine, is \textit{locally regular} at $o\in U$, if $f = \frac{g}{h}$ in some nbh of $p$ where $g.h$ are polynomials and $h\neq 0$ at $p$.
    
    \

    Note that $g,h$ may depend on $p$.
\end{definition}


\begin{definition}\label{regular function}
    A \textbf{regular function} on a quasi-affine variety $U$, is a function (to $k$) that is \textit{locally regular} at all points $p\in U$.
\end{definition}

\paragraph*{Problem:} Suppose $Y$ is affine, are locally regular functions (regular at each point), globally regular (polynomials in $\mathbb{A}^n$)?

Since $Y$ is affine it is compact, assume that $Y = U_1\cup U_2\cup\ldots\cup U_n$, where $U_i$ are open. Furthermore, since the sets of the form $Y\setminus V(F)$ form a basis for the Zariski topology of $Y$, we can assume that each of the open sets $U_i = Y\setminus V(f_i)$ for some polynomial $f_i$. 

\

Let $f$ be a function on $Y$, such that $f$ is regular on all $U_i$.

\[f = \frac{g_i}{h_i}, \text{ on }U_i, \text{ with }g_i,h_i\text{ polynomials and }h_i\neq 0 \text{ on }U_i\]

\begin{proposition}
    $f$ is a polynomial

\begin{proof}
    
Note that since $Y$ is covered by $U_i$, no point of $Y$ is outside all the $U_i$ so no maximal ideal of $k[x_1,\dots,x_n]$ containing $I$ contains all the $h_i$ (since if so we would have a point of $Y$ that is zero on all the $h_i$, so it would not be in any of the $U_i$ by definition of $h_i$,)

Therefore, the ideal generated by $h_1,h_2.\dots$ and $I$ is the unit ideal. Therefore:

\[1 = a_1h_2+a_2h_2+\cdots+{a_n}h_n \mod I, \text{ for some polynomials }a_i\]

By multiplying both sides by $f$ we get:\begin{align*}
    f &= a_1h_1f + a_2h_2f +\cdots \mod I\\
      &= a_1g_1 + a_2g_2+\cdots \mod I 
\end{align*}

Define \[\tilde{f} = a_1g_1 + a_2g_2+\cdots\]

Note that for all $i,j$ we have $\frac{g_j}{h_j} = f = \frac{g_i}{h_i}$  on the dense open sets $U_i\cap U_j$, so we see that ${h_i}{g_j} = {h_j}{g_i}$ on $Y$. 

So: \begin{align*}
    h_i\tilde{f} &= a_1{h_i}g_1 + a_2{h_i}g_2+\cdots\\
                 &= a_1{h_1}g_i + a_2{h_2}g_i+\cdots\\
                 &= g_i \mod I
\end{align*}

So on $U_i$ we have $\tilde{f} = \frac{g_i}{h_i}$, so $f = \tilde{f}$ is a polynomial.
\end{proof}
\end{proposition}

\subsubsection{Regular functions on quasi-projective variety}

\begin{definition}
    $U$ is \textbf{quasi-projective} is it is an open subset of a \textit{projective} variety.
\end{definition}

Any quasi-projective variety is covered by a finite number of open subsets of affine subvarieties, since projective space is covered by a finite number of copies affine space. 

\begin{definition}
    A function on a quasi-projective variety is \textbf{regular}, if it is \textbf{locally regular}. I.e. each point has a nbh that is an open subset of affine nbh where our function is regular (in the sense as in \ref{regular function} ).
    
    If $Y$ is quasi-projective, for each open subset $U\subseteq Y$, we have a ring $\mathcal{O}(U)$ of regular functions.
\end{definition}

\begin{theorem}\label{sheaf of rings}
    Suppose that $Y$ is quasi-projective and $U\subseteq Y$, is open such that $U = U_1\cup U_2\cup \ldots$, where $U_i$ are open then:\begin{itemize}
        \item If $f\in \mathcal{O}(U)$ is such that $f= 0$ on $\mathcal{O}(U_i)$, then $f = 0$.
        \item If $f_i\in \mathcal{O}(U_i)$ and $f_i=f_j$ on $U_i\cap U_j$ then we can find $f\in \mathcal{O}(U)$ such that $f = f_i$ on $U_i$.
    \end{itemize}
\end{theorem}
(We will see later that the regular function on open sets form what is called a \textbf{Sheaf of rings})

\begin{example}
    Find regular functions on $\mathbb{P}^1$, we know that the regular functions on $\mathcal{A}^1 = k[x]$. 

    To specify a regular function on $\mathbb{P}^1$, pick a regular function on $\mathbb{A}^1$, i.e. $\in k[x]$ and another regular function on $\mathbb{A}^1$, i.e. in $k[y]=k[x^{-1}]$ such that they are the same on $\mathbb{A}^1\setminus 0$, i.e. in $k[x,x^{-1}]$.

But if $f\in k[x]$ and $g\in k[x^{-1}]$ is such that $f = g$ in $k[x,x^{-1}]$ this means that $f = \text{ constant}$. All regular function on the projective line are constant.
\end{example}

\subsubsection{Morphisms of varieties}
We will know define what a morphisms of varieties is, and show how to make the category of varieties.

\

Say we have $X,Y$ varieties, how do we define a ``nice'' function $f\colon X\rightarrow Y$? Well we know what a ``nice'' function $Y\colon k$ looks like, it is a regular function. The correct definition of $f$ is such that the composition of $f$ and a regular function is a regular function.

\begin{definition}
    $f$ is called a \textbf{morphism} if for any open $U\subseteq Y$, any regular $g\colon U\rightarrow k$, then $g\circ f$ is regular on $f^{-1}(U)$. (This is a morphism of \textbf{ringed spaces}, this is not the same a morphism of locally ringed spaces which we will define later)

    The Quasi-projective varieties and morphisms form a \textbf{category}. 
\end{definition}

\begin{definition}
    A \textbf{ringed space} is a topological space with a sheaf of rings. 
    
    I.e. with the properties listed in Theorem\ref{sheaf of rings} 
\end{definition}

\paragraph*{Examples of Ringed Spaces:}

\begin{itemize}
    \item Topological spaces are ringed spaces, by taking continuous functions on open sets.
    \item $C^1$ manifolds, where differentiable functions with continous derivatives.
    \item $C^2$ manifolds, where twice differentiable
    \item $\ldots$
    \item $C^\infty$ (smooth) manifolds, ringed space given by taking all smooth functions.
    \item $C^\omega$ (analytic) manifolds
    \item Algebraic varieties over $\C$ or $\R$
\end{itemize}

Algebraic varieties over $\C$ or $\R \subseteq C^\omega \subseteq C^\infty\subseteq \ldots \subseteq C^2\subseteq C^1\subseteq $ Top space manifolds.  

\

As we go in further in the chain in the direction of topological spaces, things get ``floppier'', we can bend the manifolds very easily. Whilst algebraic varieties are very rigid, hard to bend. The division is made in between $C^\omega$ and $C^\infty$. Recall by analytical continuation if we define a function in one point, it is ``automatically'' defined near other points.
The function is very rigid and we can't modify it. But for smooth functions, we can make smooth function that are zero somewhere and non-zero somewhere else (bump functions) so we can bend the function quite easily.

\

So the category of algebraic varieties is very similar to the category of smooth or topological manifold. The main difference is that we have a different choice of rings, where the rings determine the function we are interested in.

\paragraph*{Locally ringed spaces:}
\begin{definition}
    \textbf{Local ring at a point }$p\in X$, where $X$ is a topological space with a sheaf of rings.

    \

    For each open set $U$ of $X$ we have a ring $\mathcal{O}(U)$, such that if $U\subseteq V$ we have a restriction map $\mathcal{O}(V)\rightarrow \mathcal{O}(U)$, the local ring at $p$ is informally ``functions defined near ''$p$. 

    More precicely, an element of the local ring is given by:\begin{itemize}
        \item An open set $U\ni p$
        \item A function $f\in \mathcal{O}(U)$
    \end{itemize}

    However $(f,U),(g,V)$ are considered to be the same if we can find $p\in W\subseteq U\cap V$ such that $f = g$ on $W$. The set of equivalence classes is a ring called the \textbf{local ring at p}.
\end{definition}

\

All of our earlier examples (topological spaces, smooth manifolds, analytic manifolds) if we do this construction we get a local ring, it has a unique maximal ideal which is equal \{functions vanishing at $p$\}. In these cases we call the ringed space a \textbf{locally ringed space}.

\paragraph*{Examples}

\begin{example}
    We take the hyperbola $xy = 1$ which is a subset of the plane an the line $\mathcal{A}^1\setminus 0\subseteq \mathcal{A}^1$.

    Then the maps $(x,y)\rightarrow x$ and $x\rightarrow (x,x^{-1})$ are morphisms from the hyperbola to the $\mathcal{A}^1\setminus 0$ and from $\mathcal{A}^1\setminus 0$ to the hyperbola respectively. Furthermore these maps are inverses of eachother.

    So these two objects are isomomorphic in the category of quasi-projective varieties.
\end{example}

\begin{example}
    Let us take the curve $y^2 = x^3$, there is a map from the affine line to this curve:\begin{align*}
        \mathbb{A}^1 &\rightarrow (y^2 = x^3)\\
        t&\rightarrow (t^2.t^3)
    \end{align*}
    This is \begin{enumerate}
        \item continuous
        \item Bijection
        \item Homeomorphism
        \item Morphisms of varieties
        \item \textbf{NOT} an isomorphism of varieties
    \end{enumerate}
So even though this is a morphism of varieties and an isomorphism of the underlying topological space, this does not imply that this is an isomorphism of varieties
    This is intuitive since $(y^2 = x^3)$ has a singularity but $\mathbb{A}^1$ does not. There is no way to define a regular map from this curve back to $\mathbb{A}^1$, we can try $t = \frac{y}{x}$, but this is not regular at $x=0$.
\end{example}

\section{Affine algebraic sets and commutative rings}
Relation between morphism of varieties and homomorphism of the corresponding coordinate rings. 

\begin{theorem}
    Suppose $Y$ is affine, then morphisms from $X$ to $Y$ are ``same as'' homomorphisms of rings from $\mathcal{O}(Y)$ to $\mathcal{O}(X)$.

    Note $X$ can be any quasi-projective variety.

    \begin{proof}
    Suppose $\varphi\in \text{Mor}(X,Y)$. This means we have \[X\overset{\varphi}{\rightarrow} Y\rightarrow \mathbb{A}^1\], so this gives us a homomorphism of rings $\text{Hom}(\mathcal{O}(X),\mathcal{O}(Y))$, where for $\psi\in \text{Mor}(Y,\mathbb{A}^1)$ we associate $\psi\circ \varphi$.
This works for all $Y$.

Now we will construct a map from $\text{Hom}(\mathcal{O}(Y),\mathcal{O}(X))$ to $\text{Mor}(X,Y)$. Suppose that $h\in \text{Hom}\text{Hom}(\mathcal{O}(Y),\mathcal{O}(X))$, note we can think of $\mathcal{O}(Y)$ as $k[x_1,\ldots,x_n]/I$, since $Y$ is affine $\subseteq \mathbb{A}^n$.


We construct $\psi\colon X\rightarrow Y$, note since $x_1,\ldots,x_n\in k[x_1,\ldots,x_n]/I = \mathcal{O}(Y)$ we can look at elements $h(x_i)$.

If $p\in X$, $(h(x_1)(p),\cdots,h(x_n)(p))\in k^n$, this defines a map $X\rightarrow \mathbb{A}^n = k^n$. We claim that this is $\psi$.
\begin{exercise}
    We need to check:\begin{enumerate}
        \item The image is in $Y$, this follows from the fact that $h(I) = 0$
        \item $\psi$ is a morphism, this follows from $x_i\circ\psi$ is regular on $X$ for each $x_i$
        \item Need to check this is the inverse of the map $\varphi$.
    \end{enumerate}        
\end{exercise}
\end{proof}

Affine varieties over $k$ are the ``same as'' finitely generated algebras over $k$ with no nilpotents. So we don't only get a one to one correspondence of the objects but also of the morphisms. ``The same as'' means that the first category is equivalent to the opposite of the second. (Note equivalent categories are hard to define precisely c.f. McLane category for the working mathematician; informally we can treat varieties as the same as fin gen algebras reversing the morph)


\end{theorem}
\paragraph*{Consequences}
\begin{example}
    Products of varieties is equivalent to coproduct of algebras. Note a coproduct is a an object $R\cup S$ such that these maps commute:    \[\begin{tikzcd}
        C  & &\\
        & R\cup S \arrow[ul]  & \arrow[l] \arrow[ull, bend right] R \\
        & \arrow[uul, bend left] \arrow[u] S &
        \end{tikzcd}
        \]
Recall in commutative rings the coproduct is just the tensor product. So the product of affine varieties corresponds to the tensor product of commutative rings.

Note this fails for over non-perfect fields. If $K$ is an inseperable extension of $k$, then $K\otimes k$ may have NILPOTENT elements. 
\end{example}

\begin{definition}\textbf{Algebraic group}
    An algebraic group G, is an algebraic variety with morphisms:\begin{itemize}
        \item $G\times G\rightarrow G$, product
        \item $G\rightarrow G$, the inverse
        \item $\text{point}\rightarrow G$, identity
    \end{itemize}
    Where these maps follow the axioms of groups.
\end{definition}

\begin{example}
    $G_a$ is the affine line $\mathbb{A}^1$with $\mathbb{A}^1\times \mathbb{A}^1 \rightarrow \mathbb{A}^1$ is $(x,y)\rightarrow x+y$.

    The inverse is $x\rightarrow -x$, and the identity is $0$.

    \

    Now from the point of view of rings:\[k[x]\otimes k[y]\leftarrow k[z] \text{ where }z\rightarrow x+u\]

    So we have two maps:\[\begin{cases}
        k[x]\otimes k[y]\rightarrow k[z] \text{ ring multiplication}\\
        k[x]\otimes k[y]\leftarrow k[z]  \text{Group operation} 
    \end{cases}\] 

    These maps are in some sense duals of eachother, this is \textbf{Cartier Duality}.

\end{example}


\begin{example}
    \begin{align*}
        G_m\colon & \mathbb{A}^1\setminus 0 \ (x,y)\rightarrow xy\\
        &k[x,x^{-1}] \rightarrow k[x,x^{-1}]\otimes k[x,x^{-1}] \text{ by }x\rightarrow x\otimes x
    \end{align*} 
\end{example}

\begin{example}
    \begin{align*}GL_2(k)\colon \begin{pmatrix}
        a_1 & b_1\\
        c_1 & d_1
    \end{pmatrix}\begin{pmatrix}
        a_2 & b_2\\
        c_2 & d_2
    \end{pmatrix}&=\begin{pmatrix}
        a_1a_2+b_1c_2 & a_1b_2+b_1d_2\\
        c_1a_2+d_1c_2 & c_1b_2+d_1d_2
    \end{pmatrix}\\
    \begin{pmatrix}
        a&b\\
        c&d
    \end{pmatrix}^{-1} &= \frac{1}{ad-bc}\begin{pmatrix}
        d&-b\\
        -c&a
    \end{pmatrix}
\end{align*}

What is the coordinate ring?

\[R = k[a,b,c,d, (ad-bc)^{-1}] = k[a,b,c,d,l]/((ad-bc)l = 1) \]
With product \begin{align*}
    &R\rightarrow R\otimes R\\
    & a \rightarrow a_1a_2+b_1c_2\\
    & b\rightarrow a_1b_2+b_1d_2\\
    & c \rightarrow c_1a_2+d_1c_2\\
    & d\rightarrow c_1b_2+d_1d_2
\end{align*}

The inverse \begin{align*}
    &R\rightarrow R\\
    & a\rightarrow \frac{d}{ad-bc}\\
    & b\rightarrow \frac{-b}{ad-bc}\\
    &\vdots
\end{align*}

The coordinate rings of affine alg groups like this are called \textbf{Hopf Algebras}.
\end{example}

\paragraph*{The twisted cubic}

We will show that the twisted cubic is isomomorphic to $\mathbb{P}^1$. Recall that the twisted cubic is \[(w\colon x\colon y\colon z) = (s^3\colon s^2t\colon st^2\colon t^3)\in \mathbb{P}^3\]
Equivalently it can be described as\[\text{the vanishing of }\{wy-x^2, xz-y^2, wz-xy\}\]
So the coordinate ring is given by $k[w\colon x\colon y\colon z]/(wy-x^2, xz-y^2, wz-xy)$.

\

First of all there is an obvious map from $\mathbb{P}^1\rightarrow $ twisted cubic:\[(s\colon t)\rightarrow (s^3\colon s^2t\colon st^2\colon t^3)\]
This is an isomorphism of topological space, but this doesn't imply that this is an isomorphism of varieties.

\

We need to find a regular map: Twisted cubic $\rightarrow \mathbb{P}^1$. To to this we will cover the cubic by open affine sets $U_i$ and choose functions on each $U_i$, and check they are the same on $U_i\cap U_j$. 


Recall that $\mathbb{P}^3$ is covered by four copies of $\mathbb{A}^3$, the twised cubic is covered by two affine sets $w\neq 0$ and $z\neq 0$. Indeed from the definition we see that $w=z=0$, then $x=y=0$.

So we have maps: \begin{align*}
    w\neq 0&\colon (w\colon x\colon y\colon z)\rightarrow (w\colon x)\in \mathbb{P}^1\\
    w\neq 0&\colon (w\colon x\colon y\colon z)\rightarrow (y\colon z)\in \mathbb{P}^1
\end{align*}
Furthermore these maps are compatible, on the intersection $w\neq 0, z\neq 0$ since $wz=xy$ on the cubic we see $(w\colon x) = (y\colon z)$. So we have a morphism from the cubic to $\mathbb{P}^1$, it is straightforward to see that this is the inverse of the map above.

\

Recall two affine varieties are isomorphic if and only if there corresponding coordinate ring are isomorphic. Is this true for projective varieties? No!

\

If we look at $\mathbb{P}^1$ it's corresponding graded ring is $k[x, y]$ where $\deg x = \deg y = 1$. On the cubic the corresponding graded ring is $k[w,x,y,z]/(wy-x^2, xz-y^2.wz-xy)$ with $\deg x,y,z,w = 1$

These two rings are NOT isomorphic. In degree 1, the first ring is 2 dimensional but the second one is four dimensional.

\paragraph*{$\mathbb{A}^2\setminus (0,0)$ is NOT affine}
We will show this space is not affine by calculating the ring of regular functions on it. In order to do this we will cover $\mathbb{A}^2\setminus (0,0)$ by two affine sets $U,V$, 

We let $U=\mathbb{A}^2 \setminus \text{x-axis}$ and $V=\mathbb{A}^2 \setminus \text{y-axis}$, their coordinate rings are $k[x,y,y^{-1}]$ and $k[x,y,x^{-1}]$ respectively. The coordinate ring of the intersection has coordinate ring $k[x,y,x^{-1},y^{-1}]$.

So we are looking for functions on $l[x,y,x^{-1},y^{-1}]$ that can be expressed as polynomials in $k[x,y,x^{-1}]$ and $k[x,y,y^{-1}]$. These are nothing but the polynomials in $k[x,y]$.

So the regular function on $\mathbb{A}^2\setminus (0,0)$ are $k[x,y]$. Now notice since we have a morphism $\mathbb{A}^2\setminus (0,0)\rightarrow \mathbb{A}^2$, we have the corresponding homomorphism from the coordinate rings: $k[x,y]\leftarrow k[x,y]$.

Since morphism of affine varieties correspond exactly to homomorphism of coordinate ring, if $\mathbb{A}^2\setminus (0,0)$ where affine then since $k[x,y]\leftarrow k[x,y]$ is an isomorphism then $\mathbb{A}^2\setminus (0,0)\simeq \mathbb{A}^2$, but this is not possible. So $\mathbb{A}^2\setminus (0,0)$ is NOT AFFINE.

Note that $\mathbb{A}^1$ is affine, since in general if we remove a subset that is codimension $1$ from an affine variety it remains affine, but is we remove a subset of codimension 2(like for $\mathbb{A}^2\setminus (0,0)$) or more it quite often no longer affine. 
\textbf{anolgue in complex analysis}: functions can't have zeros that are of codimension 2 or more.


\paragraph*{More examples}
We want to show that the \textit{product} of two algebraic varieties is also an algebraic variety. To show this we just need to show that the product of two projective space is a projective variety.

\

Recall the Segre embedding:\[\mathbb{P}^m\times\mathbb{P}^n\rightarrow \mathbb{P}^{mn+m+n} \text{ given by } ((x_0\colon \ldots x_m),(y_0\colon \ldots y_n)) \rightarrow (x_0y_0\colon x_0y_1\colon \ldots\colon x_my_n) = (z_{00}\colon z_{01}\ldots)\]

We want to show that the image of this embedding is a product in the category of varieties. 

\

We just need to show that the image of the Segre embedding maps to $\mathbb{P}^m$ and $\mathbb{P}^n$ and is universal with this property.

If $z_{00}\neq 0$, we have a map:\begin{align*}
    (SEGRE)&\rightarrow \mathbb{P}^m\\
    (z_{00}\colon z_{01}\colon \ldots) &\rightarrow (z_{00}\colon z_{10}\colon z_{20}\ldots)\in \mathbb{P}^m
\end{align*}

If $z_{01}\neq 0$:\begin{align*}
    (SEGRE)&\rightarrow \mathbb{P}^m\\
    (z_{00}\colon z_{01}\colon \ldots) &\rightarrow (z_{01}\colon z_{11}\colon z_{21}\ldots)\in \mathbb{P}^m
\end{align*}

Do these maps coincide on the intersection of the open sets? Yes it follows from the fact that $z_{ij}z_{kl} = z_{il}z_{kj}$, we have that:\begin{align*}
    (z_{00}\colon z_{10}\colon z_{20}\ldots) &= (z_{01}z_{00}\colon z_{01}z_{10}\colon z_{01}z_{20}\ldots) \text{ since }z_{01}\neq 0\\
    &= (z_{01}z_{00}\colon z_{11}z_{00}\colon z_{21}z_{00}\ldots)\\
    &= (z_{01}\colon z_{11}\colon z_{21}\ldots)
\end{align*}. Similarly for the other open subsets covering the SEGRE variety we can check that we get a well defined map as above on $\mathbb{P}^m$ and they are the same on the intersection.

Now assume we have any other variety $C$ mapping to $\mathbb{P}^m$ and $\mathbb{P}^n$. Since we can cover $C$ by open affine varieties, if we show that each of these open subsets give us a map to SEGRE, since they must coincide in their intersections by uniqness of map of product, it will gives us a map from $C$ to SEGRE.

So we can assume that $C$ is affine. \begin{align*}
    C &\underset{\text{morphism}}{\rightarrow} \text{open subset}x_0\neq 0 \text{ at }\mathbb{P}^m\\
\end{align*}
Is given by $(\bverteq{f_0}{1}, f_1,\ldots,f_m)$ where the $f_i$ are regular on $C$.

Likewise for $y_0\neq 0\in \mathbb{P}^n$, we have \[(\bverteq{g_0}{1}, g_1,\ldots,g_n)\]

\

So we get a map from $C$ to SEGRE by $C\rightarrow (f_0g_0\colon f_0g_1\colon \ldots \colon f_mg_n)$


\subsection{Automorphism of space}
In general the group of automorphism of an algebraic set is the trivial group, however affine space and projective sapce are very symetrical and have a lot of automorphisms.

\paragraph*{Automorphism of affine space}

First of all we look at $\mathbb{A}^1$, recall it's coordinate ring is $k[x]$, so morphisms of $\mathbb{A}^1\rightarrow \mathbb{A}^1$ correspond to \begin{align*}
    k[x] &\rightarrow k[x]\\
    x&\rightarrow p(x), \text{ for some polynomial}
\end{align*}

This is invertible iff $p(x) = ax+b$ for $a\neq 0$. We call the group of automorphism the $``ax+b$ group'' for this reason. We can identify it with the group:\[\begin{pmatrix}
    a & b\\
    0 & 1
\end{pmatrix}\]
Properties:\begin{enumerate}
    \item This group is noncommutative
    \item It has got a normal subgroup, consisting of the elements of the form $\begin{pmatrix}
        1 & b\\
        0&1
    \end{pmatrix}\simeq k$, the quotient is isomorphic to $k^\ast$
\end{enumerate}

This group is a counterexample to a lot of things.

\

Now what about $\mathbb{A}^2$? Some obvious automorphisms are:\[\begin{array}{ccc}
    k[x,y]&\rightarrow &k[x,y]\\
    x&\rightarrow &ax+by+c\\
    y&\rightarrow &dx+ey+f
\end{array}\]

Where $\det \begin{pmatrix}
    a&b\\d&e
\end{pmatrix}\neq 0$

The set of elements of this form does give us a group of automorphism, but it is not the full group. The full group of automorphisms is much bigger.

For example we have the following automorphism:\[\begin{array}{ccc}
    x\rightarrow x; & &x\rightarrow x\\
    &\text{inverse}&\\
    y\rightarrow y+p(x);&&y\rightarrow y-p(x)
\end{array}\]
For some polynomial $p(x)$. This give us an infinite dimensional abelian subgroup.

\

Suppose we have an automorphism of $\mathbb{A}^n$ where we map $x_i\rightarrow f_i(x_1,\ldots,x_n)$, when is this an automorphism? 

If we look at the Jacobian, $\det(\frac{\partial f_i}{\partial x_j})$. 

The Jacobian of $FG$ is the Jacobian of $F$ times the Jacobian of $G$, so if $FG = 1$ then Jacobian of $F$ is invertible. So we can conjecture that our map is an automorphism of affine space if and only if the jacobian is invertible.

\textbf{Jacobian Conjecture}: If the Jacobian of $F$ is invertible then $F$ is an automorphisms. This is still a very difficult open problem.


\paragraph*{Morphisms of projective space}

We will start looking at $\mathbb{P}^1$.

If we have a map $\mathbb{P}^1\rightarrow \mathbb{P}^1$, we can restrict it to a map from $\mathbb{A}^1\rightarrow \mathbb{P}^1$. This gives us a map:\[\underbrace{\text{open subset }\mathbb{A}^1\rightarrow \mathbb{A}^1}_{\text{rational function } \frac{p(x)}{q(x)}}\subseteq \mathbb{P}^1\]
Since $\mathbb{P}^1 = \mathbb{A}^1\cup \infty$.

\

Conversely given a rational function from an open subset of $\mathbb{A}^1$ to $\mathbb{A}^1$ can be extended to a map from $\mathbb{P}^1$ to $\mathbb{P}^1$.

\

So this gives us all the morphisms of $\mathbb{P}^1$ to itself, which of these maps have inverses? Those of the form $\frac{ax+b}{cx+d}$, where $\det\begin{pmatrix}
    a&b\\c&d
\end{pmatrix}\neq 0$

\[GL_2(k) = \{\begin{pmatrix}
    a&b\\c&d
\end{pmatrix}\mid \det\neq 0\]This almost gives us the group of automorphism we need to quotient out by the diagonal matrices since \[\begin{pmatrix}
    a&0\\0&a
\end{pmatrix} \text{ which corresponds to the map }x\rightarrow \frac{ax}{a} = x\]

So the group of automorphisms is:\[\mathbb{P}GL_2(k) = GL_2(k)/(\begin{pmatrix}
    a&0\\0&a
\end{pmatrix})\]

This is the projective general linear group, it is of dimension $4-1= 3$ and contains the $ax+b$ group as a subgroup.

\

Recall the Riemann sphere from complex analysis, the endomorphisms of the projective line are the same as the endomorphisms of Riemann space. 
However the group of endomorphisms of the complex plane is the group of entire functions which is much bigger than the group of endomorphisms of the affine line, which is only the group of polynomials. 

\

For the projective line algebraic geometry is very similar to doing this analytically, whereas in the affine line it is different. C.f. \textbf{SERRE: Géométrie Analytique et Géométrie Algébrique}, roughly speaking it says that working over projective manifolds, then analytic maps tend to be algebraic.
But if we are working with affine manifolds analytic things are much more common then algebraic things and quite often not algebraic.

\paragraph*{Affine plane}

We look at at the map \[\begin{array}{cc}
    \mathbb{A}^2&\rightarrow\mathbb{A}^2\\
    (x,y)&\rightarrow (x,xy)
\end{array}\]

What is the image of this? It is $\{(x,y)\mid x\neq 0\}\cup \{(0,0)\}$, the image is not open, closed or even locally closed!



\section{The Ax Grothendieck theorem}
\begin{definition}
    A \textbf{first order} statement is a statement that can be written in first order logic with these logical connectives:\[\wedge \text{(and), }  \lor \text{(or), } \neg \text{(not), }\rightarrow\text{(implies.)} \] 

And with the operators of our field:\[a+b, a\cdot b, a-b\]

And we are allowed to quantify over all elements we are working on:\[\forall x\in X, \ \exists x\in X\]
\begin{example}
    To say a field is algebraically closed, we mean for all polynomials $a_nx^n+\cdots$, there exists a root, but this is a wrong statement in first order logic since we can't say \textit{ for all polynomials}, we can only talk about elements in our field.
    
    \

    \textit{We can say} for all polynomials of degree $n$, $a_nx^n+\cdots$ since this is equivalent to saying for all $a_n,a_{n-1},\cdots, a_0\cdots$

    \

    So in order to say that $X$ is algebraically closed we need to take an infinite statements of first order logic.
\end{example}
\end{definition}

\begin{theorem}
    The Theory of alg closed field of a given characteristic $p$, is \textbf{COMPLETE} (everything is provable or disprovable).
    \begin{proof}
        There is a unique algebraically closed field of a given characteristic and cardinality $c$. From theorem of first order logic we know that if there is a unique model of some theory in some characteristic then that theory is complete. (c,f, Gödel's completeness theorem).
    \end{proof}
\end{theorem}

\

From this we can say that anything frue for algebraically closed fields of large characteristic is true for algebraically closed fields of characteristic zero.

\paragraph*{How do we say something is a field of characteristic zero?}
We can't say this with first order logic with finite number of statements, we need to say $2x\neq 0, 3x\neq 0, 5x\neq 0, \ldots$.

So suppose we prove something about algebraically closed fields of characteristic zero, every proof uses a finite number of axioms. If we proved something about algebraically closed fields of characteristic zero, it is true for alg closed field sufficiently large characteristic, since our proof can only eliminate a finite number of characteristics (the $2x\neq 0,$ etc... statements).

Conversely if something is true for all algebraically closed fields in large characteristics it is also true for characteristic fields in characteristic $0$. This implies the following theorem: 

\paragraph*{Ax Grothendieck Theorem}\begin{theorem}
    Let $V$ be an alegbraic variety. If we have a regular map $f\colon V\rightarrow V$ over an algebraically closed field $k$, if $f$ is injective it is surjective,
\end{theorem}
\paragraph*{Counter-example}\begin{itemize}
    \item If $k$ is not algebraically closed this thorem is false, for example take $k=\Q$ and $f(x) = x^3$.
    \item If $k=\C$ we can take $f(x) = x^2$, this map is surjective but not injective.
\end{itemize}

\begin{proof}
    This thorem is trivial if $k$ is finite. It is also true over any algebraic extension of a finite field (the coeffs of all equations defining a variety and the coeffs of a point, all lie in some finite field. So we if we have an injective map over an algebraic extension if we have to show it is surjective take a finite extension of the finite field containig a certain point, and apply the finite case.)


    Since this is true for all algebraic extension of a finite field it is true for all algebraically closed fields. It is true for all algebraic closures of finite fields, a result from model theory tells us that a (first order) statement is true for alg closed field in char $0$ if and only if it is true for alg closed fields of all sufficiently large characteristic.
    
    A second result from logic says that if a (first order) statement is true for one algebraically closed field of a given characteristic it is true for all.  This is the Lefschetz principle.
\end{proof}
For other proofs where we prove something in characteristic $p$ in order to get a statement in characteristic zero, cf Mori's Bend or Break argument.

\lecture{Rational maps}

We will now start the lecture on \textbf{Rational maps}

\section{Rational functions and rational maps}
\begin{definition}
    Suppose $Y$ is an affine variety, then $\mathcal{O}(Y)$ is an integral domain. We can form the quotient field $K(Y)$, we call these \textbf{rational functions}(on $Y$).
\end{definition}

For example if $Y=\mathbb{A}^2$, then $\mathcal{O}(Y) = k[x,y]$, so $K(Y) = k(x,y)$, i.e. the elements of the form $\frac{p(x,y)}{q(x,y)}$, where $q(x,y)\neq 0$.

\

Recall regular functions on affine varieties correspond to holomorphic functions for Riemann surface. Whereas rational functions correspond to meromorphic functions (the field of quotient of holomorphic functions).

\paragraph*{Example with projective variety}
If $Y$ is a projective variety, we can't do the previous construction since the ring of regular functions on $Y$ is $k$. So the field of quotients is $k$, which is not interesting.

So we need to define rational functions on projective variety in a different way.

\begin{definition}
    \textbf{Rational function} is given by:\begin{enumerate}[label = (\alph*)]
        \item Dense (affine) open set $U$.
        \item Rational function on $U$.
    \end{enumerate}

Suppose we have two rational functions $(f_1,U_1)$ and $(f_2,U)2$ they are identified if $f_1 = f_2$ on a dense open set contained in $U_1$ and $U_2$.

\

The rational functions are given by taking a direct limit over desnse open affine sets $U$ of $K(U)$.
\end{definition}

It is not difficult to check that the rational functions on $K(Y)$ form a field (note we take $Y$ to be an irreducible variety).

\subsection{Rational maps between varieties}

Suppose $X,Y$ are varieties. A rational map $X\rightarrow Y$ is a regualar map $U\rightarrow Y$, where $U$ is dense and open in $X$. We identify two of these maps in the obvious way. $(f_1,U_1) = (f_2,U_2)$ if $f_1=f_2$ on $U\subseteq U_1\cap U_2$, where $U$ is dense and open.

Rational maps $X\rightarrow \mathbb{A}^1$ are rational functions on $X$.

\

Unfortunately rational maps do not form a category, since composition is not defined. Taking the rational maps:\[\mathbb{A}^1\underset{0}{\longrightarrow}\mathbb{A}^1\underset{x\rightarrow \frac{1}{x}}{\longrightarrow}\mathbb{A}^1\]

The image of the first map is $0$, but the domain of the second map does not contain zero, so composition is not defined.

\begin{definition}
    A rational map is said to be \textbf{dominant}, if it's image contains a dense open set.
\end{definition}
If we restrict ourselves to dominant rational maps then composition is well-defined.


\paragraph*{Birational}

\begin{definition}
    We say $X,Y$ are \textbf{birational} if we got rational maps from $X\rightarrow Y$ and $Y\rightarrow X$, whose composition is equal to the identity on a dense open subset.
\end{definition}
So isomomorphic maps are birational, but not all birational map is isomomorphic.

\begin{example}
    Take varieties $\mathbb{A}^1,\mathbb{P}^1, xy= 1, x^3=y^2$.

    We have seen earlier that these four varieties are all birational to eachother. $\mathbb{A}^1$ is $\mathbb{P}^1$ minus a point, $xy=1$ is identified to $\mathbb{A}^1$ minus a point, and there is a map from $\mathbb{A}^1$ onto $x^3=y^2$ which is an isomorphism onto $\mathbb{A}^1$ minus a point.

    However no two of these varieties are equivalent. For example the curve defined by $x^3=y^2$ has some sort of singularity, etc$\ldots$
\end{example}

\begin{example}\textbf{Two dimensional example}
    $\mathbb{P}^1\times \mathbb{P}^1, \mathbb{P}^2,\mathbb{A}^2,\mathbb{A}^2\setminus (0,0)$.

    These are all birational to $\mathbb{A}^2$, but no two of these are isomorphic.
\end{example}

\begin{definition}
    Varieties that are birational to an affine space are called \textbf{rational varieties}.
\end{definition}
Are there non-rational varieties? Yes.
\begin{example}\textbf{Fermat's Last theorem for polynomials}
    $x^3+y^3 = 1$ is not birational to $\mathbb{A}^1$. More generally there is no dominant rational map from the affine line onto this curve. 

    Assume we had a map from $\mathbb{A}^1$ to this curve which is dominant (so non-constant). This means we have \[{x(t)}^2+{y(t)}^2 = 1, \text{ where }x,y \text{ rational functions}\]

    By clearing denominators we can find $f,g,h$ polynomials such that: \[{f(t)}^3 + {g(t)}^2 = {h(t)}^3 \]
We can assume they are coprime, and let's work over $\C$ to factor them:
\[
    \underbrace{(f(t)+g(t))}_{\text{cube, we call }h_1^3}\underbrace{(f(t)+\omega g(t))}_{\text{cube, we call }h_2^3}\underbrace{(f(t)+\omega^2 g(t))}_{\text{cube, we call }h_3^3} = {h(t)}^3 \text{ note }K[t]\text{ form a }UFD    
\]

\begin{remark}
    We can use Bézout or something similar to show that $f(t)+g(t)$, $f(t)+\omega g(t)$ and $f(t)+\omega^2 g(t)$ are coprime. Which is why we know they are cubes, since the product of coprime polynomials is a cube iff they are each a cube.


\end{remark}


\

So we get three equations: $f+g = h_1^3$, $f+\omega g = h_2^3$, $f+\omega^2 g = h_3^3$. We can eliminate $f,g$ and get a linear relation between $h_1^3,h_2^3,h_3^3$. If we multiply $h_i$ by constants we get another relation of polynomial that says:\[{h_1'}^3+{h_2'}^3={h_3'}^3 \text{ where }h_i' ={a_i}{h_i}, \text{ }a_i\in \C \]

So if $f,g$ and $h$ are not constant, then $h_1',h_2',h_3'$ are non-constant have smaller degree than $f,g$ and $h$. So for any solution of this equation we get a solution of smaller degree. $\bot$

Note this works for any exponent that is at least $3$, the result is false for exponent two though, since for example we have:\[{(1-t^2)}^2 + {(2t)}^2 = {(1+t^2)}^2 \]
This comes from the fact that $x^2+y^2=1$ IS rational as was seen in lecture $1$.

\

The reason we can't replicate our previous proof is that we can't create a relation between $h_1,h_2,h_3$ as we did above. We end up with \[{(t^2+1)}^2 = {(\underbrace{t+i}_{h_1})}^2{(\underbrace{t-i}_{h_2})}^2\]

\

Fermat's Last theorem for polynomials is almost trivial, we have just done it. Whereas for the integers it is incredibly difficult. The reason for the difference is that polynomials over a field are ``nicer'' than the cyclotomic field we need to use for the integer case. In particular polynomials over a field form a $UFD$ and all units are nth powers for any n. 
\end{example}

\section{Elliptic functions and cubic curves}
\begin{definition}
    An \textbf{elliptic function} is a meromorphic function such that:\begin{equation*}
        f(z+\lambda) = f(z)
    \end{equation*}
    For all $\lambda\in L$, where $L$ is a lattice in $\C$, i.e. there are $a,b\in \C$ such that $L = \{na+mb \mid n,m\in Z\}$.

Note if $f$ is holomorphic it will be bounded so constant by Liouville. So if $f$ is non-constant it must have poles.
\end{definition}

How can find functions with this property? One way is take a function $g$ and define:\begin{equation*}
    f(z) = \sum_{\lambda\in L}g(z+\lambda)
\end{equation*}
Note the function $g$ we need to choose must be chosen so that this sum converges. For example $g(z) = \frac{1}{z}$ doesn't converge, so we can't use it. If we take $g(z) = \frac{1}{z^3}$, it converges well. If we take $g(z) = \frac{1}{z^2}$ it is ``borderline'', it doesn't converge but if we modify it slightly it does converge.

\begin{definition}
    \textbf{The Weistrass P-function:}\[\wp(z) = \frac{1}{z^2}+\sum_{\lambda\neq 0}\bigg(\frac{1}{{(z-\lambda)}^2}-\underbrace{\frac{1}{\lambda^2}}_{\text{constant}}\bigg) \]    
\end{definition}

Note we can show that $\wp'(z)$ is doubly periodic so $\wp$ is doubly periodic up to a constant. So \[\wp(z+a) = \wp(z)+\text{constant} \]

This constant is zero since $\wp$ is an even function.

\paragraph*{Laurent expansion at z=0}
\[\wp(z) = \frac{1}{z^2} + 0z+\ast z^2+\cdots\]
So for the derivative:\[\wp'(z) = -2z^{-3}+\ast z+\cdots\]
\begin{align*}
    {\wp'(z)}^2 &= 4z^{-6}+\ast z^{-2}+\ast z^0+\cdots\\
    4\wp(z)^3 &= 4z^{-6}+\ast z^{-2}+\cdots
\end{align*}

So we can see that:\begin{equation*}
    \underbrace{{\wp'(z)}^2 = {4\wp(z)}^3 + \ast \wp(z)+\ast}_{\text{doubly periodic, no poles, vanishes at }z=0} + O(z)
\end{equation*}
Since it is doubly periodic, and has no poles it is constant but since it vanishes at $0$, it must be zero. So the Weistrass p function satisfies the following differential equation:\begin{equation*}
    \wp'(z)^2 = {4\wp(z)}^3-g_2\wp-g_3
\end{equation*} 

\paragraph*{Why are these things called elliptic functions?}

Since if we take the arc length of an ellipse we get what is called an \textbf{elliptic integral}:\begin{equation*}
    \int \frac{dx}{\sqrt{4x^3-ax-b}}
\end{equation*}
We find that the Weistrass p function is some sort of inverse:\begin{equation*}
    z = \int \frac{d\wp}{\sqrt{4\wp^3-g_2\wp-g_3}}
\end{equation*}

The inverses of elliptic ingegrals are doubly periodic, so doubly periodic are called elliptic functions.

\paragraph*{Back to algebraic geometry}
Let us look at the equation:\[y^2 = 4x^3-g_2x-g_3\]
If we take $y = \wp'(z)$ and $x = \wp(z)$, then $(x,y)$ lies on the curve. So we have a map from $\C\setminus L$ onto the curve $y^2 = 4x^3-g_2x-g_3$ taking $z\rightarrow (\wp'(z),\wp(z))$.

We can extend this a function from $\C$ to the projective curve by mapping the point $0$ to the point at infinity.

\

But since the map is doubly periodic we get a homeomorphism of topological spaces:\begin{equation*}
    S^1\times S^1\simeq \C/L\rightarrow \text{PROJECTIVE CURVE}
\end{equation*}
So this curve with a point added at infinity is homeomorphic to the two-dimensional torus, so this can't be rational. Since any rational curve is the same as $\mathbb{P}^1$ up to a finite number of points. And $\mathbb{P}^1\simeq S^2$, there is no way to turn a torus into a sphere by adding and removing a finite number of points.

If we have a rational map between $\mathbb{P}^1$ and our curve, it will be defined except at a finite number of points, and would have an inverse that is well-defined except at a finitie number of points. So any two birational curves are the same topologically, so these two curves can't be birational.

\paragraph*{Singly periodic functions}

We can take the non absolutely convergent sum $\sum \frac{1}{z-\lambda}$, and make it converge by summing it over $\lambda$ by order of absolute value and this a periodic function equal to \[\pi \cot(\pi z)\]

This gives rise to the product formula of the sin function:\begin{equation*}
    \sin (\pi z) = z\prod_{n\neq 0}(1-\frac{z}{n})
\end{equation*}
Taking the log derivative on both sides we get the above identity.

\section{Rationality of cubic surfaces}

Recall that for cubic curves, for example of the form $x^3+y^3 + z^3 = 0$ in $\mathbb{P}^2$ are not rational. If we look at a cubic surface, $w^3+x^3+y^3+z^3 = 0$ in $\mathbb{P}^3$, we may think it is not rational but it is!

\

Informal argument to show that cubic surfaces are rational.

\

If we take two non-intersecting lines:\begin{align*} \tag{$\dagger$}
    w+x = 0&, y+z = 0\\
    w+y = 0&, x+\zeta z = 0, \text{ where }\zeta^3 = 1
\end{align*}
These lines don't intersect, recall the point $(0\colon 0\colon 0\colon 0)$ is not in $\mathbb{P}^3$.

If we take a point on each of these lines and draw the line through the two points, the line will generally intersect the cubic surface at three points (so two points on each line and a third point on the surface). There is a problem, it is possible that the line lies interely on the cubic surface. But let us ignore that for now.

What this is giving us a point $x$ on a copy of $\mathbb{P}^1$ and $y$ on another copy of $\mathbb{P}^1$ and we get a point on the cubic surface. $\mathbb{P}^1\times \mathbb{P}^1\rightarrow $ cubic surface.

\

This map is not defined everywhere, since if $x,y$ happen to lie on a line that lies interely on the cubic surface the third point is not defined. So this map is not defined everywhere and is in fact a rational map.

\subparagraph*{Does this map have a rational inverse?}
Suppose we have a point, that lies on two different lines, i.e. we have two different $(x,y)$ and $(w,z)$ such that our point is on the line intersecting $x$, $y$ and $w,z$. These two lines form a plane which contain our lines $\dagger$, which mean they intersect which is a contradiction.

This strongely suggest that this map is map is 1 to 1 and has a rational inverse, so we can see show that in any cubic surface of two non-intersecting lines is birational to $\mathbb{P}^1\times \mathbb{P}^1$.


\paragraph*{Different argument for why most cubic surfaces are non-rational}

Pick $6$ points in $\mathbb{P}^2$, in \textbf{general position}. Points in general position, are points that don't satisfy a number of special conditions:\begin{itemize}
    \item No three are on a line
    \item Not all on a conic
\end{itemize} 

We will look at the space of all cubic functions. This space is ten dimensional, and we look at the cubics vanishing on all six points. This gives us six linear relation on a ten dimensional space, this gives us a four dimensional space spanned by $f_1,f_2.f_3.f_4$.  

If we try to define the map $(x\colon y\colon z)\rightarrow (f_1\colon f_2\colon f_3\colon f_4)$, then this map is well-defined everywhere except at six points, where all these four functions vanish.

What is the image of this map from $\mathbb{P}^2$ to $\mathbb{P}^3$? Let's assume it is a hypersurface, what is it degree? We work it out by counting the number of intersection points with the line.

Pick a line in $\mathbb{P}^3$, say $f_1=f_2=0$. By bézout's theorem which we will prove later, we can show that these have nine intersection points. Six of these are the six points we started with, which don't really count since their images aren't in $\mathbb{P}^3$.

So this leaves us with three points, so the hypersurface has degree $3$. 

The dimension of the space of cubics is $19$ and so is the dimesion of the space of cubic surfaces obtained by taking six points. (We see this with an argumennt based on the dimesnions of aut groups). So we can assume that we get all cubic surfaces in this way.

\

Everything we did here is a bit wishy-washy, this is typical of old style algebraic geometry arguments. The arguments are quite short and generally give the right answer, but every now and then they give a hoplessly \textbf{wrong} answer. While this is not a proof the result is correct, we need to do more work to show this rigourously.

\section{Blowing up a point}
Suppose we take the curve $y^2 = x^3+x^2$, it has a singularity at the origin. We simplify it by making the substiution $y = tx$. This gives us\begin{align*}
    x^2t^2 &= x^3+x^2\\
    x^2(t^2-(x+1)) &= 0
\end{align*}


So in the $x-t$ plane, we get two curves, the curve $t^2 = x+1$ and the curve $x=0$. So our original curve has been transformed into a curve without singular points, except we pick up a new line called the exceptional curve.

\href{https://youtu.be/dGnfONlZwhQ?t=118}{\includegraphics[scale = 0.25]{exceptional-curve.png}}

So this point in the plane is replaced by the red line, which contains a point for each direction going through the origin; but it doesn't include a point for the vertical line going through the origin. So blowing up includes one extra point.

\paragraph*{General form for blowing up at a point}

Suppose we have a general plane $\mathbb{A}^2$ and we blow it up at the point $(0,0)$. To do this include two new variables, $(X\colon Y)\in \mathbb{P}^1$.

\

Now look at the variety \begin{align*}
    \mathbb{A}^2&\times \mathbb{P}^2\\
    x,y&X\colon Y
\end{align*}
And we look at the subset of this such that $xY = yX$. If $(x,y)\neq (0,0)$, there is just one point of $\mathbb{P}^1$ corresponding to it. Otherwise if $(x,y) = (0,0)$ then we get the whole affine line $\mathbb{P}^1$.
\begin{definition}
    The subset of $\mathbb{A}^2\times \mathbb{P}^2$ defined by this is called the \textbf{blow up} of $\mathbb{A}^2$ at a point.
    
    So we have a map:\begin{align*}
        \text{Blow up} &\rightarrow \mathbb{A}^2\\
        x,y \ X\colon Y\rightarrow (x,y)
    \end{align*}

    And the Point$(0,0)\Rightarrow \mathbb{P}^1$.
\end{definition}
Recall $\mathbb{P}^1$ is defined by the two affine lines:\begin{itemize}
    \item $X\neq 0, \text{i.e.} X=1$. In this case $xY=y$
    \item $Y\neq 0, \text{i.e.} Y=1$. In this case $x=yX$ 
\end{itemize}

\begin{definition}
    In $n$ dimensions this is similar. Take $\mathbb{A}^n$, given by $(x_1,\cdots,x_n)$ and $\mathbb{P}^{n-1}$ given by $(X_1\colon\cdots\colon X_n)$ and look at:\begin{equation*}
        \text{the subset of }\mathbb{A}^n\times \mathbb{P}^n\text{ given at }x_iX_j = x_jX_i \forall i,j
    \end{equation*}
    The subvariety defined by all these points mapps to $\mathbb{A}^n$, the inverse image of \[(x_1,\cdots,x_n) = \begin{cases}
        \text{point, if }x\neq (0,\cdots,0)\\
        \mathbb{P}^{n-1} \text{ otherwise}
    \end{cases}\]
Now if $X_1 = 1$ we have $x_i  = x_1X_i \Rightarrow X_i = \frac{x_i}{x_1}$.
\end{definition}

\subsection{Application of Blow up: Resolving singularities}
We look at the point $y^2 = x^3$, it has a singularity at the origin. Let us look at the blow up at the origin:

\href{https://youtu.be/dGnfONlZwhQ?t=582}{\includegraphics[scale = 0.25]{resolving-singularities.png}}

\subsubsection{Higher dimensions:}


\href{https://youtu.be/dGnfONlZwhQ?t=781}{\includegraphics[scale = 0.25]{resolving-singularities-higher.png}}
\newline 

Another example is the curve $y^8 = z^5$:\begin{itemize}
    \item Blow up: $z = yt$, we get $y^5(t^5-y^3) = 0$. But $(t^5-y^3)$ also has a singularity;
    \item Blow up: $y=ts$, then $t^3(s^3-t^2) = 0$; another singularity;
    \item Blow up: $t = su$, so $s^3 = s^2u^2\Rightarrow s = 0$ or $s=u^2$, so finally we have transformed our curve into a curve without a singluarity.
\end{itemize}

\paragraph*{Withney umbrella}
We look at the set $xy^2 = z^2$. It gives us a plane that sort of folds in on itself, with the axis $y=z=0$ a line of singular points. The worst point is given at $(0,0,0)$. So let's blow it up.

\href{https://youtu.be/dGnfONlZwhQ?t=1189}{\includegraphics[scale = 0.25]{witney-umbrella.png}}


Take $y = xY$ and $z = xZ$ we get:\[x(x^2Y) = x^2Z^2 \Rightarrow x^2(xY^2-Z^2) = 0\]
Which gives us the same singular point. So what can we do? We can try to blow it up along a line.


Take:\[\begin{array}{cccc}
    \mathbb{A}^2&\times& \mathbb{P}^2 &\\
    (x,y,z)&\times & (s\colon t)& yt = zs
\end{array}\]

So, choosing $s=1$ we get:\begin{align*}
    xy^2 = z^2 &\Rightarrow xy^2 = y^2t^2\\
    &\Rightarrow y^2 = 0\text{ or }x=t^2
\end{align*}


\subsection{More on blow ups}
What happens if we blow up a point on $\R^2$? Let's blow up the origin:

We take the set of all points \[\{((x,y), (s\colon t)) \mid xt=ys\}\subseteq \R^2\times \mathbb{P^1} \]

The point $(0,0)$ corresponds to $\mathbb{P}^1$ over the reals which is just the circle $S^1$. So we can think of this as having a point for each possible directions through the origin.

So we have a map \[\begin{array}{cc}
    E = \text{Blow up} &\rightarrow \mathbb{P}^1(\R) = S^1\\
    (x,y),(s\colon t)&\rightarrow s\colon t
\end{array}\]

The inverse image of $(s\colon t)$ is $\R$. 
So we have a map from $E$ to $S^1$ such that the fibres are $\R$. We may think $E$ corresponds to the cylindre but that is wrong, since the blowup is non-orientable. So $E$ is homeomorphic to an open Moebius band.

This is surprising, we started with an orientable surface and blowing it up at a point gave us a non-orientable surface.


\paragraph*{Next example}

Let's try to construct a map from $\mathbb{P}^1\times \mathbb{P}^1\rightarrow \mathbb{P}^2$. 

\[(x_0\colon x_1),(y_0\colon y_1)\rightarrow (x_0y_0\colon x_1y_0\colon x_0y_1) \text{ not defined at }x_0=y_0 = 0\]

So this is defined everywhere except at a point $(0\colon 1)\times (0\colon 1)$. Let's fix this with blow ups.

\

Blow up $\mathbb{P}^1\times \mathbb{P}^1$ at $(0\colon 1)\times (0\colon 1)$. 

Cover with a copy of $\mathbb{A}^2$ given by the points where $x_1 = 1$ and $y_1 = 1$ (so the affine plane is $(x_0,y_0)$). If we blow this up at $x_0 = y_0 = 0$ by putting:\begin{align*}
    &x_0 = ty_0\\
    &\Rightarrow (x_0y_0\colon x_1y_0\colon x_0y_2)\\
    &\Rightarrow (ty_0^2\colon x_1y_0\colon ty_0y_1)\\
    &\Rightarrow (ty_0\colon x_1\colon ty_1)\in \mathbb{P}
\end{align*}
We've started with a rational map, and we found that if we blow up on point we get a regular map. Is this an isomorphism? No! If we look at $(0\colon 0\colon 1)\in \mathbb{P}^2$ this is the image of the line in $\mathbb{P}^1\times \mathbb{P}^1$.

This is the image of the line where $y_0 = 0$.

Likewise the point $(0\colon 1\colon 0)$ is the image of the line $x_0 = 0$.

\

We can show that this map from the blowup to $\mathbb{P}^2$ is the same as the map if we blow up these two points of $\mathbb{P}^2$. So the blow up of $\mathbb{P}^1\times\mathbb{P}^1$ at one point is equal to the Blow up of $\mathbb{P}^2$ at two points.

\paragraph*{More generally:}
There is several other things we can do with blowups

\begin{center}
    \begin{tabular}{cc}
        Blow up & Geometric meaning\\
        \hline \\
        Point $p$ & $p\rightarrow \mathbb{P}$ (the tangent space of $p$)\\
        Subvariety $X$ & $p\in X\rightarrow \mathbb{P}$ (normal space at $p$)\\
        Ideal\\
        ``Quasicoherent sheaf of graded algebras'' & Projective space of graded algebra at each point
    \end{tabular}        
\end{center}
\begin{remark}
    What is a quasicoherent sheaf of graded algebra? If we have a graded algebra we can construct a projective variety from it. Suppose we are given a graded algebra for every point of the variety and we replaced every point by the corresponding projective space of that graded algebra, that would be a blow up.
    
    \

    A quasicoherent sheaf of graded algebra, is a way of saying that we have assigned a graded algebra to every point of the variety in a nice way.
\end{remark}

\begin{example}
    Simple example of quasicoherent sheaf of graded algebra, suppose our variety is just a point then our quasi... is just a graded algebra, say $k[x_0,ldots,x_n]$ and the blowup is $\mathbb{P}^n$.
\end{example}

In a rather famous papaer, Hironaka showed that we can resolve singularities by repreatedly blowing up. This proof only works in characteristic zero, one of the biggest open problems is to try to generalize it varieties of non-zero characteristic.



\paragraph*{Blowing up along ideal}\begin{example}
    First look at $\mathbb{A}^2$ with coordinate ring $k[x,y]$. Blowing up at a point corresponds to blowing up along an ideal in $k[x,y]$. So blow up at $(0,0)$ corresponds to blowup at $(x,y)$. 

Blowing up along a more general ideal is a generalisation of this. Suppose we have an ideal with generators $(G_1,\ldots,G_n)$ take:\[\begin{array}{c}
    \mathbb{A}^2\times \mathbb{P}^{n-1}\\
    (x,y, (G_1,\ldots,G_n)) \text{ this is defined if }x,y\not\in V
\end{array}\]

We look at the image of $\mathbb{A}^2\setminus V$ in $\mathbb{A}^2\times \mathbb{P}^{n-1}$ and take the closure. This is a sort of blowing up corresponding to the ideal.
\end{example}

\begin{example}
    $I$ is the ideal generated by $x^2,y^2$. We map $x,y\in \mathbb{A}^2$ to $x,y$ $(x^2\colon y^2)$ in $\mathbb{A}^2\times \mathbb{P}^1$,

    If call these variables $x,y,(s\colon t)$, we have ${x^2}{y^2} = x^2t = y^2s$; we can take $s=1$: \[x^2t = y^2\]
    This is just the Withney umbrella all over again, so we introduced singularities by blowing up! We need to be more careful when blowing up, certainly amoung more complicated ideals. Mysterious but powerful operation. 
\end{example}


\section{The Atiyah flop}
We start by looking at hypersurface $xy=zt$ in $\mathbb{A}^4 = \{(x,y,z,t)\}$. What does this variety look like? It acts like a cone over $\mathbb{P}^1\times\mathbb{P}^1$. This is because since this is a homogenous equation we can look at this over $(x\colon y\colon z\colon t)\in \mathbb{P}^3$.
This is a quadric which we say earlier is a product of two projective lines.

\

So in affine four space this is more or less a cone over $\mathbb{P}^1\times\mathbb{P}^1$. This has a singularity at $(0,0,0,0)$ which we resolve by blow up.

\[\begin{array}{cc}
    \mathbb{A}^4&\times \mathbb{P}^2\\
    (x,y,z,t)&(X\colon Y\colon Z\colon T)
\end{array}\]

Under the conditions $xy=zt$, $xY=yX, \ xZ = zX, \ldots$ and $XY = ZT$(since we take Zariski closure)

The point $(0,0,0,0)\rightarrow \mathbb{P}^1\times \mathbb{P}^1$. 

We can check the blowup is non-singular for exampe, along $X=1$: $Y=ZT$ with coordinate $(x,Y,Z,T)$ which is a non-singular variety. So we have resolved the singularity.


This exceptional variety can be mapped to $\mathbb{P}^1$ in two different ways (first or second coordinate), so instead of blowing up the origin to $\mathbb{P}^1\times \mathbb{P}^1$ we can blow up along $\mathbb{P}^1$.

So we take $xy = zt$, and blow up along the line $y=t=0$:

\[\begin{array}{cc}
    \mathbb{A}^4&\times \mathbb{P}^1\\
    (x,y,z,t)&(X\colon Z)
\end{array}\]
With equations $xy=zt$, and $xZ = zX$ and taking the closure we get extra equation $tZ=yX$.

If we take $X=1$, we get $y=tZ$, $z=xZ$ and $xy=zt$. Which is also non-singular. This time the point $(0,0,0,0)$ is blown up to $\mathbb{P}^1$.

(Likeise we can blow up along $x=z=0$),

Let's draw what we get;

\href{https://youtu.be/OkOfa8vDdiQ?t=589}{\includegraphics[scale = 0.25]{atiyah-flop.png}}

Four varieties are the same except at the coloured points. The Atiyah Flop is the birational map marked on the image. It is a strange map since it taking a copy of $\mathbb{P}^1$ and changing it to $\mathbb{P}^1$ embedded in a subtily different way.

The point of this example is to show that there is no minimal way of resolving singularities in general. We would want to have when given a singularity inside a variety, we want a ``minimal'' resolution of it, a nonsingular variety mapping to it such that any nonsingular variety can be factored through it.

We can't do this here, since there are two ways of resolving the singularity, but not way of resolving it so that they both map through, so no minimal way of resolving. So if we want a minimal model(a resolution of singularites that is as small as possible) we need to allow some mild singularities, the \textbf{terminal singularities}.  

\lecture{Non-singular varieties}
\section{Singular Points}

In this section we will define what a singular point actually is.

\begin{definition}
    Let $p = (0,0,\ldots,0)\in \mathbb{A}^n$, we define a hypersurface $V = \{f=0\}$, where $p$ is irred polynomial. Then the \textbf{Tangent space} at $p$ is the zeroes of linear part of $f$.
    
    \begin{example}
        If $f = x - 2y + 3x^2+9xy = 0$, then the tangent space is given by \[x-2y = 0\]
    \end{example}
\end{definition}


\begin{definition}
    Let $p\in V$, $p$ is called singular if the tangent space at $p$ has greater dimension then dimension of $V$.(The wrong dimension)
\end{definition}
\paragraph*{Example with plane curves}

Let us look at the plane curve given by $f(x,y) = 0$. Then
 \[(0,0) \text{ is singular } \iff f(0,0) = 0 \text{ and the }\textbf{linear terms vanish}\]
Indeed if the linear terms vanish the tangent is the $\mathbb{A}^2$ which is of wrong dimension.

\begin{example}
    \begin{center}
        $y^2 = x^3$
        \href{https://youtu.be/U6czoljbt4g?t=292}{\includegraphics[scale = 0.25]{singular-points-1.png}}
    \end{center}

    \begin{center}
        $y^2 = x^2+x^3$
        \href{https://youtu.be/U6czoljbt4g?t=292}{\includegraphics[scale = 0.25]{singular-points-2.png}}
    \end{center}
\end{example}

\paragraph*{n-dimensions}
This is similar if we have a surface defined by $f(x_1,\ldots,x_n)$ then:
\[(0,\ldots,0) \text{ is singular } \iff f(0,\ldots,0) = 0 \text{ and }\frac{\partial f}{\partial x_i}(0,\ldots,0) = 0 \ \forall i\]


\paragraph*{What about codim greater than 1?}
\begin{definition}
    Let $V\subseteq \mathbb{A}^n =$ zero set of $(f_1,f_2,\ldots)$.

    For simplicity we can assume $p = (0,\ldots,0)$ and all the $f_i$ vanish at $p$. The \textbf{Tangent space} is the common zeroes of all linear parts of $f_1,f_2,\ldots$ 

    \[n - \text{rank}\underbrace{\frac{\partial f_i}{\partial x_j}}_{\text{Jacobian}} = \text{ dimension of the tangent space at }p\]
    
    From this we see that the subset of singular points are closed, because a matrix having rank < constant is closed on entries. (Recall we saw this when we say determinal varieties, this condition is the condition that whole lot of minors of some size vanish).

    A tangent space having $>$ than usual is closed condition.
\end{definition}

\begin{example}
    Take the curve given by \[x^3+y^3 = 1\]
    We find the singular points, we take the derivatives:\[\begin{array}{c}
        3x^2 = 0\\
        3y^2 = 0
    \end{array}\]
    This has no singular points over $\C$, since these equations vanish iff $(x,y) = (0,0)$ but this point is not on our curve.

    \textbf{HOWEVER} over a field of characteristic $3$, all points are singular!

    What went wrong?
    Note that over a field of characteristic $3$ this is not irreducible, $x^3+y^3 - 1 = {(x+y - 1)}^3$.

\end{example}

\begin{theorem}
    For varieties, nonsingular points are \textit{dense}.
    \begin{proof}
       We just need to show that the set of nonsingular points are nonempty.
    \begin{enumerate}[label = (\roman*)]
        \item Reduce to the case of hypersurfaces. We claim that $V$ is \textit{birational} to a hypersurface.
        
        By showing this we use field theory: let us look at the function field:
        \[K = k[\underbrace{x_1,\ldots,x_n}_{\text{alg. indep}},\ldots]\]
    We can choose $x_1,\ldots,x_n$ so that $K$ is seperable over $k(x_1,\ldots,x_n)$ so generated by one element. So $K = k(x_1,\ldots,x_n,y)$ and there is some $f(x_1,\ldots,x_n,y) = 0$. Our variety is birational to the hypersurface given by this $f$.


    We can change notation to write our hypersurface as $f(x_1,\ldots,x_n) = 0$ and assume that $f$ is irreducible and $k$ is algebraically closed.

    All points are singular implies that \[\frac{\partial f}{\partial x_i} = 0 \text{ at ALL points of }V\]
    But $\deg \frac{\partial f}{\partial x_i}<\deg f$, and since $f$ is irreducible $f\mid \frac{\partial f}{\partial x_i}$. But since $f$ divides something of smaller degre we conclude \[\frac{\partial f}{\partial x_i} = 0 \text{ for all }i\]

    This implies $f = 0$ in characteristic zero. In char $p>0$ this implies that $f = \sum \text{pth powers of monomials} = \underbrace{g^p}_{\text{in alg closed}}$
But since $f$ is irreducible it can't be the pth power of another polynomial, so $f=0$.

This contradicts the fact that we have a surface.

Therefore every variety contains a nonsingular point, so the nonsingular points are dense.
\end{enumerate}
    \end{proof}
\end{theorem}
\begin{remark}
    At nonsingular points over $\R$ or $\C$, $V$ looks locally like a smooth manifold(this comes from diff geo). Is the converse true? If $V$ looks locally like a manifold is it nonsingular?

    \paragraph*{We need to be careful}\begin{example}
        Over $\R$, if we take the manifold $y^2=x^3$, it is a topological manifold but has a singularity.
    \end{example}
\end{remark}
There is a problem with our definition, they seem to depend about how our varieties are embed into affine space. It is not at all obvious that the tangent space at a point is the same if we embed the same variety into affine space in a different way.  
\subsection{Zariski tangent space}
\begin{definition}
    The \textbf{cotangent space} at a point $p$, is $\mathfrak{m}/\mathfrak{m}^2$, where $\mathfrak{m}$ is the max ideal of local ring at $p$; recall the local ring at $p$ is ring of functions $f/g$, such that $g\neq 0$ at $p$.

    The maximal ideal, $m$ can be thought as the functions vanishing at $p$.
\end{definition}

\begin{definition}
    The \textbf{tangent space} at $p$, is the dual of the cotangent space at $p$.
\end{definition}

The definition of cotangent space only depends on the local ring and not the embedding of $V$ into affine space. So this definition of the tangent space doesn't depend on the embedding.

\paragraph*{Is it equivalent to the earlier definition?}
If we take $p = (0,\ldots, 0)$, and $V\in \mathbb{A}^n$ with coordinates $(x_1,\ldots,x_n)$. The maximal ideal of the local ring at $p$ is generated by $x_1,\ldots,x_n$.

So $V$ is the set of zeroes of $(f_1,f_2.\ldots)$.

\

\begin{align*}
    \mathfrak{m}/\mathfrak{m}^2 &= (x_1,\ldots,x_n)/(f_1,\ldots,f_n)\text{ and all monomials of degree }\geq 2\\
    &= \text{vector space spanned by }(x_1,\ldots,x_n)/\text{linear parts of }(f_1,\ldots,f_n)
\end{align*}
This is just the dual of the tangent space defined earlier.
\begin{definition}
    The space $\mathfrak{m}/\mathfrak{m}^2$ is called the \textbf{Zariski tangent space} for any local ring.    
\end{definition}

\paragraph*{Another way of looking at cotangent space}
Take a local ring $R$ and look at $\mathfrak{m}/\mathfrak{m}^2$ which is a vector space over the field $R/\mathfrak{m}$.

The dual $\mathfrak{m}/\mathfrak{m}^2 = $ ring homomorphisms $R\rightarrow k[\epsilon]/(\epsilon^2)$, where $k[\epsilon]/(\epsilon^2)$ is a 2 dimensional vector space with basis elements $1$ and $\epsilon$ such that $\epsilon^2 = 0$.

\

This ring homomorphism takes:\[\mathfrak{m}^2\rightarrow 0 \text{ and }\mathfrak{m} \rightarrow \text{multiple of }\epsilon \]

\paragraph*{What does the above ring look like?}
Note that $k[\epsilon]/(\epsilon^2)$, is not an algebraic set since the ideal contains nilpotents. This actually corresponds to a scheme, Spec$k[\epsilon]/\epsilon^2$. This looks like a point with a tangent vector attached to it, and maps from the spectrum of this to the variety correspond to picking a point in the variety and a tangent vector at that point.

\paragraph*{Vector fields of smooth manifold V over the reals}
Let us look at the ring:\[C(V) = \text{smooth functions}\]
So the space of vector fields is a module over $C(V)$.

\

We can define:\[W = \text{cotangent of vec. fields} = \text{dual of the module of vector field}\]

Recall from diff geo that there is a map $d\colon C(V)\rightarrow W$, which takes $f$ to the $1$-form $df$; such that $d$ satisfies the Liebnitz rule: \[d(fg) = fdg + (df)g\]

\textbf{More generally} in algebraic geometry instead of $C(V)$ we might look at the ring; $R = k[x_1, \ldots,x_n]/I$.

So we want to find a map\[d\colon R\rightarrow W\]
Where $W$ is a module over $R$, ``the module of cotangent vectors''.

\

\paragraph*{How do we construct W?}

$W$ is \textbf{generated} (as a module) over $R$ by elements of the form $dr$ with $r\in R$, satisfying the relations:\begin{align*}
    dr &= 0 \text{ for }r\in k
    d(fg) &= (df)g+f(dg)
\end{align*}
The we define $W$ to be the module with these generators and relations.

\begin{example}
    If $R = k[x_1,\ldots,x_n]$. Then $W$ is the free module on elements $dx_1,\ldots,dx_n$.
    We can also define the module of \textit{tangent vector fields}: $Hom_R(W,R)$.
\end{example}

In general the module of cotangent vector fields is a bit complicated. It is not usually a free module over the ring, so it is useful to have an alternative description of the module of cotangent space, we define $W$ like this:\begin{definition}
    Suppose we take the homomorphism: \[\underset{a\otimes b}{R\otimes R}\rightarrow \underset{ab}{R}\]
We define $I$ to be the kernel of this map, so it is an ideal of $R\otimes R$, and we can take $W = I/I^2$.
$W$ is an $R$-module: $R$ acts by multiplication on the left $R$ in $R\otimes R$. 

\

We define \[d\colon R\rightarrow W \text{ such that }dr = 1\otimes r - r\otimes 1\]

\

\textbf{Problem:} Show that this follows the Liebnitz rule.

\textbf{Problem:} Check the universal property. What we mean by this, we want to show: suppose we have any module $M$ with a map from $R$ to $M$ (satisfying the Liebnitz rule). We want to show there is a unique map from $W$ to $M$, making this diagram commute:
\[\begin{tikzcd}
    R \arrow{r}{d} \arrow{dr}{d} & M \\%
    & W \arrow{u}
    \end{tikzcd}
    \]

We can do this as follows, we have $I\subseteq R\otimes R\rightarrow R$, we can define a map $f\colon R\otimes R\rightarrow M$ by $f(a\otimes b) = adb$.
This gives a map $I\rightarrow M$, we need to check that this map $f$ is $0$ on $I^2$. Pick some element $\sum s_I\otimes t_i\in I$ and $\sum u_j\otimes v_j\in I$.

This means that \begin{align*}
    \sum s_i t_i &= 0\\
    \sum u_j v_j &= 0
\end{align*}
We claim that \begin{align*}
    f(\underbrace{(\sum s_i\otimes t_i)(\sum u_j\otimes v_j)}_{\in I^2}) &= \sum s_i u_j d (t_i v_j)\\
    &=\sum s_i dt_i \sum u_j v_j + \sum s_i t_i \sum u_j dv_j\\
    &= 0 
\end{align*}

From this we can check that $W$ is the universal module such that there is a map from $R$ to $W$ satisfying Liebnitz's rule.
\end{definition}
We say the local ring is \textbf{regular} if the dimension of the Zariski tangent space is equal to the dimension of the local ring.

We say a variety is nonsingular at a point, if it's local ring is regular.

\section{Du Val singularites}
These are also called, simple surface singularites, Kleinian singularites, rational double points or canonical singularites in dimension $2$.


\

We will introduce them by Klein's method.
\paragraph*{Klein's method}
    Take a finite group $G$ acting on $\C^2$, and take $\C^2/G$. For reasonable groups $G$ this gives us a variety with a singular point at the origin.

    \

    Recall the coordinates ring of $\C^2/G$ is given by ${k[x,y]}^G$.


\begin{example}
    Suppose we have a cyclic group of order $n$ generated by $\sigma$ such that \[\sigma x = \zeta x \text{ and }\sigma y = \zeta^{-1} x\text{ where }\zeta^n = 1\]
    Then the invariants of this action are given by \[X = x^n, \ Y = y^n, \ Z = xy\]

    So the coordinate ring is given by \[k[X,Y,Z]/(Z^n = XY)\]
    We can also write this as $Z^n = \frac{{(X+Y)}^2}{4}-\frac{{(X-Y)}^2}{4}$
\end{example}
\begin{definition}
    The \textbf{Du Val singularities} are as follows:\begin{itemize}
        \item Cyclic $A_n$: given by $x^2+y^2+z^{n+1} = 0$
        \item Dihedral $D_n$: given by $x^2+zy^2+z^{n-1} = 0$
        \item Tetrahedral $E_6$: given by $x^2+y^3+z^4 = 0$
        \item Octahedral $E_7$: given by $x^2+y^2+yz^3 = 0$
        \item Icosahedral $E_8$: given by $x^2+y^3+z^5 = 0$
    \end{itemize}
    Note for the last case, it is not given by the Icosahedral group of order $60$ acting on $\C^2$ but by the binary Icosahedral group of order $120$.
\end{definition}
For  all of these we are going to work in characteristic zero, and for all of these the only singular point is $(0,0,0)$.

\paragraph*{Resolving the Icosahedral singularity with Blow ups}
So we have $x^2+y^3+z^5 = 0$,    
Let $(x_1\colon y_1\colon z_1)$ be the coordinates for $\mathbb{P}^2$, so we look at the subseteq of $\mathbb{A}^3\times \mathbb{P}^2$ satisfying the relationships:\begin{align*}
        xy_1 &= yx_1\\
        xz_1 &= zx_1\\
        &\vdots
    \end{align*}
    So $\mathbb{P}^2$ is covered by three copies of $\mathbb{A}^2$:\begin{itemize}
        \item $x_1 = 1$, we have $y = y_1x$ and $z=z_1x$; so we have:\begin{align*}
            x^2 + y_1^3 x^3 + {z_1}^5 x^5 &= 0\\
            1 + y_1^3 x + {z_1}^5 x^3 &= 0
        \end{align*}
        Which we can check has no singularites.
        \item $y_1 = 1$, we have\begin{align*}
            x_1^2y^2+y^3+z_1^5y^5&=0\\
            x_1^2+y+z_1^5y^3 &=0 \text{ no singularities}
        \end{align*} 
        \item $z_1 = 1$, we have $x=x_1z$, $y=y_1z$\begin{align*}
                x_1^2z^2+y_1^3z^3+z^5 &= 0 \\
                x_1^2+y_1^3z+z^3 &= 0
        \end{align*}
        Taking the partial derivatives:\begin{align*}
            2x_1 &=0\\
            3y_1^2z &=0\\
            3z^2 &=0
        \end{align*}
        So we have a singular point at $x_1=y_1=z=0$
    \end{itemize}

    So now we resolve the singularity at $x_1^2+y_1^3z+z^3 = 0$; again we take $\mathbb{A}^3\times \mathbb{P}^2$ where $\mathbb{P}^2$ has coordinates $(x_2\colon y_2\colon z_2)$.
    Once again we cover $\mathbb{P}^2$ with three copies of $\mathbb{A}^2$:\begin{itemize}
        \item Like before we can check that $x_2=1$ and $z_2=1$ yield no singularities.
        \item When $z_2 = 2$: we have $z=z_2y_1$ and $x_2 = x_1y_1$\begin{align*}
            x_2^2 y_1^2+y_1^4z_2+y_1^3 z_2^3 &=0\\
            x_2^2+y_1^2 z_2+ y_1z_2^3&=0
        \end{align*} 
        This has a singularity at $(0,0,0)$.
    \end{itemize}
    We don't seem to have made any progress, but we actually have the new singularity is simpler than the previous one.

    \

    Again we blow up this singularity, with new coords $(x_3\colon y_3\colon z_3)$:\begin{itemize}
        \item $x_3=1$: No singularities
        \item $y_3=1$: \[x_3^2y_1^2+y_1^3z_3+y_1^4z_3^3 = 0\] The only singularity is at $x_3=y_1=z_3=0$.
        \item $z_3=1$: \[x_3^2+y_3^2z_2+y_3z_2^2 = 0\] so a singularity at $x_3=y_2=z_2=0$
    \end{itemize}
We now have two singularites to look at:\begin{enumerate}[label = (\arabic*)]
    \item \[x_3^2+y_1z_3+y_1^2z_3^3\] We blow up introducing $(x_4\colon y_4\colon z_4)$; and at $x_4 =1$, $y_4=1$ and $z_4=1$ we have no singularites.
    \item \[x_3^2+y_3^2z_2+y_3z_2^2 = 0\]We blow up introducing $(x_5\colon y_5\colon z_5)$. At $x_5=1$ we get no singularites, but at $y_5=1$ we get 2 singularities and at $z_5 = 1$ we get 2 singularities. But one of the singularites of $y_5=1$ is the same as a singularity in $z_5=1$; so we have three singularites to resolve.
\end{enumerate}

If we take each of these three singularites and blow them up once we get something non-singular. So after eight blowups we end up with something nonsingular, each blowup introduces a exceptional curve that looks like $\mathbb{P}^1$ and when they intersect they look like this:

\href{https://youtu.be/s2LYtd_UPY8?t=1102}{\includegraphics[scale=0.25]{E8-diagram.png}}

If we draw a point for each lines and connect the points if the corresp lines intersect, we get the $E_8$ Dynkin diagram as seen above. 

We see that a problem with Blow up involves a lot of book keeping and each don't do very much, we only make a tiny improvement to our singularity. Furthermore unlike in this example, if we blow up a point we can find the singularities form a subset of dimension greater than $1$.

\section{Resolution of singularites}
\begin{example}
    The first of resolution of singularites we are going to look at is the equation:
    \[x^4+y^4=z^2\]
    \begin{remark}
        This is a historically famous equation since it was used by Fermat to show $x^4+y^4=z^4$ has no integer solutions.    
    \end{remark}
    We find singularites by differentiating:\begin{itemize}
        \item $4x^3 = 0$
        \item $4y^3=0$
        \item $2z=0$
    \end{itemize}
    So the only singular point is at the origin. So let us resolve it by blowing up at $(0,0,0)$ introducing new variables $(X\colon Y\colon Z)$.\begin{itemize}
        \item $Z=1$: $x=zX$ and $y=zY$. So\begin{align*}
            z^4X^4+z^4Y^4&=z^2\\
            z^2X^4+z^2Y^4&=1
        \end{align*} We can check that this is nonsingular.
        \item $Y=1$ $x=yX$ and $z=yZ$.  So\begin{align*}
            y^4X^4+y^4&=y^2Z^2\\
            y^2X^4+y^2&=Z^2
        \end{align*}
        By differentiating with respect $X,y,Z$ we get:\begin{itemize}
            \item $y^2 4X^3 = 0$
            \item $2y(X^4+1) = 0$
            \item $2Z=0$
        \end{itemize} So we get singular points when $y=Z=0$.
        \item $X=1$: Similarly to $Y=1$, we get a line of singular points.
    \end{itemize}
    This is different from the du Val singularities from last time, we get a whole line except of points. We might think this made things worst since the dimension of singularity got bigger, but that is not true.
    The nastiness comes from the codimension, so turning a point to a line is actually good.
    
    So let us finish resolving it, we get the equation\[y^2X^4 +y^2 = Z^2\] And we blow up along the line $y=Z=0$. Let us introduce two new variables from a copy of $\mathbb{P}^1$, $(s\colon t)$, such that $sZ=tY$.
    Covering the projective line by two copies of affine line:\begin{itemize}
        \item $s=1$: We find $z=ty$, and our equation becomes:\begin{align*}
            y^2X^4+y^2 &= t^2y^2\\
            x^4+1&=t^2
        \end{align*} Which is nonsingular.
        \item $t=1$: We again find we get a nonsingular line.
    \end{itemize}        
\end{example}
\paragraph*{When things go wrong}
This example illustrates that if we choose our blow up badly we can make the singularity worst.
\begin{example}
    Let us look at $x^2-yz = 0$, this is a cone with a singularity at the origin. Suppose instead of blowing up at the origin as we did before, we blow up at the line $y=z=0$.
    
    So we introduce two new coordinates $(Y\colon Z)\in \mathbb{P}^1$, with $yZ=zY$.\begin{itemize}
        \item $Y=1$: $z=yZ$ our equation becomes $x^2-y^2Z=0$ the Withney umbrella. We started with a simple singularity and we have a more complicated singularity.
        \item $Z=1$
    \end{itemize}
    We need to be careful of where we blow things up, it can make things worst. 
\end{example}

\paragraph*{Example from number theory}
\begin{example}
    We can look at $\Z[\sqrt{-3}]$, this ring has a problem since it doesn't have unique factorization:\[2\cdot 2 = (1+\sqrt{-3})(1-\sqrt{-3})\]
    We pretend that $\Z[\sqrt{-3}]$ is a coordinate ring of a variety (in actuality it is the coordinate ring of a scheme). What are the points of this variety? They correspond to maximal ideals for example\begin{equation*}
        (2,\sqrt[]{-3}- 1)
    \end{equation*}
    We shall show that in some since, the variety (scheme) has a singular point at this point.

    The local near this point is $R = Z_{(2)}[\sqrt{-3}]$, where $Z_{(2)}$ is the ring we get by inverting all primes not equal to $2$; i.e. $Z_{(2)} = \{\frac{a}{b}\in \Q \mid \gcd(a,b) = 1\text{ and }b=1\pmod 2 \}$. 
    
    The maximal ideal of this local ring:\[(2,\sqrt{-3}-1) = \mathfrak{m}\]
    
    We have:\[R/\mathfrak{m} = \mathbb{F}_2\] Indeed take the map $\varphi(\frac{a}{b}+\frac{c}{d}\sqrt{-3}) = a+c$. Since $\varphi(\frac{a}{b}+\frac{x}{y}) = \varphi(\frac{ay+bx}{xy}) = ay+bx=a+x$, since $y=b=1\pmod 2$. And $\varphi(\frac{a}{b}\frac{x}{y}) = ax$, we see that this map is a homomorphism with kernel $\mathfrak{m}$,

    Likewise we can easily check that $R/\mathfrak{m}^2$ has order $8$. So \[\text{ the cotangent space at the point corresponding to the max ideal }\mathfrak{m}/\mathfrak{m}^2 \text{ has order }4\]

    And is a two dimensional vector space over $R/\mathfrak{m}$. We find that the Krull dimension of $R$ has dimension $1$. So the cotangent space at $\mathfrak{m}$ has larger dimension than $R$, so this corresponds to a singular point (in some sense as it not exactly a variety).

    The fact that $\Z[\sqrt{-3}]$ has a singular point actually implies that it is not a UFD.

    \paragraph*{Resolving the singularity}
    We can solve this by blowing up, but we won't do it. In number theory we resolve singularities by taking a normalization or integral closure of a ring. Taking the integral closure of \[\Z[\sqrt{-3}]\subseteq \Z[\underbrace{\frac{\sqrt{-3}+1}{2}}_\omega]\]
    So $\omega^2+\omega+1=0$, so $\omega$ is an algebraic integer in the field of our first ring.

    \

    The new ring does have unique factorization and it's points are nonsingular. Taking the integral closure of our ring is sort of the same as taking the desingularisation of our variety.
\end{example}
\paragraph*{Application of resolution of singularities}
Recall the Gamma function:\[\Gamma(s) = \int_0^\infty e^{-t}t^{s-1}, \text{ with }Re(s)>0\]
We can extend this to a meromorphic function of $s$ by integrating by parts.

\

Now suppose we have:\begin{align*}
    \int |f(x_1,\ldots,x_n)|^s\varphi(x_1,\ldots,x_n)dx_1\ldots dx_n
\end{align*} 
Where $f$ is a polynomial and $\varphi$ is smooth with compact support. Note that $|f(x_1,\ldots,x_n)|^s$ is a holomorphic function of $s$, if the real part of $s$ is large.

Can we analytically continue to all $s$? We see that this is more or less the same as the Gamma function, $f$ corresponds to $t$ and $\varphi$ corresponds to $e^{-t}$. (Realise that $e^{-t}$, doesn't have compact support but Borcherds doesn't really care about that).

\textbf{Problem:} Occurs at the zeroes of $f$. The zeroes of $f$ is some sort of hypersurface with singularites. The more complicated the singularites are the more difficult it is to figure out what is going with this integral.

By Atiyah, there is a very easy way to settle this question. We only need to resolve singularites of the variety $f=0$. What we end up with is not actually a nonsingular space, since when we blow up singularities according to Hironaka we introduce exceptional curves. The exceptional curves don't really matter because they all cross transversally. 
So we end up with things that look like $x_1x_2\cdots x_k =0$, a lot of hyperplanes interseceting with each other.

We are reduced to the case\begin{align*}
    \int |x_1\cdots x_k|^s\varphi(x_1,\ldots,x_n)dx_1\ldots dx_n = 0
\end{align*}  
This splits as a lot of integrals of the form $\int |x_1|^s f(x_1)dx_1=0$, and we can do this in the same as the gamma function by intergrating by parts.

So resolving singularites allow us to prove the existance of meromorphic continuations of integrals as above.

Bernstein found another to prove this, with polynomials he introduced, which are called \textit{Bernstein-Sato polynomials}.

\section{Completions of a ring}
\begin{example}
    Suppose we take the ring of polynomials \[k[x] = \sum_{i=0}^n a_i x^i\]
    The completion is the ring of formal power series \[k[[x]] = \sum_{i=0}^\infty a_i x^i\]

    How do we construct the formal power series ring from the polynomial ring?

    If we let $k[x] = R$, we take the ideal $I = (x)$ and we have:\begin{itemize}
        \item \[R/I = k = \{a_0\}\]
        \item \[R/I^2 = \{a_0+a_1x\}\]
        \item \[R/I^3 = \{a_0+a_1x+a_2x^2\}\]
        \item \[\vdots\]
    \end{itemize}
    An element of the completion, means we pick elements of one of these rings, and they have to be compatible, so if we pick an element in $R/I^2$ is must have an image in $R/I$.

    So we have a sequence of rings:
    \begin{center}
        \begin{tikzcd}
            \arrow{r} & R/I^3 \arrow{r} & R/I^2 \arrow{r} & R/I\\
            && \hat{R} \arrow{u} \arrow{lu} \arrow{llu} \arrow{ru}&
        \end{tikzcd}        
    \end{center}
    
    And we pick an element in each of these rings that are all compatible, i.e. commute with these automorphisms; that is an element of the power series ring, which we call the \text{completion} $\hat{R}$. 
\end{example}

This works for any ring $R$ and any ideal $I$.

\paragraph*{Example from number theory}
\begin{example}
    If we take $R=\Z$ and $I=(p)$ for a prime $p$; then $\hat{R} = \Z_p$, the $p$-adic integers.    
\end{example}

\begin{definition}
    What we usually do in algebraic geometry, we take $R = $ a local ring, and $\mathfrak{m} = $ max ideal. And we look at the completion with respect to this local ring:
    \[\hat{R} \text{ given by }R/\mathfrak{m}^3\rightarrow R/\mathfrak{m}^2 \rightarrow R/\mathfrak{m}\]
\end{definition}
Is the map \[R\rightarrow \hat{R}\] injective?
Yes if $R$ is Noetherian, but no in general.
\begin{example}
    Let $R = $ smooth functions on $\R$ and $I = (x) = $ functions vanishing at zero.

    \

    Then $R/I^n$ are power series where we ignore all terms greater than $n$, so $\hat{R}$ is just power series.

    However the map \[R\rightarrow \hat{R}\]
    Is not injectivem since for example we can have smooth function that vanish to all orders at $0$, for example the function \[f(x) = \begin{cases}
        e^{1/x^2} \text{ for }x\neq 0\\
        0 \text{ for }x=0
    \end{cases}\]
    This function is in the kernel of the map from $R$ to its completion.
\end{example}

\paragraph*{More algebraic example}
\begin{example}
    Take \[R = k[[x,x^{1/2},x^{1/4},\ldots]]\]
By this we mean take the union of the power series:\[k[[x]]\rightarrow k[[x^{1/2}]] \rightarrow k[[x^{1/4}]]\]

Take the ideal: \[I = (x,x^{1/2},x^{1/4},\dots)\]
Note that $I = I^2$, so \[\hat{R} = k\]
The map from $R$ to the completion in this case fails badly to be injective.
\end{example}
\subsection{Hensel's lemma}
This lemma is not just one lemma but many, since there are a lot of variations of this lemma.
\begin{lemma}
    Solutions in $R/\mathfrak{m}$ can be lifted to solutions in $\hat{R}$ under some conditions.
\end{lemma}
\paragraph*{Typical example}
Suppose $f(z) = g_0(z)h_0(z)\pmod \mathfrak{m}$, where $g_0,h_0\in k[z]$ and $f\in R[z]$. Then this lifts to \[f(z) = g(z)h(z)\text{ where }g,h\in R[z]\]
If $g_0,h_0$ are coprime in $k[z]$.

\paragraph*{Sketch of idea of proof}\begin{proof}
    Suppose $f=gh\pmod \mathfrak{m}^n$, try to find $a,b$ such that:\begin{align*}
        f = (g+a)(h+b) \pmod \mathfrak{m}^{n+1}
    \end{align*}
    Here $a,b\in \mathfrak{m}^{n+1}/\mathfrak{m}^{n+2}$.

    If we do this we need to solve:\[g_0 b + h_0 a = \text{(complicated)}\]
    The idea is that we can solve this equation, since $g_0$ and $h_0$ are coprime. Since two coprime elements generate the unit ideal.
\end{proof}

The advantage of complete local ring over local rings, is that it is very easy to solve equations and find roots of equations since we have multiple versions of Hensel's lemma. 

\begin{example}
    Let $y^2=x^3+x^2$,  which looks like:
\begin{center}
    \includegraphics[scale = 0.5]{singular-points-2.png}    
\end{center}
Note if we look really close the origin, it sort of looks like two lines crossing: \[\times\]
Which is the graph of $x^2 = y^2$.

We can't really see this with the local ring, since if it was the union of two components it would have zero divisors, but it is the subring of a field. 
The local ring isn't aware that near the origin, it splits into two components.

\

Let $R = \text{local ring at }(0,0) = $ i.e. the coordinate ring of $R$ and inverting eveything that doesn't vanish at $(0,0)$

The completion of the local ring is:\[\hat{R} = k[[x,y]]/(y^2-x^3-x^2)\]
The point is that $y^2 - x^2-x^3 = (y-x)(y+x) \pmod I^2$, so by taking Hensel's lemma or square roots directly we see that:\[y^2-x^2-x^3 = (y-x+\text{ power series})(y+x+\text{ power series})\] 


So over the completion of the local ring we do see that there are two components, each factor corresponds to a component of the $\times$ graph.

\

Furthermore, whilst the local ring $R$ has no zero divisors, but the completion has zero divisors. So completions may cause problems, c.f. Henselazation 

\end{example}

\begin{definition}
    \textbf{Analytically isomorphic} means that the completions of the local ring are isomorphic with eachother.
\end{definition}


\paragraph*{Series of rings}
\[R\rightarrow \hat{R} \rightarrow \ldots \rightarrow R/\mathfrak{m}^3\rightarrow R/\mathfrak{m}^2\rightarrow R/\mathfrak{m} = k\]
We can think of these rings as corresponding to finer and finer nbhs of a point. 
\begin{itemize}
    \item $R/\mathfrak{m}$, is the coordinate ring of a point
    \item $R/\mathfrak{m}^2$, is the coordinate ring of a point, with a ``tiny bit sticking out'', the first order neighbourhood of a point.
    \item $R/\mathfrak{m}^3$, looks like a point with ``two more points in the same place''
    \item \dots
    \item $\hat{R}$, looks like a point with a short smooth curve sticking out of It
    \item $R$, the local ring looks like the line with a finite number of points missing.
\end{itemize}
\section{Elimination theory}
\begin{remark}
    c.f. Theorem $5.7$ from Hartshorne.    
\end{remark}

Elimination theory has caused a fair amount of controversy, André Weil in his book ``Foundations of algebraic geometry'' had a rather scathing critique of it. It was also talked about in Abhyankar's poem ``Polynomials and Power series, may they forever rule the world!''

\paragraph*{But what is elimination theory?}
\begin{example}
    Suppose we have two polynomials \[x^3 y^4-7x^2-xy^8\]
    and \[3x^2 y^5 + 4y^2 + x^4 y^7\]
    We eliminate $y$, by this we mean we find an expression for $y$ with the first polynomial, and we substitiute it in the second polynomial.

    \

    This is hard, since by Bézout's theorem there should be $9\cdot 11 = 99$ points of intersection, so $x$ should satisfy a polynomial of degree $99$. How do we write out this polynomial?
\end{example}

\paragraph*{General problem}
Suppose we have:\begin{align*}
    f(x) &= \sum_{i=0}^m a_i x^i\\
    g(x) &= \sum_{i=0}^n b_i x^i
\end{align*}
What is the condition for a common root? For now we assume $a_m\neq 0$ and $b_n\neq 0$. If these have a common root then:\[f(x)p(x) = g(x)q(x)\]
For some $p,q\neq 0$ with $\deg p<n$ and $\deg q <m$. Indeed if they have a common root just put \[p = \frac{g}{x-\alpha}, \text{ and }q = \frac{f}{x-\alpha}\] Where $\alpha$ is the common root.

So the condition for a common root is to find such $p,q$ such that $fp = gq$. If we expand it out we get a whole bunch of homogenous linear equations whose coeffs depends on the $a_i,b_j$. If this set of homogenous equations has a non-trivial solution then, some big determinant vanishes.

\

The matrix looks like:
\href{https://youtu.be/nM8aU0IRHGM?t=485}{\includegraphics[scale = 0.25]{resultant.png}}

\begin{definition}
    This determinant is called the \textbf{resultant}.    
\end{definition}

\paragraph*{Problem: What to do when $a_m = b_n = 0$?}
The condition that $a_m = 0$ can be thought as the polynomial having a root at infinity. If we take the corresp homogenous poly:\[a_m x^m y^0 + \cdots + a_0 x^0 y^m\]
Then we can think of a root being a point in $(x\colon y) \in \mathbb{P}^1 = k\cup \infty$, where $k$ is equiv $(x\colon 1)$ and $\infty$ corresponds to $(1\colon 0)$.

So the resultant is zero, is equivalent to the two homogenous polynomials:\[    
    f(x) = \sum_{i=0}^m a_i x^i y^{m-i}\\
    g(x) = \sum_{i=0}^n b_i x^i y^{n-i}\]
have a common zero in $\mathbb{P}^1 = k\cup \infty$.


\paragraph*{Example}
When does the polynomial $f(x) = ax^2+bx+c$ have a double root. Well note it has a double root if $f$ and $f'(x) = 2ax+b$ have a common zero.

The resultant:\[\begin{vmatrix*}
    a & b & c\\
    2a & b & 0\\
    0 & 2a & b
\end{vmatrix*} = ab^2 -b(2ab)+c(4a) = -a\underbrace{(b^2-4ac)}_{\text{discriminant}} \]
So if $a$ is non-zero, then $f$ has a double root if and only if the discriminant is zero. (If $a=0$, recall $f$ and $f'$ have a root at infinity).

\paragraph*{More complicated example}
Suppose $f(x) = x^3+bx+c$, when does this have a double root? Well $f'(x) = 3x^2+b$, we look at the Sylvester matrix:\[
\begin{vmatrix*}
    1 & 0 & b & c & 0\\
    0 & 1 & 0 & b & c\\
    3 & 0 & b & 0 & 0\\
    0 & 3 & 0 & b & 0\\
    0 & 0 & 3 & 0 & b
\end{vmatrix*} = 4b^3+27c^2
\]

This is not quite the discriminant of a cubic, there is a sign problem the discriminant is $-4b^3-27c^2$. In all cases the polynomial has a double root if this discriminant is zero.

\lecture{Nonsingular curves}

\section{Informal review of classification of algebraic curves}
We are going to look at algebraic curves over $\C$ to simplify this. There are three ways of viewing algebraic curves over $\C$:\begin{itemize}
    \item Algebraic curves, this is birational to a complete curve since we can embed it in projective space and take it's closure. Furthermore, we will see later that it is birational to a \textbf{nonsingular} projective curve.
    \item nonsingular projective curves are more or less the same as a compact Riemann surface.
    \item nonsingular projective curves also correspond to f.g.\ fields over $\C$ of transcendence degree equal to $1$.
\end{itemize}

So there are three ways of looking at the same object. An analyst talks about compact Riemann surface, a algebraist talks about f.g.\ fields over $\C$ of transcendence degree equal to $1$ and a geometer talks about nonsingular projective curves.

\

\paragraph*{Genus}
\begin{definition}
    The \textbf{genus} is the basic invariant of a curve.


    It is easy to talk about the genus of compact Riemann surface (look at the holes), defining the genus for algebraic curves can be done but involves a bit more work. 

    For each genus there is a family of curves of that genus, and something called a \textbf{moduli space} whose points correspond to isomorphism classes of curves of that genus.    
\end{definition}

It is a fundamental problem to try to understand this moduli space, which is more or less equivalent to classifying all curves of a given genus. 

\begin{itemize}
    \item Genus $0$: Is easy, the only nonsing curve of genus $0$ is $\mathbb{P}^1$,
    \item Genus $1$: Elliptic curves, as we saw these are of the form $\C/L$ where $L = $ is a lattice $m+n\tau$.
\end{itemize}

\

\paragraph*{When are two elliptic curves the same?}
If we have any three points on the projective line there is a linear fractional transformation:\[\tau \rightarrow \frac{a\tau + b}{c\tau + d} \]
Taking any points $\lambda_1,\lambda_2,\lambda_3$ to $0,1,\infty$ respectively and $\lambda_4$ to some point $\lambda$.

So any elliptic curve over $\C$ can be defined by giving a point $\lambda$ and we are looking at the equation:\[y^2 = x(x-1)(x-\lambda)\]
However two values of $\lambda$ can sometimes give the same algebraic curve. For example, there is a group of order $6$ permuting, $0,1,\infty$ which takes \[\lambda \rightarrow \begin{cases}
    \lambda\\
    1-\lambda\\
    \frac{1}{\lambda}\\
    \frac{1}{1-\lambda}\\
    \frac{\lambda}{\lambda-1}\\
    1-\frac{1}{\lambda}
\end{cases}\]

This gives a group of order $6$ isomorphic to $S_3$, and this acts on $\lambda$. Any two $\lambda$ that are equivalent on this group of order $6$ give the same elliptic curve since they just permute the $\lambda_i$.

So it turns out that \[\text{Moduli space} = \text{affine line param by }\lambda/S_3\]

In order to take the quotient of the affine line by this group of order $6$, we want to find the invariants of this action.

\begin{definition}
    \textbf{j-invariant} \[j = \frac{256(\lambda^2-\lambda+1)}{\lambda^2(\lambda-1)^2}\]
    Is an invariant of this action.
\end{definition}


Recall that we also said that elliptic curves can be given by $\C/\langle 1,\tau\rangle$; so given $\tau$ there is a corresponding $j$-invariant:\begin{definition}
    If $q  = e^{2\pi i \tau}$, then \[j = q^{-1} + 744+196884q+21493760q^2+\cdots\]
\end{definition}

So two elliptic curves are isomorphic, iff they have the same $j$-invariant. So the moduli space is more or less isomorphic to the affine line $\mathbb{A}^1$ (not really since the moduli space of elliptic curve is not really a variety it is something called an \textbf{Algebraic stack}).


\paragraph*{Genus 2 curves}
We can write down genus $2$ curves in a similar way to how we write genus $1$ curves.
\[
    y^2 = (x-a_1)(x-a_2)(x-a_3)(x-a_4)(x-a_5)(x-a_6)    
\]
So we are taking six points in $\C$ and taking a curve that is ramified to order two at these six points. 

\

\subparagraph*{In general}
If we have a curve of the form\[y^2 = (x-a_1)\cdots (x-a_{2n})\]
We can work out the Euler characteristic as: $\chi = 2+2n - 4n+2 = 402n = 2-2g$, so $g = n-1$.

\
\begin{remark}
    This also shows that there are curves of any genus.    
\end{remark}

\begin{definition}
    Curves of the above form are called \textbf{hyperelliptic curves}.
\end{definition}

It turns out in genus $2$ all curves are hyperelliptic, so this gives all of them.

\subparagraph*{Classifying curves of genus 2}

The classification is equivalent to taking distinct six points in $\mathbb{P}^1$ and modding by the action of the group $PSL_2(\C)$,

Taking six distinct points of the projective line is more or less the same as classifying binary forms of degree six:\[a_0x^6+a_1x^5 y+\cdots  + a_6y^6 / SL_2(\C)\]
This is the problem of classifying invariants of binary quantics. 

This looks like $\mathbb{A}^3/(Z/5Z)$, where $(x,y,z)\rightarrow (\zeta x, \zeta^2 y, \zeta^3 z)$, where $\zeta^5=1$. The set of generators of this are:\[\langle x^5, x^3y, xy^2, y^5, x^2 z, xz^3, z^5, yz\rangle\]

There is no similar description of moduli space in higher dimension, it gets too complicated.

\paragraph*{Genus 3}
Genus $3$ curves include: \begin{itemize}
    \item hyperelliptic; $y^2 = (x_1-a_1)\cdots$
    \item degree $4$ nonsingular curves in plane
\end{itemize}

\begin{remark}
    Genus of degree $d$ nonsingular plane curve:\[\chi = 2d-d(d-1) = 2-2g \Rightarrow g = \frac{(d-1)(d-2)}{2}\]

    So:\[\begin{tabular}{cc}
        g & d\\
        0 & 2\\
        1 & 3\\
        3 & 4\\
        6 & 5
    \end{tabular}\]

    So it is quite unusual for an algebraic curve to be nonsingular plane curve.
\end{remark}

There is in fact a six dimension family of degree four nonsingular curves in the plane. On the other hand the hyperelliptic curves form a family of dimension $5$.

\subparagraph*{Trott curve: example of genus 3}

\[44(x^4+y^4)-255(x^2+^2)+350x^2y^2+81 = 0\]

This is an example of a curve where we can see all of it's $28$ bitangents (lines meeting the curve twice).

\href{https://en.wikipedia.org/wiki/Bitangents_of_a_quartic}{\includegraphics[scale=0.75]{trott.png}}

\paragraph*{Genus 4 and 5}
\begin{itemize}
    \item Genus $4$, given by the intersection of cubic and quadric in $\mathbb{P}^3$
    \item Genus $4$, given by the intersection of three quadric in $\mathbb{P}^4$
\end{itemize}

The higher the genus, the more complicated it is to describe what the curves look like. Representing curves of high genus is difficult.

\section{Hurwitz curves}
They are the most symmetric curves of genus $g$.\begin{itemize}
    \item $g = 0$: The only curve is projective line, and group of automorphisms is infinite
    \item $g = 1$: This is an elliptic curve, again the group of automorphisms is infinite 
    \item $g > 1$: Here Hurwitz proved that the order of the automorphism group $|G|\leq 84(g-1)$. If characteristic is zero.
\end{itemize}

\paragraph*{Idea of the proof}
We look at the \textbf{Orbifold Euler char}. Recall if we have a surface, then the euler characteristic is: \[\chi = n_2-n_1+n_0 \text{ where }n_i = \text{ number of cells of dim }i\]

In general the Euler characteristic for a compact orientable surface is \[\chi = 2-2g\]

\begin{definition}
    \textbf{Orbifolds} are given by \[\text{surface}/\text{finite group}\](at least for our purposes in general they can be more complicated)

    The idea is that we have a point fixed by the subgroup $H$. Then when working with the Euler characteristic the quotient of the point by the group $H$ should be thought as only $\frac{1}{|H|}$ of a point.
\end{definition}

\begin{example}
    Suppose we have a disc and we take an automorphism of order two that rotates by $180$ degrees. If we quotient out by this we have a sort of cone and the vertex of the cone counts only as half a point. 
\end{example}

\begin{remark}\textbf{Key point}
If we have a surface $S$ then:\[\chi(S/|G|) = \frac{\chi(S)}{|S|}\]
Furthermore we can consider the orbifold $S/|G|$ as a surface, and consider the regular Euler characteristic on it. So we find this definition:
\begin{definition}
    Suppose the orbifold \[S/G = \text{ surface }T\text{ with conical singular points of order }p_1,p_2,\ldots\]
    Then the \[\textbf{Orbifold }\chi = \text{Topological }\chi - (1-\frac{1}{p_1})-(1-\frac{1}{p_2})-\cdots\]
\end{definition}
\end{remark}

\subparagraph*{We want to show:} 
That the maximal value of this is $-\frac{1}{42}$. Where does this come from? The topological euler charn will be $2-2h$, for some $h$ where $h$ is the genus of the topological surface. And each of the $(1-\frac{1}{p})$ can be equal to $\frac{1}{2}, \frac{2}{3}, \frac{3}{4}, \ldots$
Since $g>1$ we have $\chi<0$ so this is a negative number and we want to make it as close to zero as possible.
\begin{enumerate}
    \item $h=0$, since if $h>0$, then \[2-2h - (1-\frac{1}{p_1})\cdots\] This is either zero which is not allowed or $\leq \frac{-1}{2}\leq \frac{-1}{42}$
    \item Suppose there are $\leq 2$ conical points, then the char is $2 - (1-\frac{1}{p_1}) - (1-\frac{1}{p_2}) > 0$, which is not allowed.
    \item Suppose there are $\geq 4$ conical points, then $2 - (1-\frac{1}{p_1})-\cdots -(1-\frac{1}{p_1}) \leq \underbrace{\frac{-1}{6}}_{2,2,2,3}\leq \frac{-1}{42}$
    \item So there are exactly three conical points, $p_1,p_2,p_3$.
\end{enumerate}
\subparagraph*{Examining $p_1,p_2,p_3$}

Suppose there are no conical points of order $2$, then:\begin{enumerate}
    \item $3,3,3$ which gives us $2-\frac{2}{3}-\frac{2}{3}-\frac{2}{3}-\frac{2}{3} = 0$
    \item $3,3,4$ which gives us $2-\frac{2}{3}-\frac{2}{3}-\frac{2}{3}-\frac{3}{4} = \frac{-1}{12}<\frac{-1}{42}$
    \item Anything else gives us a number even smaller than $\frac{-1}{12}$.
\end{enumerate}
So we can't have zero conical points of order two. So there is at least one point of order $2$. 

\subparagraph*{Examining $2,p_2,p_3$}

If there are two points of order two:\[2-0.5-0.5-(1-\frac{1}{p_3}) > 0\]Which is impossible.

So there is exactly one point of order $2$.

\subparagraph*{The case where $p_2,p_3\geq 4$}\begin{enumerate}
    \item $2,4,4$; $2-\frac{1}{2}-\frac{3}{4}-\frac{3}{4} = 0$.
    \item $2,4,5$; $2-\frac{1}{2}-\frac{3}{4}-\frac{4}{5} = \frac{-1}{20}<\frac{-1}{42}$.
    \item Anything else will be smaller than $\frac{-1}{20}$.
\end{enumerate}

So we can't have both points of order $\geq 4$:


\subparagraph*{Examining $2,3,p_3$}
So now suppose $p_3 > 2$:\begin{itemize}
    \item $p_3 = 3$: Euler char: $\frac{1}{6}$, not allowed since Euler char must be $<0$
    \item $p_3 = 4$: Euler char: $\frac{1}{12}$, not allowed since Euler char must be $<0$
    \item $p_3 = 5$: Euler char: $\frac{1}{30}$, not allowed since Euler char must be $<0$
    \item $p_3 = 6$: Euler char: $0$, not allowed since Euler char must be $<0$
    \item $p_3 = 7$: Euler char: $\frac{-1}{42}$
    \item $p_3 = 8$: Euler char: $\frac{-1}{24}\leq \frac{-1}{42}$
    \item $p_3>8$: Euler char: $<\frac{-1}{24}\leq \frac{-1}{42}$
\end{itemize}

If $\chi<0$, then $\leq \frac{-1}{42}$ so:\[\frac{2-2g}{|G|}\leq \frac{-1}{42} \text{ if }<0\]
\[\therefore |G| \leq 84(g-1)\]
Moreover if this bound is satisfied this implies $G$ is generated by $\alpha, \beta,\gamma$ where $\alpha^2 = 1$, $\beta^3 = 1$, $\gamma^7 = 1$ and $\alpha\beta\gamma = 1$. Such a finite group is called a \textbf{Hurwitz group}.

\subsection{Example of Hurwitz curves}
For genus $g\leq 1$ we still get interesting information on $G$. \begin{itemize}
    \item If $g=0$: This is the case when the orbifold euler char is positive, there aren't that many those give us the orders of a finite group of automorphisms of a genus $0$ surface(more or less a sphere). The smallest positive value is:\[\frac{2}{|G|} = 2-\frac{1}{2}-\frac{2}{3}-\frac{4}{5} = \frac{1}{30}\] So a finite value of $|G|<\infty$ acting on $\mathbb{P}^1$ is $60$.
    \item If $g=1$: The orbifold characteristic is zero, so the choices for $p_i$ can be \[\{p_i\} = \begin{cases}
        \{\}\\
        \{2,2,2,2\}\\
        \{3,3,3\}\\
        \{2,4,4\}\\
        \{2,3,6\}
    \end{cases}\] These correspond to various finite cyclic groups acting on elliptic curve and fixing a point.
\end{itemize}

\paragraph*{Genus $g=2$: Most symmetric curve?}
Can we find a genus $2$ curve with automorphism group $G$ of order $84$? The answer is \textbf{no}.

Suppose $|G| = 84$, by the Sylow theorems, $G$ has a normal subgroup of order $7$ (since $84 = 2^2\cdot 3\cdot 7$). So the quotient of $G$ and this group of order seven, has order $12$ and has a normal subgroupn of order $3$ or $4$ by the Sylow theorems. In the second case, if it is of order $4$, then any element of order $2$ or $7$ is in the normal subgroup of order $28$ which has no elements of order $3$. So there are no elements of order $2$ and $7$ whose product is of order $3$.
This is a contradiction since any Hurwitz group has three elements $a,b,c$ such that $a^2=b^3=c^7=abc=1$.

The case where $G/7$ has a normal subgroup of order $3$ causes a similar contradiction.

So in all cases a group of order $84$ can't be generated by three elements as above, so it can't be a Hurwitz group.

\

But recall that if $|G| \neq 84(g-1)$, then $|G| \leq 48(g-1)$.

\begin{example}
    We can take the \textbf{double branched cover of }$\mathbb{P}^1$, branched over $0,\infty, 1,-1,i,-i$. We can think of this as being something like the vertices of an octohedron. So the point is the subgroup of $PSL_2(\C)$ fixing this set of six points is generated by the following automorphism:\begin{align*}
        z&\rightarrow iz\\
        z&\rightarrow \frac{z+1}{1-z}\\
    \end{align*}
\end{example}

\paragraph*{What about $g=3$?}
In this case there is a Hurwitz curve given by the \textbf{Klein quartic}:\[x^3y+y^3z+z^3x = 0 \subseteq \mathbb{P}^2\]


We can check that this is nonsingular and the genus of a degree four nonsingular curve is $3$. And up to isomomorphim this is the only genus $3$ curve whose automorphism group achieves the bound $84(g-1) = 168$.

It's automorphism group is infact:\[PSL_2(F_7)\] Which is a simple group of order $168$, the next smallest simple group after $A_5$ of order $60$.


\begin{remark}
    This upper bound fails in characteristic $p>0$. This proof doesn't work since we can't define the Euler characteristic in a way that behaves as in characteristic zero. 
\end{remark}

\section{Desingularising an algebraic curve}
Recall f.g.~function fields of transcendence degree $1$ corresponds to nonsingular proj curves. 
(c.f.~Hartshorne)

First of all if a function field has transcendence degree $1$ and is f.g., then it is generated by two elements it is of the form $k(x,y)$ with $x$ transcendental over $k$. And there is some polynomial $f(x,y) = 0$. This gives us a projective curve possibly singular.

\paragraph*{How do we get rid of the singularities? How do we resolve singularities of curves?}
\subparagraph*{Newton}
We can resolve singularities of $f(x,y) = 0$ by repeated blow ups. We will assume that char = $0$. This proof doesn't work in char p but the result is still true.

\begin{proof}
    We assume the singular point is $(0,0)\subseteq \mathbb{A}^2$, we look at the lowest degree terms:\[a_n y^n+ a_{n-1} y^{n-1}x+\cdots+ a_0 x^n + (\text{degree }>n)\] $n$ is called the multiplicity of the singularity.
    
    THe lowest degree terms form a homogenous polynomial so they have roots in $\mathbb{P}^1$. These roots correspond to the directions of the singularity. 

    \

    Blowing up the point means seperating out the different directions. One blow up reduces the multiplicities \textit{unless} all $n$ points of $\mathbb{P}^1$ are the same. 
    \[\text{i.e.} a_n y^n+\cdots + a_0x^0 = a_n{(y+\alpha x)}^n \text{ for }a_n\neq 0\]
    
    So we can consetrate on the case where all the n roots are the same. By a change of variable: $y\rightarrow y-\alpha x$, then we get $y^n + (\text{terms of degree }>n)$.

    \begin{example}
        Suppose we have $y^2 + x^9+y^5x^3$, if we blow it up we change $y\rightarrow yx$ and we get:\[
            y^2x^2+x^9+y^5x^5x^3 = 0 \Rightarrow y^2+x^7+y^5x^6 = 0    
        \]
    \end{example}

    \subparagraph*{Picture of what is going on}
    \begin{center}
        \href{https://youtu.be/Qhq5vwFf0kQ?t=713}{\includegraphics[scale=0.25]{newton-proof}}    
    \end{center}
    
    What does blowing up do to these monomials? $y^n x^k$ doesn't change anything since we are going to change $y\rightarrow yx$ and then divide by $x^n$ again.

    In the case $y^{n-1}x^3 \rightarrow y^{n-1}x^{n-1+3}\rightarrow y^{n-1}x^{-1+3}$, so these kind of monomials go one step to the left, likewise any monial whose power of $y$ is less then $n$ are shifted to the left and any monomial whose power of $y$ is greater than $n$ to the right.
    
    So we can just keep blowing up and shift monomials to the left until one of them hits the blue line (the line $y^{n-i}x^i$), then we are back as in our first case, where we reduce multiplicity by blowing up.

    This almost works but there is one problem. If we do this the monomials of degree $n$ might be a another $n$th power, it is not clear that this process terminates.

    We keep doing transformations of the form $y\rightarrow y+\ast x^n$, for $n=2,3,4,\ldots$. There are two possibilities:\begin{enumerate}
        \item We keep going through this process an infinite amount of times.
        \item This stops.We get an expression where we have a term in $y^n$, the term in $y^{n-1}x$ is zero and we have some non-zero terms under that in $y^{n-i}x^i$. But at this point we are done since \[{(y+\alpha x)}^n = y^n+n\alpha y^{n-1}x+\cdots\] And $n\alpha\neq 0$ in char $0$.
    \end{enumerate}
What if we get an infinite number of substiutions $y\rightarrow y+\ast x^n$? If this happens we find an infinite power series. The substiution \[y\rightarrow y^n+\ast x^2+\ast x^3+\cdots\]
transforms our polynomial to a power series $y^n + \text{terms in }y^{\geq n}$, so our original polynomial is divisible by some ${(\text{power series})}^n$, where $n\geq 2$. But this is not possible as the original polynomial $f(x,y)$ has no multiple factors.
\begin{remark}
This can happen with polynomials with multiple factors, for example with the polynomial $f(x,y) = {(y^2-y+x^2)}^2$, we can never resolve the singularities. A reason for this is that this polynomial has singularities at every point since it is a power 
of a nontrivial polynomial.
\end{remark}
\end{proof}

\subsection{Newton's rotating ruler}
Let start by taking $f(x,y)\in \C[x,y]$ such that $f$ is not a polynomial in $x$. The result that Newton proved is that we can write $y$ as a power series in $x^{1/N}$ for some $N$.

\begin{definition}
    \textbf{Puiseux Series} \[\bigcup_N \C[[x^{1/N}]][x^{-1/N}]\]
    They were first used by Puiseux in 1850, and first used by Newton in 1676.
\end{definition}

\begin{example}
    If $f(x,y) = y^2-x^2-x^3$, then:\begin{equation*}
        y=x\sqrt{1+x} = x(1+\frac{1}{2}x-\frac{1}{8}x^2+cdots)
    \end{equation*}

    Another more trivial example:

    If $f(x,y) = y^2-x^3$; then $y=x^{3/2}$ which is a power series in $x^{1/2}$.
\end{example}

\begin{theorem}\textbf{Newton's theorem}
    We can find solutions to alegbraic equations that have power series in some fractional power of $x$.

    \begin{proof}
        
    
We can assume $f(x,y)$ has a term $y^n$ but not $y^i$ for $(i<n)$ (if everything in $f$ is divisible by $x$ we divide and continue). Then we plot all monomials $x^i y^i$ in $f$ and mark the ones that occurs in $f$.

Then we take our ruler and rotate it until we can draw a line between $y^n$ and a marked point, and we can continue by drawing a line from the second point to another marked point etc. The polygon we get is called a \textbf{Newton polygon}.
\href{https://youtu.be/paenRVq0vnc?t=396}{\includegraphics[scale=0.25]{newton-ruler}}

Then we add extra points, so that our line passes through a point on the line $y^{n-1}$, we do this by making a change of variable $x\rightarrow x^{1/r}$ for a suitable $r$.

\href{https://youtu.be/paenRVq0vnc?t=530}{\includegraphics[scale=0.25]{newton-ruler-2}}


We think of the points on the line as the smallest degree terms, where $\deg y = m$ and $\deg x = 1$. These monomials on the line form a homogenous polynomial in $y,x^m$. We factorize it to 
\[
    (y-\alpha x^m)(y-\beta x^m)\cdots    
\]

We do a change of variable $y\rightarrow y+\alpha x^m$
\[
    y(y-\ast x^m)\cdots    
\]

What this means is that there is no term in $x^{mn}$. So $0$ is a root possibly of high multiplicity.

Now we do a change of variable $y\rightarrow x^m y$, and all our terms are now divisible by $x^{mn}$, so we can divide by $x^{mn}$. So what happens to our coefficients? All the coefficients on the Newton ruler line are shifted. 

\href{https://youtu.be/paenRVq0vnc?t=725}{\includegraphics[scale=0.25]{newton-ruler-3}}

If any of the coefficients as marked in the picture are non-zero we have changed $y^n\rightarrow y^{smaller}$, so we can go back and start again, with the smaller power of $y$.

This happens if the orginal root $\alpha$ is of multiplicity less then $n$. What do we do if it has multiplicity equal to $n$? Then after the change of variable all the coefficients on newton's ruler are zero, so we rotate it until we reach a new non-zero point.

This either continues forever or $\underbrace{\text{we reduce the smallest power of }y}_{\text{this happens only a finite number of times}}$

So we eventaully reach were we change\[y\rightarrow y+\ast x+\ast x^2+\cdots\]
So the coeffs $y^i x^i$, with $i<n$ are now zero.

So our series is divisible by $y$. So the \textit{original} $f(x,y)$ is divisible by ($y-$ power series in $x^{1/r}$), for some $r$

So we have found \[y=\text{power series in }x^{1/r}\]
Such that $f(x,y) = 0$.
 
\end{proof}
\end{theorem}


\paragraph*{Consequences}
\begin{corollary}
    The Field of Puiseux series is algebraically closed.

    \begin{proof}
        Use Newton's method $f(x,y)$ which is a Puiseux series in $x$. We can think of this as a polynomial in $K[y]$, where $K$ is the field of Puiseux series. So if we have a polynomial in $y$ whose coefficients are Puiseux series, then we can find a root that is also a Puiseux series. So this is field is alg closed
    \end{proof}
\end{corollary}


\begin{definition}
    The Puiseux series are an example of a \textbf{Quasi finite field}. Which means that they behave like a finite field in the following sense. 

    The collection of field extensions of $K$ are got by adding a root of $x$, this is similar to with finite fields as this diagram shows:

    \href{https://youtu.be/paenRVq0vnc?t=1243}{\includegraphics[scale=0.25]{newton-ruler-4}}

    In both cases the ``absolute Galois group'' can be written as the completion of the integers, $\hat{\mathbb{Z}}$
\end{definition}

\paragraph*{What does Newton's theorem have to do with resolutions of singularities?}

Suppose we have a plane curve $f(x,y)$ with singularities. Then Newton's theorem says that if we change $x\rightarrow x^{1/r}$. We have $f(x^{1/r}, y)$ which looks like this:

\begin{center}
    \href{https://youtu.be/paenRVq0vnc?t=1351}{\includegraphics[scale=0.25]{newton-ruler-5}}   
\end{center}

This is nearly a desingularisation of the singularity. We split off the singularity into a bunch of nonsing curves that touch at one point. We can ``pull the curves apart'' to resolve our singularity.


\lecture{Intersections}
\section{Hilbert Polynomials of graded modules over a graded ring}

Suppose $M$ is a graded module over a graded ring, i.e.\[
    M = \oplus M_n \text{ and }R = k[x_1,\ldots,x_n]/(I) \text{ where }\deg x_i = d_i>0
\]
We also take $M$ to be finietely generated.

We can ask what is the ``growth rate'' of the components $M_n$? We encode these dimensions as a function\[
    f_M(x) = \sum x^n \dim(M_n), \text{where }\dim M_n\text{ as a vector space over }k    
\]

\paragraph*{Basic result:}

$f_M(x)$ is a \textit{rational function}, we see this by looking at the following exact sequence:\[
    0\rightarrow \ker x_k\rightarrow M\underbrace{\rightarrow}_{(1)} M(d_k)\rightarrow M(d_k)/x_k M\rightarrow 0    
\]

Where, $M(d_k)$ means $M$ with a grading shifted by $d_k$ and $(1) \text{ is multiplication by }x_k$. Moreover $\ker x_k$ and $M(d_k)/x_k M$ are modules over $R/x_k$.

\

So \[f_{M(d_k)} = x^{d_k}f_M \Rightarrow (1-x^{-d_k})f_M = \text{rational function}\]

Note the above function is indeed a rational function since it is something involving $\ker x_k$ and $M(d_k)/x_k M$ which are modules over rings with smaller number of generators so by induction are rational function.

\

Therefore $f_M$ is a rational function! So \[f_M = \text{ rational function with denominators }(1-x^{d_1})(1-x^{d_2})\cdots (1-x^{d_k})\]
A special case is when all $d_i = 1$ the denom will be ${(1-x)}^k$. We see that the coeffs of $x^i$ for $i\geq 0$ are given by polynomials in $i$, for example\begin{align*}
    \frac{1}{1-x} &= 1+x+x^2+\cdots\\
    \frac{1}{{(1-x)}^2} &= 1+2x+3x^2+\cdots
\end{align*}
\begin{definition}
    This means that $\dim(M_n)$ is a polynomial in $n$ for $n$ large ($>0$). This is called the \textbf{Hilbert Polynomial of }$M$. It describes how fast the graded pieces of $M$ grow for large degree. 
\end{definition}
\paragraph*{Special property of Hilbert polynomial}
In general the coefficients of this polynomial need not be integers, but this polynomial has the following property:\[f(n) \text{ is an integer for }n\text{ integer}\]
So this in an integer valued polynomial. Since for any value of $n$ (sufficiently large), this polynomial will be the dimension of some graded piece of $M$, and if a polynomial is a polynomial for large interger values then it is for all integer values. 


\paragraph*{What do integer valued polynomials look like?}

Well it is clear that $a_0+a_1x\cdots+a_k x^k\in \Z$ if $x\in \Z$ and $a_i\in \Z$. But the converse is not true:\[\frac{x(x-1)}{2}\in \Z \text{ for }x\in \Z\]
But it doesn't have integer coefficients.

\

Let us classify the integer valued polynomials:\begin{align*}
    1 &= \binom{x}{0}\\
    x &= \binom{x}{1}\\
    \frac{x(x-1)}{2} &= \binom{x}{2}\\
    \frac{x(x-1)9x-2}{3!} &= \binom{x}{3}
\end{align*}
\begin{proposition}
    We claim that the binomial coefficients span all integer valued polynomials.
\begin{proof}
    Suppose $f(n)$ is a degree $k$ integer valued polynomial, we can find a linear combination:\[
        f(n) - a_0\binom{x}{0} - a_1\binom{x}{1} - \cdots - a_k\binom{x}{k} = 0 
    \]
    
    For $x=0,1,\ldots,k$. We can do this since we can choose $a_0$ to make this polynomial vanish at $x=0$, then we can choose $a_1$ such that this polynomial vanishes at $x=1$, and since $\binom{x}{k}$ vanishes at $x=0$ for $k>1$ it will not affect the value of our polynomial at $x=0$. And so on and so on....
    
    \
    
    So we get a polynomial of degree $k$ vanishing at $k+1$ points ($0,\ldots,k$) so it is $0$ for all $n$. So we indeed see that any integer valued polynomial is a linear combination of binomial coefficients polynomials.
\end{proof}    
\end{proposition}

\subparagraph*{Important special case}
The leading coefficient of $f$ is of the form \[a_k \frac{x^k}{k!}\] Where $a_k\in \Z$. This integer will turn out to give us the degree of various algebraic varieties.


\subsection{Application: Degree of a variety}
Suppose we have a projective variety $V\subseteq \mathbb{P}^n$, if $V$ is a hypersurface then it would be given by $f(x_0,\ldots,x_n) = 0$ and we can set \[\deg V = \deg f\]

\paragraph*{What if $codim>1$?}

Notice that in the above case $\deg(V) = \# \text{ of intersection points of V}\text{ with }H\text{(hypersurface)}$, most of the time. Indeed if we take $V$ to take a line and the hypersurface to be the same line then it would have infinte amount of intersection point instead of the usual $1$.

So we say that the degree of $V$ is the number of intersection points of $V$ with a ``generic'' hyperplane. What does generic mean? It is not precicely defined, a hyperplane is generic if it makes things ``work properly''.

\
\begin{definition}
    In all cases for any variety $V$:\[
        \deg(V) = \# \text{ intersection points with a LINEAR VARIETY of codim equal to }\dim V    
    \]    
    Note a linear variety is: a line, plane, hyperplane, etc
\end{definition}
This was the old and classical definition of a variety, which is fine for informal arguments but is tedius when we try to prove things.

\paragraph*{Cleaner definition with Hilbert polynomial}

\begin{definition}
    The variety $V$ corresponds to an ideal $I\subseteq k[x_0,\ldots,x_n] = R$ (where $V = $ zeroes of $I$). Since $R/I$ is a graded ring it is a module over itself and so we get a Hilbert polynomial.

\

    Recall that the degree of the Hilbert polynomial is equal to the $\dim V$   
    (This comes from commutative alg, where we have a theorem that says $\dim$ of a local ring = $1+\deg$ Hilbert poly of a graded ring $m^n/m^{n+1}$).
As we saw last lecture the leading coefficient of the Hilbert polynomial is $a\frac{x^d}{d!}$, for some integer $a$. This integer $a$ is called the \textbf{degree} of $V$ 
\end{definition}


\begin{example}
    Let $V = \mathbb{P}^n$, then we take the graded ring $k[x_0,\ldots,x_n]$ and $I=0$. 
    
    We look at the graded piecese of this graded ring. There are $1$ thing of degree $1$, there are $\frac{n(n-1)}{2}$ things of degree $2$\dots
    
    In general we see that we have \[\binom{n+k}{n} = \frac{k^n}{n!}+\text{smaller terms}\]

    So we have $\dim(\mathbb{P}^n) = n$ and $\deg(\mathbb{P}^n) = 1$.
\end{example}

\paragraph*{Hypersurface of degree $d$ polynomial $f(x_0,\ldots,x_n)$}
\begin{example}
    We are looking at the graded ring $k[x_0,\ldots,x_n]/(f)$. The Hilbert polynomial is:\[
        \binom{n+k}{n} - \binom{n+k-d}{n} = \frac{dk^{n-1}}{(n-1)!} + \text{smaller terms}    
    \]

    We are taking the number of monomials of degree $k$ and substracting the monomials of degree $k-d$(we subsstract those since any of these times $f$ will be of degree $n$).

    So the hypersurface has dimension $n-1$, and the degree is $d$.

\end{example}


\paragraph*{Twisted cubic $\subseteq\mathbb{P}^3$}
\begin{example}
    Recall that if we take the ring of polynomial to be $k[w,x,y,z]$, then the Twisted cubic is defined by the equations:\[
        wz = xy, \ x^2 = wy, \ y^2 = xz    
    \]
    
    In genreal, we can kill off any terms containing $x^2,y^2$ or $xy$. Since we can turn them into smaller powers of $x$ and $y$. So a basis of the quotient ring is \[
        w^i z^{k-i}, w^i xz^{k-i-1}, w^i yz^{k-i-1} \text{ degree }k\geq 3    
    \] 
    
    There are $k+1+k+k = 3k+1$ possibilities, so $\frac{3k}{1!}+1$ is the Hilbert polynomial. So we see that degree is equal to three and has dimension equal to $1$.
    
    But recall that the twisted cubic is isomorphic to $\mathbb{P}^1$, which has degree $1$. So the degree is not an invariant, it depends on the embedding of the abstract variety into projective space.        
\end{example}

\paragraph*{Invariant of projective variety, the euler polynomial of $V$}

\begin{definition}
    We call \[\chi(V) = \text{ constant term of Hilbert polynomial}\] The \textbf{Euler polynomial} of $V$.
\end{definition}

\subparagraph*{Property 1:}
$\chi(V)$ depends only on the abstract variety $V$, not on the embedding into projective space

\

\begin{definition}
    For historical reasons people would use \[{(-1)}^\text{dim}(\chi(V)-1)\]    
Which is called the \textbf{Arithmetic genus}
\end{definition}

\subparagraph*{Property 2:}
$\chi(V\times W) = \chi(V)\chi(W)$.
Which is not a property that the Arithmetic genus has.

\begin{remark}
    When we do sheaf cohomology we will see that there is another definition of $\chi(V)$.    
\end{remark}

\paragraph*{Working out arithmetic genus of plane curve, degree d}
\begin{example}
    The Hilbert polynomial:\[
        \binom{2+k}{2} - \binom{2+k-d}{2} = 1 + dk - \frac{(2-d)(1-d)}{2}
    \]    
    So the arithmetic genus is $\frac{(2-d)(1-d)}{2}$, this is where the name comes from, for topological curves the arithmetic genus is the same as the topological genus.
\end{example}

The hilbert polynomial is more or less the only discrete invariant of closed subvarieties of projective space: Theorem by R. Hartshorne.

\section{Bézout's theorem}
In it's most simplest form \textbf{Bézout's theorem} says that ``Ìf we have two curves of degree $m,n$ then they intersect in $mn$ points''. The problem is that this version of the theorem is false.

\paragraph*{Problems with this theorem}\begin{enumerate}
    \item They may have parallel lines, so they would intersect in zero points. In order to work around this we need to work in $\mathbb{P}^2$ (we need points at infinity).
    \item If we take two curves like $y=0$ and $y=x^2+1$, they intersect in no real points. We need to use an algebraically closed field ($\C$ in our case)
    \item They may have a component in common, in fact they may be the same curve. We need to say that they have no components in common.
    \item Points may have multiplicities, for example $y=0$ and $y=x^2$ only have one point in common, but we need to think of this intersection point as a point with multiplicity $2$. If we deform the parabola slightly it meets the straight line at two points.  
\end{enumerate}


\begin{theorem}\textbf{One version of Bézout's theorem}
    Two distinct irreducible curves in $\mathbb{P}^2(\C)$ of degrees $m,n$ repec. Intersect in $mn$ points counted with multiplicities.
\end{theorem}

Note that we haven't stated what \textit{multiplicity} is in this question.
\paragraph*{Variations of Bézout's theorem}
\begin{theorem}\textbf{n-dim version of Bézout's theorem}
    $n$ distinct hypersurfaces in $\mathbb{P}^n(\C)$ of degrees $d_1,\ldots, d_n$ \textit{usually} intersect at $d_1\cdots d_n$ points.
\end{theorem}
Roughly speaking \textit{usually} means that the intersection shouldn't have the wrong dimension.


\begin{remark}
    Take two algebraic sets of degree $m,n$ is their intersection of degree $mn$?
\end{remark}

We have the usual problem, that the intersection must be of the wrong degree.

\

\paragraph*{Informal ``proof''}
While an informal proof is not a proof this is a sort of intuition for it.
\textit{``Proof''}

    $f(x,y) = 0$ and $g(x,y) = 0$. We deform $f,g$ so they are both the union of a finite number of lines. Then $f$ is the union of $m$ lines and $g$ is the union of $n$ lines, so there are $mn$ points of intersection. 

\

    \boxed{QED?}

\

This is not a proof, since we haven't shown that the number of interesection points doesn't change as we deform $f,g$.


\paragraph*{How do we state and prove Bézout's theorem correctly?}
\subparagraph*{Defining multiplicity}
We need to use the theory of finitely generated modules $M$ over a Noetherian ring $R$.

We have:\[0=M_0\subseteq M_1\subseteq M_2\ldots\subseteq M_n = M \]
So that $M_i/M_{i-1}\simeq R/p$ for some prime ideal $p$. 
(\textit{proof:} We choose $I$ maximal among ideals such that $R/I\subseteq M$, it is easy to check that $I$ is prime, and let $M_1\simeq R/I$ and repeat for $M/M_1$) 



\begin{definition}\textbf{Phoney definition}
    We define multiplicity of $R/p$ in $M$ to be the number of times $R/p$ is isomomorphic to $M_i/M_{i+1}$.
\end{definition}
The problem is that this definition doesn't work in general.

\

\begin{example}\textbf{An example where this works}
    Let $R=\Z$, and $M = $ a finite group. Then:\[
        R/(p) = \Z/2\Z, \Z/3\Z,\ldots    
    \]
    So the number of times $\Z/p_1\Z$ is in $M$ is equal to $n_1$, where $|M| = p_1^{n_1}p_2^{n_2}\cdots$ 
\end{example}

\begin{example}
    However if $M$ is f.g. this might not be well defined. If $M=\Z$, then we have:\[
        \underset{M_0}{0}\subseteq \underset{M_1}{\Z} \text{ then }\Z/2\Z \text{ occurs }0\text{ times}
    \]
    But we can take another filtration of $\Z$:\[
        \underbrace{0\subseteq}_{\simeq \Z/(0)} 2\Z \underbrace{\subseteq \Z}_{\Z/2\Z} \text{ then }\Z/2\Z \text{ occurs once}    
    \]

    So we can't define multiplicity in $\Z$, it varies depending on what filtration we use.
\end{example}

\begin{example}
    $M$ is a f.g. module over $\Z$, the multiplicity of $\Z/(0)$ in $M$ is defined.
\end{example}

Sometimes the multiplicity is not defined, and sometimes it is defined what is the difference?

\
\begin{remark}
    Look at prime ideals $p_1,p_2,\ldots$ with $R/p_i$ in a filtration of $M$. If $p_i$ is \textit{minimal} in this set, we get a well-behaved multiplicity.    
\end{remark}
Indeed if $p_i$ minimal, the multiplicity is equal to the length of $M_p$ over $R_p$ where $R_p$ is the localization of $R$ at $p$. 

\subparagraph*{Geometric way of looking at this}
Let $R=\C[x,y]$; prime ideals correspond to points, curves and the plane.

Suppose we have a picture like this:
\begin{center}
    \href{https://youtu.be/UJssbO-e2yw?t=1194}{\includegraphics*[scale = 0.25]{minimal-primes.png}}    
\end{center}

The multiplicity of the red and green primes are well-defined but not of the blue prime. Furthermore recall that while the blue prime seems smaller than the green one, the order gets reversed when we look at ideals.

To summarise, if we have a module over a ring $R$, we can define the multiplicity for some primes of $R$. 

\

There is a similar theorem for graded modules over graded rings, the only difference is that if we have a graded prime we might have to shift the degree of everything by some integer $\ell$ denoted like this $(R/p)(\ell)$


\paragraph*{Finally, Bézout's theorem}
\begin{theorem}
    Let $Y$ be a variety in $\mathbb{P}^n$ and take $H$, a hypersurface given by $f(x_0,\cdots,x_n) = 0$. We assume that $Y$ is not contained in $H$. 

    \

    Then \[\sum_j i(Y, H; Z_j)\deg Z_j = \deg Y\deg H \tag{$\dagger$} \]

Here $Y\cap H$ is the union of irred components of $Z_j$ and $i(Y, H; Z_j)$ is the multiplicity of $Y$ and $H$ at $Z_j$. 
\end{theorem}

We look at the following exact sequence\[
0\rightarrow \underset{1}{k[x_0,\ldots,x_n]/I_Y} \overset{\cdot f}{\rightarrow} \underset{2}{k[x_0,\ldots, x_n]/I_y}\rightarrow \underset{3}{k[x_0,\ldots, x_n]/(I_y, f)}\rightarrow 0
\]

Note when we multiply by $f$, we must shift the degree by $-\deg f$.

\

Now the hilbert polynomial of $(2) - $ the hilbert polynomial of $(1) = $ hilbert polynomial of $(3)$. So \[
    \textbf{Hilbert polynomial of }(3) = p_Y(z) - p_Y(z-d) \text{ note we subtract by }d\text{ since we shifted the degrees}    
\]

So the hilbert polynomial is \[\frac{\deg Y Z^{\dim Y}}{(\dim Y)!} + \cdots - \frac{\deg Y {(Z-d)}^{\dim Y}}{(\dim Y)!} \]


Since ${(Z-d)}^{\dim Y} = Z^{\dim Y} - \dim Y\cdot dZ^{\dim Y - 1}$; the the leading term of our hilbert polynomial is:\[
    d\deg Y \frac{Z^{\dim Y - 1}}{(\dim Y - 1)!}   
\]
The conclusion is \[
\text{Hilbert polynomial of }k[x_0,\ldots, x_n]/(I_y, f)\text{ has leading term of }\deg H \deg Y\frac{Z^{\dim Y - 1}}{(\dim Y - 1)!}
\]

\subparagraph*{Proving $\dagger$}

Now the module $k[x_1,\ldots,x_n]/(I_Y,f)$ has a filtration:\[
    0 = M_0\subseteq M_1\subseteq M_2\subseteq \cdots \subseteq M_n = M    
\]
Where the quotients are $(R_i/q_i) (\ell_i)$, for some prime ideal $q_i$. Shifting the the degree by $\ell_1$ doesn't change the leading coefficient. So the Hilbert polynomial of each of these has leading coefficients

\[\frac{\deg Z_j}{(\dim Z_j)!}Z^{\dim Z_j} \] Where $\dim Z_j = \dim Y - 1$


So we see that \[
    \deg H \deg Y = \sum_{i}\deg Z_j \cdot\text{multiplicity of }(R/q_i) \text{ in }k[x_0,\ldots,x_n]/(I,f)    
\]

So we need to show that $\text{multiplicity of }(R/q_i) \text{ in }k[x_0,\ldots,x_n]/(I,f)$ is the multiplicity of the intersection.

We need to use the \textit{sharphooter technique}\texttrademark, we define our target to be what we shot at i.e. We \textbf{define the multiplicity of the intersection} to be $\text{multiplicity of }(R/q_i) \text{ in }k[x_0,\ldots,x_n]/(I,f)$ 


\

\begin{center}
    \Large{Part 1 DONE!}

    \qed
\end{center}


\end{document}

